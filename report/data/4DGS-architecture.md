# Maximum Rotation Robustness for 4D Gaussian Splatting Classification

Building rotation-robust classification systems for 4D Gaussian Splatting requires combining three complementary architectural paradigms: **Vector Neurons** provide the mathematical foundation for SO(3)-equivariance through 3D vector representations, **VN-Transformer** extends this with rotation-equivariant attention achieving **90.8% accuracy with 50× fewer parameters** than alternatives, and **Mamba4D** enables efficient temporal modeling with **87.5% memory reduction** over transformers through state space models. [arxiv](https://arxiv.org/html/2405.14338) Processing spherical harmonic coefficients equivariantly demands Wigner D-matrix transformations, where coefficients of degree l transform via **(2l+1)×(2l+1)** rotation matrices.

The core insight is that 3D Gaussian Splatting's heterogeneous attributes—positions, covariances, quaternions, spherical harmonics, and opacity—each transform differently under rotation. Positions and covariances require type-1 (vector) equivariant processing, spherical harmonics need degree-specific Wigner D-matrix transformations, while opacity remains invariant. This mathematical heterogeneity makes rotation-robust 4DGS classification significantly more complex than standard point cloud classification.

## Vector Neurons establish SO(3)-equivariance through dimensional lifting

The Vector Neurons framework from Deng et al. (ICCV 2021) provides the mathematical foundation by lifting scalar neurons z ∈ ℝ to 3D vectors **v** ∈ ℝ³. For a point cloud with N points and C feature channels, the representation becomes **V** ∈ ℝ^(N×C×3), where each feature channel is a 3D vector that transforms coherently under rotation. [arxiv](https://ar5iv.labs.arxiv.org/html/2104.12229) This structure guarantees that for any rotation matrix R ∈ SO(3), applying R to all feature vectors produces outputs that are also rotated by R.

The **VN-Linear layer** is mathematically elegant: given weight matrix W ∈ ℝ^(C'×C) and input V ∈ ℝ^(C×3), the operation V' = WV applies weights to the channel dimension while leaving the spatial dimension untouched. [arxiv](https://ar5iv.labs.arxiv.org/html/2104.12229) Since rotation acts on spatial dimensions and linear combination acts on channels, these operations commute, guaranteeing equivariance. Critically, bias terms are excluded because a fixed bias vector cannot transform under input rotation. [arxiv](https://ar5iv.labs.arxiv.org/html/2104.12229)

The **VN-ReLU layer** addresses the challenge that element-wise nonlinearities break rotation equivariance. The solution learns data-dependent directions for activation: computing q = WV and k = UV, the output projects q onto the half-space defined by k when ⟨q,k⟩ < 0, effectively applying a directional ReLU along a learned, input-dependent direction. Both q and k are linear in V, so they transform equivariantly, and the inner product ⟨qR, kR⟩ = ⟨q,k⟩ remains invariant, preserving equivariance through the nonlinearity. [github](https://duanyueqi.github.io/ICCV21_Vector Neurons A General Framework for SO(3)-Equivariant Networks.pdf)

**VN-BatchNorm** normalizes only the rotation-invariant component of features—their 2-norms—by computing N_b = ElementwiseNorm(V_b) per channel, applying standard batch normalization to these scalars, then rescaling the original vectors. The **VN-Invariant layer** extracts rotation-invariant features by learning a local coordinate frame T ∈ ℝ^(3×3) from the features themselves, then expressing features in this frame via V·Tᵀ. Since both V and T transform under rotation, their product remains invariant.

The VN framework enables parameter-efficient networks: VN-DGCNN and VN-PointNet use approximately **≤ 2/9 the parameters** of scalar counterparts while achieving consistent accuracy across arbitrary rotations. [arxiv](https://ar5iv.labs.arxiv.org/html/2104.12229) On ModelNet40, VN-DGCNN achieves **89.5% accuracy** whether trained/tested on aligned data (z/z) or arbitrary rotations (SO(3)/SO(3)), compared to standard DGCNN's **33.8% accuracy** when trained on aligned data but tested under rotation.

## VN-Transformer introduces rotation-equivariant attention via Frobenius inner products

The VN-Transformer (TMLR 2023, Waymo) extends Vector Neurons with self-attention, addressing the limitation that VN-DGCNN relies on edge convolution which limits receptive field growth. The key innovation is using **Frobenius inner products** between VN representations to compute attention weights. For two VN features V^(n), V^(n') ∈ ℝ^(C×3), the Frobenius inner product ⟨V^(n), V^(n')⟩_F = Σ_c V^(n,c)·V^(n',c)ᵀ sums the dot products across all channels.

This inner product is rotation-invariant because ⟨V^(n)R, V^(n')R⟩_F = Σ_c V^(n,c)RRᵀV^(n',c)ᵀ = ⟨V^(n), V^(n')⟩_F, using the orthogonality property RRᵀ = I. Attention weights computed as softmax(⟨Q,K⟩_F/√(3C)) are therefore rotation-invariant, while value aggregation Σ_n A^(m,n)Z^(n) with scalar weights A and equivariant values Z produces equivariant outputs.

The full **VN-MultiHeadAttention** operation applies learned projections W^Q, W^K, W^Z to queries, keys, and values respectively, computes attention per head, then projects outputs through W^O. The VN-Transformer encoder block follows the standard transformer pattern—LayerNorm → MultiHeadAttention → residual → LayerNorm → MLP → residual—with VN-equivariant versions of each component, including a novel **VN-LayerNorm** that normalizes 2-norms per channel while preserving vector directions.

For computational efficiency, VN-Transformer introduces **VN-MeanProject** for multi-scale downsampling: computing the mean of all N input tokens (permutation-invariant and rotation-equivariant), then projecting through learnable weights to produce M < N tokens. This reduces self-attention complexity from O(N²C) to O(M²C'), enabling processing of large point clouds.

To handle non-spatial attributes like color or opacity, VN-Transformer supports **early fusion** by extending features from C×3 to C×(3+d_A) matrices, concatenating spatial and non-spatial dimensions. The paper demonstrates that introducing small **ε-approximate equivariance** through bias terms (||bias|| ≤ ε ≈ 10⁻⁶) significantly improves numerical stability on accelerator hardware, with theoretical bounds showing the composition of K approximately-equivariant layers has violation bounded by ε₁...K = L_K(...(L₂ε₁ + ε₂)...) + ε_K where L_k are Lipschitz constants.

On ModelNet40 with non-spatial attributes (polka-dot variant), VN-Transformer with ε=10⁻⁶ achieves **95.4% accuracy** versus 91.1% with strict equivariance (ε=0), demonstrating the value of controlled equivariance relaxation. Implementation is available at github.com/lucidrains/VN-transformer. [github](https://github.com/lucidrains/VN-transformer)

## Mamba4D achieves efficient temporal modeling through disentangled state space models

Mamba4D (CVPR 2025) represents the first purely SSM-based backbone for 4D point cloud understanding, processing dynamic sequences without explicit point correspondence between frames. The architecture disentangles spatial and temporal processing into two stages: **Intra-frame Spatial Mamba** encodes local geometric structures within short-term clips, while **Inter-frame Temporal Mamba** captures long-range dependencies across entire videos. [arxiv](https://arxiv.org/html/2405.14338)

The temporal partitioning strategy selects **anchor frames** via stride-based sampling (temporal stride s_t), then applies Farthest Point Sampling to select n = N/s_s anchor points per frame. KNN grouping around each anchor across k_t temporal frames forms **point tubes**—spatio-temporal neighborhoods that capture local motion patterns without requiring point tracking.

Each Mamba block follows the structure F'_l = DW(Linear(F_{l-1})) and F_l = Linear(SSM(σ(F'_l)) × σ(Linear(F_{l-1}))), where DW is depth-wise convolution, σ is SiLU activation, and SSM is the selective state space model. The discrete SSM equations x_k = Āx_{k-1} + B̄u_k and y_k = Cx_k + Du_k provide **linear complexity O(L)** versus transformers' quadratic O(L²), with parameters A, B, C computed as functions of input (data-dependent/selective). [arxiv](https://arxiv.org/html/2405.14338)

Critical for spatio-temporal understanding is **4D positional encoding** via MLP(x,y,z,t), enabling awareness of both spatial coordinates and temporal frame index. For temporal scanning, Mamba4D introduces **cross-temporal scanning** strategies that scan points with largest coordinate values first across all frames before proceeding to smaller values. The best configuration (XZY, YXZ, or ZYX ordering) achieves **92.68% accuracy** on MSR-Action3D versus 91.36% for sequential scanning.

The efficiency gains are substantial: at 17,920 tokens, Mamba4D requires **~2.6GB GPU memory** versus P4Transformer's ~20.7GB (87.5% reduction) and achieves **~96ms inference** versus ~510ms (5.36× speedup). [github](https://github.com/maklachur/Mamba-in-Computer-Vision) Critically, while P4Transformer accuracy drops from 90.94% at 24 frames to 82.81% at 36 frames, Mamba4D accuracy improves from 92.68% to **93.23%** as sequence length increases. Code is available at github.com/IRMVLab/Mamba4D.

## Spherical harmonics require Wigner D-matrix transformations for equivariant processing

3D Gaussian Splatting uses spherical harmonics up to degree 3 for view-dependent color, yielding **16 coefficients per color channel** (48 total for RGB) organized as 1 (l=0) + 3 (l=1) + 5 (l=2) + 7 (l=3). Under rotation R, these coefficients transform via Wigner D-matrices: Y_l^m(Rx) = Σ_{m'} D^l_{m'm}(R) Y_l^{m'}(x), [Wikipedia](https://en.wikipedia.org/wiki/Wigner_D-matrix) where each degree l has its own (2l+1)×(2l+1) rotation matrix forming a block-diagonal structure across degrees.

The Wigner D-matrix elements are D^l_{m'm}(α,β,γ) = e^{-im'α} d^l_{m'm}(β) e^{-imγ} where (α,β,γ) are Euler angles [Wikipedia](https://en.wikipedia.org/wiki/Wigner_D-matrix) and d^l is the "small" Wigner matrix computed via Wigner's formula involving binomial-like sums. For 3DGS with l_max=3, the total rotation involves **84 matrix elements** (1 + 9 + 25 + 49), though the block-diagonal structure means computation scales as O(l³) rather than O(l_max⁶).

The **e3nn library** provides the standard implementation for equivariant spherical harmonic processing in PyTorch/JAX. Irreducible representations are denoted as "NxLp" where N is multiplicity, L is degree, and p is parity (e/o for even/odd). For 3DGS RGB spherical harmonics, the irrep specification would be "3x0e + 3x1o + 3x2e + 3x3o" representing 48 total coefficients with appropriate parities.

Tensor products between irreps decompose as l₁ ⊗ l₂ = |l₁-l₂| ⊕ ... ⊕ (l₁+l₂), mediated by Clebsch-Gordan coefficients. For example, vector × vector (1⊗1) yields scalar + vector + quadrupole (0⊕1⊕2). The **Gaunt tensor product** formulation (ICLR 2024) reduces complexity from O(L⁶) to O(L³) through Fourier basis transformation, critical for practical implementation.

For Gaussian Splatting classification, two approaches exist. The **invariant approach** extracts band energies Σ_m|c_l^m|² for each degree l, producing 4 invariant scalars per color channel (12 total for RGB). The **equivariant approach** processes full SH coefficients through networks that apply Wigner D-matrix transformations, preserving the angular information but requiring more complex architecture. The invariant approach sacrifices some discriminative power but dramatically simplifies implementation.

## Integration requires careful handling of Gaussian attribute heterogeneity

Gaussian splatting attributes transform heterogeneously under rotation: positions μ transform as μ' = Rμ (type-1 vectors), covariance matrices as Σ' = RΣRᵀ, quaternions via quaternion multiplication, spherical harmonics via degree-specific Wigner D-matrices, and opacity remains invariant (type-0 scalar). Scale vectors require careful interpretation—in the local Gaussian frame they're invariant, but interpreted globally they're type-1.

A recommended **two-stage architecture** separates rotation-invariant feature extraction from temporal processing:

**Stage 1** extracts per-frame rotation-invariant embeddings per Gaussian by computing invariant geometric features from k-NN neighborhoods (pairwise distances, angles), covariance invariants (eigenvalues), spherical harmonic band energies, and raw opacity. These feed through a graph neural network on the Gaussian proximity graph, producing per-Gaussian invariant embeddings aggregated via attention-weighted pooling to frame-level representations.

**Stage 2** performs temporal processing on these invariant frame embeddings. Since Stage 1 outputs are rotation-invariant, any temporal aggregation (transformer attention, SSM, or 4D convolution) automatically preserves invariance. Mamba4D's architecture is particularly suitable given its linear complexity and explicit temporal disentanglement. [arxiv](https://arxiv.org/html/2405.14338v3) The cross-temporal scanning strategy captures motion patterns without requiring point correspondence, matching the frame-to-frame variability inherent in 4D Gaussian representations.

For **end-to-end training**, gradient flow through equivariant layers is well-defined but requires attention to numerical stability—Wigner D-matrix operations can amplify gradients for high degrees, suggesting gradient clipping and careful initialization. Data augmentation should apply consistent rotation across all frames to maintain temporal coherence, transforming positions, covariances, quaternions, and SH coefficients appropriately while leaving opacity unchanged.

Verification is critical: testing that model(R·x) ≈ model(x) for random rotations confirms rotation invariance. For equivariant intermediate representations, verify model(R·x) ≈ R·model(x). The VN-Transformer paper demonstrates that ε-approximate equivariance with ε ≈ 10⁻⁶ provides the best accuracy-stability tradeoff.

## Conclusion: architectural synthesis enables practical rotation-robust 4DGS classification

The mathematical machinery for rotation-robust 4D Gaussian Splatting classification is now mature, combining Vector Neurons' elegant SO(3)-equivariance through dimensional lifting, VN-Transformer's parameter-efficient attention mechanism via Frobenius inner products, Mamba4D's scalable temporal processing through disentangled state space models, and Wigner D-matrix transformations for spherical harmonic coefficients.

The key architectural insight is that **invariance should be established early** in the pipeline—extracting rotation-invariant features per Gaussian before temporal aggregation eliminates the complexity of maintaining equivariance through recurrent or attention-based temporal processing. This two-stage approach trades potential discriminative power from full equivariant processing for practical implementation simplicity and training stability.

For implementations, the critical resources are the Vector Neurons codebase (github.com/FlyingGiraffe/vnn), VN-Transformer (github.com/lucidrains/VN-transformer), Mamba4D (github.com/IRMVLab/Mamba4D), and e3nn for spherical harmonic processing (github.com/e3nn/e3nn). Benchmark validation should proceed on ModelNet40 for static classification, MSR-Action3D for action recognition, and the emerging 3DGS classification datasets from recent work before tackling full 4DGS sequences.

The most promising unexplored direction is applying **ε-approximate equivariance** from VN-Transformer to the full pipeline—accepting small, bounded equivariance violations for substantial gains in numerical stability and accuracy. The VN-Transformer results showing 95.4% versus 91.1% accuracy with ε=10⁻⁶ versus strict equivariance suggest this controlled relaxation could similarly benefit 4DGS classification where the heterogeneous attribute types create multiple potential sources of numerical instability.