Processed,Name,Components,Recommendations,Problem or Purpose,My Comments,Theoretical/Conceptual,Sample,Methods,Key Findings,github,Cover,Related References,URL,Zotero URI,DOI,Edited
Yes,VGGT: Visual Geometry Grounded Transformer (2025),"2D features, 3D features, 3D-fusion, explain, pose, track","• Use VGGT to self-calibrate extrinsics online.
• Fuse depth+pose into occupancy voxels.
• Use tracking features for multi-view TTD association.
• Weight fusion by predicted uncertainty maps.
• Finetune on Isaac Sim flying objects.
• Add BA refinement for drift robustness.","• Feed-forward multi-view 3D reconstruction without optimization.
• Predict cameras, depth, point maps, tracks jointly.
• Handle one to hundreds of views efficiently.
• Provide backbone features for downstream 3D tasks.
• Outperform optimization-based SfM/MVS baselines.","• Strong multi-view RGB fit; no LiDAR required.
• Lacks classification; add separate temporal classifier.
• Heavy model; edge inference likely infeasible.
• Rolling-shutter, fast aerial motion unaddressed.
• Tracks 2D; extend to 3D MOT state.
• Potential backbone for Gaussian splatting inputs.","• Alternating frame-wise and global self-attention transformer.
• Over-complete supervision: camera, depth, point maps simultaneously.
• Coordinate normalization in first-camera world frame.
• Camera loss: Huber; depth/point: aleatoric uncertainty.
• Gradient consistency term for depth and point maps.
• PnP recoverability, Umeyama alignment for evaluation.","• Domains: indoor, outdoor, synthetic, phototourism.
• Training frames per scene: 2–24; accepts hundreds.
• #cameras/views: many images; overlap variable.
• Annotations: cameras, depth, point maps, tracks.
• 3D motion: mostly static; dynamic tracking finetuned.
• Availability: code/models public; license N/A.","• DINOv2 patchify; AA transformer processes all tokens.
• Camera tokens; DPT head predicts depth, points, features.
• Camera head regresses intrinsics/extrinsics feed-forward.
• Tracking via CoTracker2 using dense features.
• Multi-task losses: Huber, aleatoric, gradient terms.
• Trained 160k iters; 64 A100; bfloat16; checkpointing.","• Pose AUC@30: 85.3 Re10K, 88.2 CO3Dv2.
• ETH3D point Overall: 0.677 via depth+cam.
• DTU Overall: 0.382 without known cameras.
• ScanNet-1500 matching beats Roma across thresholds.
• Runtime: 0.2s/10 frames; scales to 200 frames.
• Memory: 1.9–40.6GB; model 1.2B parameters.",https://github.com/facebookresearch/vggt,,,http://arxiv.org/abs/2503.11651,https://zotero.org/alehanderoo/items/28YDSSK4,https://doi.org/10.48550/arXiv.2503.11651,12/11/2025 17:36 (GMT+1)
Yes,DUSt3R: Geometric 3D Vision Made Easy,"2D features, 3D features, 3D-fusion, pose","• Use for online extrinsics/focal self-calibration from RGB.
• Use pointmaps to seed 3D occupancy/volume priors.
• Apply global alignment for multi-cam fusion without LiDAR.
• Weight ray-voting using confidence maps.
• Finetune on Isaac Sim pairs for domain/objects.
• Beware scale ambiguity; enforce consistent scaling.","• Uncalibrated multi-view 3D reconstruction from RGB only.
• Replace brittle SfM/MVS pipelines with end-to-end model.
• Predict geometry without intrinsics/poses; unify mono/binocular.
• Provide global alignment for multi-image consistency.
• Recover cameras, depth, matches from pairwise pointmaps.","• Strong fit for calibration-drift, multi-view RGB-only.
• No classification/tracking; static-scene bias.
• Might aid track-before-detect via 3D matches.
• Pointmaps compatible with Gaussian-splat init seeds.
• Unclear performance on small fast flying targets.
• Rolling-shutter/dynamics robustness not evaluated.","• Pointmaps: dense 2D fields of 3D points.
• Confidence-weighted 3D regression loss with scale normalization.
• Transformer encoders/decoders with cross-attention.
• Global alignment minimizes 3D errors, not reprojection.
• Intrinsics via Weiszfeld-like focal optimization.
• Pose via Procrustes or PnP-RANSAC on correspondences.","• Mixed indoor/outdoor, synthetic/real general scenes.
• 8.5M training image pairs extracted.
• Pairwise inputs; multi-view via connectivity graph; variable overlap.
• Annotations: depth, poses from SfM/sensors/simulation.
• 3D motion presence: N/A.
• Public datasets; code open-source.","• Siamese ViT-L encoders; transformer decoders exchanging tokens.
• Heads regress pointmaps and per-pixel confidences.
• Calibration-free; optional intrinsics recovery from pointmaps.
• 3D fusion by global alignment with pairwise scales.
• Correspondences via 3D mutual NN; pose via PnP/Procrustes.
• Training: CroCo pretrain, 224→512 resolution, 3D losses.","• SoTA map-free localization VCRE AUC, precision improvements.
• Strong relative pose mAA on CO3Dv2, RealEstate10K.
• Zero-shot monocular/multi-view depth competitive or SoTA.
• DTU recon overall ~1.7mm; less than SoTA accuracy.
• Inference ~25ms/pair on H100; GA converges in seconds.
• Faster than COLMAP on multi-view depth tasks.",https://github.com/naver/dust3r,,,http://arxiv.org/abs/2312.14132,https://zotero.org/alehanderoo/items/4SLWXPX8,https://doi.org/10.48550/arXiv.2312.14132,12/11/2025 17:34 (GMT+1)
Yes,MUSt3R: Multi-view Network for Stereo 3D Reconstruction,"2D features, 3D features, 3D-fusion, pose","• Use memory-based multi-view fusion backbone.
• Leverage Procrustes pose for self-calibration drift.
• Adapt pointmaps to multi-camera global frame.
• Replace scene-centric assumptions with object-centric masks.
• Quick-win: test calibration refinement on Isaac Sim.
• Risk: struggles with large viewpoint changes.","• Scale pairwise DUSt3R to N-view directly.
• Reduce quadratic pairwise complexity for large collections.
• Enable real-time uncalibrated VO and SLAM.
• Predict global pointmaps without global alignment.
• Improve robustness, speed, and memory usage.","• Strong RGB-only self-calibration aligns with thesis.
• Not designed for multi-camera simultaneous capture.
• No occupancy, classification, or MOT modules.
• Pointmaps could seed occupancy voxelization.
• May fail on fast, tiny flying objects.
• Rolling-shutter handling unclear; needs verification.","• Pointmap representation unifies geometry and intrinsics.
• Symmetric Siamese decoder with shared weights.
• Multi-view cross-attention across all other views.
• Iterative memory as causal KV cache.
• Global 3D feedback module INJ3D.
• Procrustes alignment for relative pose estimation.","• Indoor/outdoor scenes, diverse datasets.
• Training: 12–14 datasets mixed.
• Evaluation: 7Scenes, DTU, TUM, ETH3D.
• Single moving RGB camera, sequential overlap.
• Annotations: depth, poses, focal lengths.
• Mostly static scenes; some dynamic sequences.","• ViT encoder; shared multi-view transformer decoder.
• Cross-attention across all other views.
• Iterative memory for causal online processing.
• Global 3D feedback (INJ3D) across layers.
• Dual heads output X_i,1 and X_i,i.
• Procrustes pose; uncalibrated focal from pointmaps.","• SOTA uncalibrated VO on TUM, ETH3D.
• 4.5 cm average ATE on TUM.
• FoV error ~4° average, better than Spann3R.
• FPS 8–12 on V100/A100; causal 11.7.
• GPU memory ~4–8 GB at 224/512.
• mAA@30 84.1 on CO3Dv2 (512).",,,,http://arxiv.org/abs/2503.01661,https://zotero.org/alehanderoo/items/XSSQAJ7C,https://doi.org/10.48550/arXiv.2503.01661,12/11/2025 17:33 (GMT+1)
Yes,VGGSfM: Visual Geometry Grounded Deep Structure from Motion (2024),"2D features, 3D-fusion, pose, track","• Use tracker for multi-cam track-before-detect cues.
• Employ differentiable BA for online extrinsic refinement.
• Integrate uncertainty-weighted reprojection for robust association.
• Replace DLT with Gaussian-splat triangulation post-process.
• Validate on Isaac Sim flying-object sequences.
• Beware static-scene assumption and scalability limits.","• End-to-end differentiable SfM from unordered RGB images.
• Replace incremental, non-differentiable SfM components.
• Jointly estimate all cameras and 3D points.
• Leverage deep 2D tracking, avoid match chaining.
• Achieve SOTA on public SfM benchmarks.","• Strong pose/self-calibration relevance for multi-view rigs.
• No occupancy, classification, or MOT; mismatch.
• Unordered multi-view aligns with overlapping cameras.
• Lacks dynamic handling; flying objects challenging.
• Uncertainty outputs beneficial for robust voting.
• Need runtime/FPS, memory for edge feasibility.","• Differentiable reconstruction function f_theta trained end-to-end.
• Transformer camera initializer and triangulator modules.
• Coarse-to-fine tracker with aleatoric uncertainty.
• Differentiable LM bundle adjustment via Theseus.
• Reprojection loss with visibility/confidence filtering.
• Pseudo-Huber losses; Gaussian NLL for tracks.","• Domains: landmarks, objects, indoor/outdoor scenes.
• Datasets: Co3Dv2, IMC Phototourism, ETH3D.
• Frames per batch: 3–30 training frames.
• Cameras: unordered multi-view; varying overlap.
• Annotations: GT cameras, tracks, point clouds.
• Dynamic 3D motion: generally static scenes.","• Pipeline: tracking, camera init, triangulation, BA refinement.
• Pose: joint global estimation from images and tracks.
• Intrinsics: learn focal length; principal point fixed center.
• 2D backbones: ResNet50 features; Transformer modules.
• 3D fusion: DLT triangulation; LM BA with Theseus.
• Training: staged then joint; pseudo-Huber, reprojection, Gaussian NLL.","• CO3D RRE@15: 92.1, RTE@15: 88.3.
• CO3D AUC@30: 74.0, best among compared baselines.
• IMC AUC@10: 73.92, AUC@5: 58.89 best.
• IMC AUC@3: 45.23, second to DFSfM.
• ETH3D Acc@5cm: 96.52, Comp@5cm: 33.96 best.
• Runtime and memory usage: N/A.",https://vggsfm.github.io,,,https://ieeexplore.ieee.org/document/10655838/,https://zotero.org/alehanderoo/items/C6NKCGMB,https://doi.org/10.1109/CVPR52733.2024.02049,12/11/2025 17:30 (GMT+1)
Yes,Grounding Image Matching in 3D with MASt3R (2024),"2D features, 3D features, pose","• Use MASt3R correspondences for online extrinsic refinement.
• Integrate fast reciprocal matcher into ray-voting.
• Triangulate multi-cam foreground seeds from matches.
• Apply coarse-to-fine windows for high-resolution inputs.
• Stress-test under calibration drift and rolling shutter.
• Avoid depth at inference; keep RGB-only pipeline.","• Cast matching as 3D, not 2D.
• Improve DUSt3R accuracy while preserving robustness.
• Produce dense, pixel-accurate correspondences under viewpoint changes.
• Reduce quadratic cost of dense reciprocal matching.
• Enable standalone calibration and pose from RGB pairs.","• Excellent for RGB-only self-calibration in multi-cam rigs.
• Assumes static scenes; moving flying objects challenging.
• No temporal modeling, tracking, or classification.
• Pairwise design; multi-view fusion needs extension.
• Adopt fast reciprocal matching for efficiency and robustness.
• Runtime on edge unknown; ViT likely heavy.","• Pointmap regression with confidence-aware loss.
• InfoNCE loss enforces one-to-one descriptor matches.
• Fast reciprocal nearest-neighbor matching, O(kWH) complexity.
• Coarse-to-fine matching using overlapping window crops.
• Metric depth training variant removes scale ambiguity.
• Pose via essential matrix; epipolar geometry grounded.","• Mixed datasets: Habitat, CO3D-v2, Waymo, RealEstate.
• 14 datasets; indoor/outdoor; real and synthetic.
• Binocular, uncalibrated RGB image pairs.
• Annotations: pointmaps, depths, camera poses.
• Extreme baselines; illumination changes; occlusions.
• Code public; dataset licenses per source.","• ViT-L encoder, ViT-B decoder with cross-attention.
• Heads: 3D pointmaps, confidences, dense descriptors.
• RGB-only inference; no LiDAR or depth required.
• FAISS/KD-tree nearest neighbors; fast reciprocal subsampling.
• Coarse-to-fine windows; greedy coverage of coarse matches.
• Pose from essential matrix or PnP; self-calibration.","• Map-free VCRE AUC 0.933; 30% absolute improvement.
• Median translation 0.36m; rotation 2.2 degrees.
• 64x matching speedup with k=3000 subsampling.
• Outperforms DUSt3R, LoFTR on localization benchmarks.
• Improved mAA on RealEstate; strong relative pose.
• Zero-shot MVS competitive without calibration.",https://github.com/naver/mast3r,,,http://arxiv.org/abs/2406.09756,https://zotero.org/alehanderoo/items/6UZXICP4,https://doi.org/10.48550/arXiv.2406.09756,12/11/2025 17:20 (GMT+1)
Yes,Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors (2025),"2D features, 3D features, 3D-fusion, pose","• Use intrinsics-guided crops for high-res small-object detail
• Adopt Procrustes for fast pairwise extrinsics
• Exploit confidence maps for multi-cam fusion weighting
• Use global alignment to refine drifting extrinsics
• Seed occupancy from fused pointmaps, RGB-only
• Benchmark Isaac Sim with drifting extrinsics scenarios","• Unify 3D regression with optional priors at inference
• Leverage intrinsics/poses/depth when available, unlike DUSt3R
• Enable high-res processing via crop intrinsics guidance
• Improve depth, focal, pose jointly with one model
• Provide faster relative pose estimation from pointmaps","• Strong multi-view geometry; no dynamics or tracking
• Useful for self-calibration, not occupancy directly
• Confidence-aware guidance robust to bad priors; valuable
• Pairwise design scales to multi-cam via batching
• Consider adding motion modeling for flying objects
• Open: rolling-shutter robustness, multi-view >2 natively","• Pointmap regression with scale-invariant normalization
• Confidence-aware loss with alpha=0.2
• Procrustes alignment for relative pose estimation
• Auxiliary modality embeddings via MLP injection
• Ray-based intrinsics encoding as dense tokens","• Static indoor/outdoor scenes across eight datasets
• 8.5M training image pairs total
• Two cameras per pair; overlapping views
• Annotations: depth, intrinsics, relative pose
• No 3D motion; static-scene assumption
• Model weights via project webpage; datasets public","• Shared ViT encoder; dual decoders with cross-attention
• Inject intrinsics/depth in encoder; pose in decoder
• Predict X1,1, X2,1, X2,2 with confidences
• Global alignment fuses pairwise predictions multi-view
• Scale-invariant pointmap loss; confidence regularization
• Train 224px then 512px; 8 A100; 5 days","• Outperforms DUSt3R when priors provided
• High-res sliding-window boosts depth accuracy
• Depth completion SOTA across sparsity on NYUv2
• Procrustes pose: 30.9 fps vs 3.2 PnP
• Inference ~0.13s per pair at 512px
• K+RT synergy; intrinsics most beneficial",,,,http://arxiv.org/abs/2503.17316,https://zotero.org/alehanderoo/items/KVXDPAQL,https://doi.org/10.48550/arXiv.2503.17316,12/11/2025 17:10 (GMT+1)
Yes,Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass (2025),"2D features, 3D features, 3D-fusion, pose","• Use Fast3R for online extrinsics/self-calibration.
• Seed Gaussian Splatting + BA for pose refinement.
• Use confidence-masked pointmaps for occupancy fusion.
• Test 5-camera rig; 512p; per-batch RANSAC-PnP.
• Finetune on Isaac Sim dynamic flights for 4D cues.
• Avoid optical-flow; rely on multi-view geometry cues.","• Efficient multi-view 3D reconstruction from many RGB views.
• Avoid O(N^2) pairwise DUSt3R alignment cost.
• Reduce error accumulation from pairwise processing bottlenecks.
• Single forward pass for 1000+ unordered, unposed images.
• Improve pose/reconstruction speed without accuracy loss.","• Strong multi-view fusion; no classification/tracking modules.
• RGB-only fits inference constraints; no LiDAR needed.
• Static-scene assumption limits flying-object tracking directly.
• Good for calibration drift via per-batch pose.
• Outputs suit Gaussian Splatting; occupancy via voxelization.
• No rolling-shutter compensation; check for fast motion.","• Pointmap regression with local/global coordinate frames.
• Confidence-weighted normalized L2 pointmap loss.
• All-to-all self-attention across views for fusion.
• Image index positional embeddings with interpolation.
• No known intrinsics; poses via RANSAC-PnP.
• Confidence maps guide robust supervision/inference.","• Training on CO3D, ScanNet++, ARKitScenes, Habitat.
• Also BlendedMVS, MegaDepth; mixed indoor/outdoor.
• Unposed, unordered multi-view images; N up to 1000+.
• Annotations: scans/poses; derived pointmaps as supervision.
• 3D motion only in 4D finetuning experiments.
• Project page; dataset licenses apply.","• ViT-L encoder per image; CroCo/DINOv2 weights.
• Fusion transformer with all-to-all attention across views.
• DPT decoders predict local/global pointmaps, confidences.
• Poses via RANSAC-PnP using top-confidence points.
• Local pointmaps aligned to global via ICP.
• Training: 512p, AdamW, FlashAttention, ZeRO, N=20.","• CO3D RRA@15: 99.7%, mAA30 up to 82.5.
• 251 FPS at 224p, 108 views.
• Processes 1500 views; DUSt3R OOM past 32.
• 14× error reduction vs DUSt3R alignment.
• 320× faster than DUSt3R; 1000× than MASt3R.
• Peak 63 GiB at 1000 views on A100.",,,,http://arxiv.org/abs/2501.13928,https://zotero.org/alehanderoo/items/FVPYIQY9,https://doi.org/10.48550/arXiv.2501.13928,12/11/2025 17:06 (GMT+1)
Yes,Generative Sparse-View Gaussian Splatting,"2D features, 3D features, 3D-fusion, pose","• Adopt geometry-aware pseudo-view hallucination for sparsity
• Add MS-SSIM depth reg to GS training
• Integrate LoRA depth adapter for priors
• Leverage 4DGS backbone for dynamic targets
• Risk: needs accurate poses; no drift handling
• Quick win: 3-view augmentation then occupancy extraction","• Sparse-view GS struggles with limited constraints
• Improve 3D/4DGS from few input views
• Generative methods lack multi-view consistency
• Regularization alone insufficient for sparse views
• Propose diffusion-guided pseudo-view augmentation","• Good fit for sparse multi-view reconstruction
• Lacks occupancy, classification, and MOT
• Assumes fixed calibration; no self-calibration
• Hallucinations may distort small flying objects
• Useful to densify before 3D MOT
• Verify runtime and edge feasibility","• Bi-level alternating optimization of GS and diffusion
• Stable Diffusion LDM loss L_LDM objective
• Geometry-aware diffusion feature consistency loss
• 3DGS alpha compositing rendering equation
• Depth MS-SSIM regularization on normalized depths
• LoRA with depth T2I-Adapter conditioning","• Datasets: Blender, LLFF, Mip-NeRF360, Neural 3D Video
• Blender: 8 objects, 8 train, 25 test
• LLFF: 8 scenes, 3 training views
• Mip-NeRF360: 9 outdoor scenes, 24 training views
• Neural 3D Video: 6 seqs, 18–21 cameras
• Annotations: RGB only; dynamic motion present; public","• Init GS from SfM using sparse views
• Depth-conditioned diffusion generates pseudo-view images
• Warp pseudo renders to train views via extrinsics
• Match diffusion features across warped pairs
• Optimize GS with L1 and D-SSIM losses
• Add depth MS-SSIM reg; 10k iterations","• Best Blender PSNR 28.57, +3 dB over FSGS
• LLFF: 24.82 PSNR, 0.737 SSIM, 0.105 LPIPS
• Mip-NeRF360: 25.87 PSNR, best across metrics
• Dynamic: +12 dB vs SpacetimeGS at 3 views
• Ablations: geometry loss and depth reg effective
• Efficiency claimed; FPS not reported",,,,,https://zotero.org/alehanderoo/items/YZQDV4VP,,12/11/2025 17:03 (GMT+1)
Yes,MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting (2024),"2D features, 3D features, 3D-fusion, pose","• Adopt motion/camera flow decoupling for motion guidance.
• Implement Gaussian-flow rendering for supervision.
• Use alternating pose refinement for drift correction.
• Leverage dynamic masks to suppress static regions.
• Test self-supervised flow loss when flow unreliable.
• Evaluate multi-cam extension using synchronized frames.","• Dynamic 3DGS lacks explicit motion guidance.
• Optical flow conflates camera and object motion.
• Inaccurate poses degrade dynamic reconstruction quality.
• Aim: decouple flow, refine poses, guide deformation.","• Strong 4DGS motion supervision; no occupancy modeling.
• Monocular focus; lacks multi-view fusion.
• No classification/tracking; add downstream MOT.
• Pose refinement aligns with self-calibration needs.
• Flow reliance may miss small, fast flyers.
• Potential track-before-detect via motion flow masks.","• Decouple optical flow: camera flow + motion flow.
• Gaussian flow matches motion flow at fixed viewpoint.
• Photometric rendering loss supervises image fidelity.
• L1 motion-vs-Gaussian flow loss.
• SE(3) residual pose optimization via gradients.","• Monocular dynamic videos; tabletop and handheld scenes.
• NeRF-DS: eight stereo sequences, 480×270.
• HyperNeRF vrig: four scenes, 536×960 downsampled.
• Train left camera; test right camera on NeRF-DS.
• No GT flow/poses; optional dynamic masks.
• Datasets Apache-2.0; public downloads.","• COLMAP initializes Gaussians and camera poses.
• GMFlow estimates optical flow between adjacent frames.
• Render 3DGS depth; compute camera flow by reprojection.
• Motion flow = optical flow − camera flow.
• CUDA Gaussian flow supervises deformation via L1 loss.
• Alternate SE(3) pose refinement with frozen Gaussians.","• Outperforms baselines on NeRF-DS PSNR/SSIM.
• HyperNeRF mean PSNR 24.8 vs 22.5 baseline.
• Motion flow beats raw optical flow supervision.
• Pose refinement further boosts metrics.
• Real-time rendering; many scenes >30 FPS.
• Training slower; peak memory up to ~18 GB.",,,,http://arxiv.org/abs/2410.07707,https://zotero.org/alehanderoo/items/VXYM6EKS,https://doi.org/10.48550/arXiv.2410.07707,12/11/2025 16:54 (GMT+1)
Yes,Object-centric Reconstruction and Tracking of Dynamic Unknown Objects using 3D Gaussian Splatting (2024),"3D features, 3D-fusion, pose, track","• Reuse object-centric 3DGS for dynamic object modeling.
• Replace depth losses with multi-view RGB photometrics.
• Integrate multi-cam keyframes and pose-graph bundle adjust.
• Add track-before-detect foreground ray-voting for initialization.
• Self-calibrate extrinsics from motion to reduce drift.
• Extend to 4D Gaussians for classification cues.","• Incremental reconstruction of unknown dynamic objects.
• Simultaneous 6-DoF pose tracking without templates.
• Unified object-centric 3D Gaussian representation.
• Avoids pretraining and prior object knowledge.
• Addresses slow, low-fidelity, multi-representation pipelines.","• Strong fit: object-centric 3DGS, dynamic unknown targets.
• Mismatch: requires RGB-D inference; single-camera only.
• No classification, occupancy, or multi-object tracking.
• Drift highlights need for global multi-view constraints.
• Suggest SH for specularities; rolling-shutter unaddressed.
• Edge feasibility limited by seconds-per-frame runtime.","• Differentiable 3D Gaussian splatting rasterization.
• Locally affine perspective projection approximation.
• Alpha compositing approximates volumetric rendering.
• Photometric L1 and SSIM color losses.
• Depth L1 for reconstruction and tracking.
• Adam optimization with alternating objectives.","• Domain: proximity operations with spacecraft.
• Ten ESA science fleet models.
• 1000 frames per sequence.
• Single moving RGB-D camera, spherical spiral trajectory.
• Annotations: GT poses, depth, segmentations.
• Synthetic, Isaac Sim; release status unspecified.","• Object-centric 3D Gaussians with color, opacity.
• Differentiable rasterization for color and depth.
• Alternate reconstruction and per-frame pose optimization.
• No 2D backbones or optical flow.
• Loss: L1+SSIM color; L1 depth; keyframe window.
• Adam; 120 recon, 80 track steps; add/prune Gaussians.","• Improving chamfer distance across views for most objects.
• Accurate tracking short-term: <0.5m, <10° for 6/10.
• Long-term drift in 6/10 sequences.
• Runtime: 1.4s recon, 1.1s tracking per frame.
• RTX 3080 Ti Laptop; not real-time.
• Failure modes: dark regions, view-dependent specularity.",,,,http://arxiv.org/abs/2405.20104,https://zotero.org/alehanderoo/items/BE6FVN5S,https://doi.org/10.1109/iSpaRo60631.2024.10688304,12/11/2025 16:53 (GMT+1)
Yes,Multi-View 3D Point Tracking (2025),"2D features, 3D features, 3D-fusion, track","• Adopt fused 3D feature cloud for multi-view fusion.
• Reuse kNN 3D correlation for track hypotheses.
• Integrate visibility head for occlusion-aware 3D MOT.
• Replace depth with multi-cam ray-voting occupancy cues.
• Add online extrinsic refinement from tracked points.
• Benchmark on Isaac Sim drones, RGB-only constraints.","• Track arbitrary 3D points across multi-view videos.
• Overcome monocular depth ambiguity and occlusions.
• Enable feed-forward online tracking without optimization.
• Operate with few cameras, flexible configurations.
• Fuse multi-view features into unified 3D space.","• Strong fit for multi-view 3D tracking needs.
• No detection; requires query seeds or foreground cues.
• Depth requirement conflicts with RGB-only inference.
• No classification or Gaussian splatting support.
• No calibration drift handling; add self-calibration.
• May struggle outdoors, long-range, rolling-shutter.","• Fused 3D feature point cloud from multi-view depth.
• kNN correlation with explicit 3D offset vectors.
• Spatiotemporal transformer with sliding-window refinement.
• Weighted L1 position loss across iterations (gamma).
• Balanced BCE loss for occlusion visibility prediction.
• Assumes known intrinsics/extrinsics; requires depth estimates.","• MV-Kubric synthetic: 5K multi-view sequences.
• Panoptic Studio: 6 scenes; DexYCB: 10 scenes.
• Inputs: 1–8 cameras; varying baselines; overlapping views.
• Annotations: 3D trajectories and visibilities, derived from poses.
• Dynamic and static motion present in scenes.
• Project page and datasets released; license N/A.","• CNN encoder extracts multi-scale per-view features.
• Lift via depth; fuse into 3D feature cloud.
• kNN correlation with feature similarity and 3D offsets.
• Transformer iteratively updates tracks within sliding windows.
• Assumes known camera poses; no self-calibration.
• Training: 200K steps, AdamW, d=128, 8×GH200.","• Outperforms baselines on AJ, δavg, MTE.
• Panoptic: AJ 86.0, MTE 3.1 cm.
• DexYCB: AJ 71.6, MTE 2.0 cm.
• Runs 7.2 FPS excluding depth estimation.
• Performance scales with more input views.
• Offset-only correlation variant performs best.",https://ethz-vlg.github.io/mvtracker,,,http://arxiv.org/abs/2508.21060,https://zotero.org/alehanderoo/items/3S9F846C,https://doi.org/10.48550/arXiv.2508.21060,12/11/2025 16:49 (GMT+1)
Yes,Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization (2025),"2D features, 3D features, 3D-fusion, explain, pose","• Adopt primitive-initialized object-centric 3DGS module.
• Replace point-cloud alignment with multi-view pose filter.
• Incorporate online extrinsic refinement from motion.
• Test robustness to calibration drift and occlusions.
• Simulate drones in Isaac; evaluate convergence speed.
• Avoid depth reliance; keep RGB-only inference.","• Fast monocular 3D modeling of noncooperative spacecraft.
• Reduce 3DGS iterations and image count substantially.
• Train with noisy or implicit pose estimates.
• Enable sequential NVS without batch SfM.
• Compare primitive-initialized CNN pose variants.","• Strong fit: RGB-only, flying rigid targets, 3DGS.
• Mismatch: no multi-camera overlap or tracking.
• No classification or occupancy representation.
• Sequential training insights transferable to multi-cam.
• Primitives improve explainability and initialization.
• Edge feasibility on space-grade hardware unquantified.","• Superquadric primitive assemblies as neural shape prior.
• 3DGS rendering with L1 and SSIM loss.
• Permutation-invariant, ambiguity-aware pose training.
• Chamfer distance for shape and rotation alignment.
• Sequential per-image optimization without batch COLMAP.
• Spherical harmonics for view-dependent color.","• Domain: synthetic spacecraft close-range imagery.
• 64 satellites, 64k images total.
• Single monocular camera; no multi-view overlap.
• Labels: masks, poses; no depth at inference.
• 3D motion: static targets, camera viewpoint changes.
• SPE3R dataset; public; license N/A.","• CNN predicts superquadrics and 6-DoF pose.
• Pose variants: original, ambiguity-aware, ambiguity-free.
• Initialize 3DGS from sampled primitive surface points.
• Sequential 3DGS: five steps per image, then discard.
• Losses: reprojection, chamfer, L1, SSIM; SH scheduling.
• Ambiguity-free uses point-cloud alignment for rotation.","• CNN init improves PSNR ~1 dB over random.
• LPIPS reduced; faster convergence to 1.5× best.
• Init overhead ~0.2–0.33 s per model.
• 1,500 iterations with 300 images sufficient.
• Ambiguity-free poses best under estimated poses.
• Random init often misses solar panels.",,,,http://arxiv.org/abs/2507.19459,https://zotero.org/alehanderoo/items/IRLMKWHD,https://doi.org/10.48550/arXiv.2507.19459,12/11/2025 16:46 (GMT+1)
Yes,Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis (2023),"3D features, 3D-fusion, explain, pose, track","• Adopt dynamic 3D Gaussians for 4D splatting module.
• Replace depth init with multi-cam differencing seeding.
• Add online extrinsic refinement to handle drift.
• Cluster Gaussians for object-centric 3D MOT.
• Derive class cues from temporal Gaussian parameter changes.
• Evaluate track-before-detect via multi-view residual ray-voting.","• Unify dynamic view synthesis and dense 6-DOF tracking.
• Model dynamic scenes with persistent, moving 3D Gaussians.
• Eliminate input correspondences like flow or keypoints.
• Enable real-time rendering with fast test-time optimization.
• Leverage multi-view synchronized cameras for metric tracking.
• Address lack of long-term correspondences in dynamic NeRFs.","• Strong fit for multi-view 4D Gaussian tracking.
• No classification; requires object-centric layering additions.
• Assumes fixed calibration; drift unaddressed.
• Initialization used depth; replace with RGB-only seeding.
• Scenes are humans; limited flying-object evaluation.
• Explainable motion via physical priors and rotations.","• Analysis-by-synthesis with differentiable Gaussian splatting renderer.
• Persistent attributes; positions and rotations vary over time.
• Local rigidity prior L_rigid on KNN motion.
• Rotation similarity loss L_rot on quaternion velocities.
• Long-term isometry loss preserving neighbor distances.
• Max volume rendering; Jacobian-projected covariances; quaternion rotations.","• Indoor multi-view sports actions in Panoptic Studio.
• Six sequences, 150 frames each at 30 FPS.
• 31 cameras; 27 train, 4 test; hemispherical layout.
• Accurate intrinsics/extrinsics; synchronized; high overlap.
• GT: 21 3D trajectories; 371 2D tracks; pseudo masks.
• Availability/license: N/A.","• Initialize static frame; densify; then per-timestep motion optimization.
• Known calibration; no self-calibration; no correspondences used.
• No optical flow or 2D features; RGB-only.
• Differentiable 3D Gaussian splatting for multi-view fusion.
• Foreground masks via frame differencing; camera color calibration.
• Losses: L_rigid, L_rot, L_iso; Adam; forward propagation.","• PSNR 28.7; SSIM 0.91; LPIPS 0.17 on PanopticSports.
• 3D MTE 2.21 cm; 100% survival; δ 71.4.
• 2D MTE 1.57 px; δ 78.4; 100% survival.
• Outperforms 3GS-O and PIPs by large margins.
• Renders at 850 FPS; train 2 hours/150 timesteps.
• Ablations: L_rigid and masks critical for accuracy.",,,,http://arxiv.org/abs/2308.09713,https://zotero.org/alehanderoo/items/Z6BWJ6C2,https://doi.org/10.48550/arXiv.2308.09713,12/11/2025 16:45 (GMT+1)
Yes,InstantSplat: Sparse-view Gaussian Splatting in Seconds (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse GauBA for online multi-camera extrinsics refinement
• Adopt co-visibility pruning for multi-view redundancy control
• Initialize Gaussians with MVS priors over overlapping cameras
• Stress-test under extrinsic drift and focal perturbations
• Integrate confidence-aware scheduling for unstable regions
• Validate RGB-only, no-LiDAR, dynamic scenes separately","• Sparse-view 3D reconstruction without SfM
• Jointly optimize scene and camera poses
• Reduce ADC sensitivity and pose errors
• Handle unposed images and focal uncertainty
• Achieve seconds-level reconstruction speed","• Strong for RGB-only self-calibration; static-scene assumption conflicts
• No classification, tracking, or occupancy outputs
• Multi-camera overlap aligns with central volume setup
• Photometric optimization may fail with moving objects
• Consider dynamic splatting or scene-flow for motion
• Unclear rolling-shutter handling; verify robustness","• Gaussian Bundle Adjustment via photometric reprojection error
• Self-supervised RGB-only loss; no LiDAR
• Co-visibility pruning and focal averaging stabilize optimization
• Confidence-aware per-point learning rate scheduling
• Differentiable splatting with SH color basis","• Tanks and Temples; MVImgNet; outdoor scenes
• Sparse training: 3/6/12 images per scene
• Twelve uniformly sampled test images
• Overlapping multi-view RGB; unposed inputs
• Self-supervised; no manual annotations
• Dynamics not targeted; mostly static scenes","• MASt3R MVS prior for pixel-aligned point maps
• Global alignment with focal averaging across views
• Co-visibility redundancy pruning and expansion
• Gaussian Bundle Adjustment optimizing Gaussians and poses
• Confidence-aware per-point learning rate scheduling
• 200–1000 iters; A100; CuPy-accelerated initialization","• SSIM 0.3755→0.7624 (3-view) vs COLMAP+3DGS
• Training time 7.5s (3-view) Ours-S
• ATE 0.0191 (3-view), outperforming baselines
• Ablations: GauBA boosts SSIM 0.8553→0.8822
• Rendering FPS up to 181 after pruning
• Outperforms pose-free baselines across datasets",https://instantsplat.github.io,,,http://arxiv.org/abs/2403.20309,https://zotero.org/alehanderoo/items/68KEFIGB,https://doi.org/10.48550/arXiv.2403.20309,12/11/2025 16:43 (GMT+1)
Yes,Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs (2024),"2D features, 3D features, 3D-fusion, pose","• Use pose-free per-pixel 3D-GS initialization module.
• Integrate frustum-based loss masking for occlusions.
• Combine with MASt3R pose regression for drift.
• Extend encoder to ≥3 overlapping RGB cameras.
• Gate splats with track-before-detect foreground votes.
• Validate on Isaac Sim flying objects.","• Pose-free 3D-GS from uncalibrated stereo pairs.
• Avoid intrinsics/extrinsics and depth at inference.
• Mitigate local minima in sparse-view GS.
• Enable extrapolated views via loss masking.","• Strong backbone for RGB-only, unposed reconstruction.
• Lacks dynamics, classification, or tracking modules.
• Stereo-only; multi-cam scalability unverified.
• No optical flow or epi-geometry exploitation.
• Constant color splats; limited appearance modeling.
• Useful for calibration-drift robustness strategy.","• Build on MASt3R pointmap regression.
• Predict full 3D-GS attributes from pixels.
• Gaussian mean parameterization: mu = x + delta.
• Rendering losses: MSE and LPIPS.
• Visibility mask via frustum, depth-consistency.","• Dataset: ScanNet++ indoor scenes.
• 450+ scenes; train/val splits.
• Inputs: two uncalibrated images per sample.
• Targets: posed novel views for supervision.
• Annotations: depth, poses used for masks.
• 3D-motion present? N/A.","• ViT encoder with cross-attention from MASt3R.
• Freeze MASt3R; add DPT Gaussian head.
• Predict q, s, alpha, color, offsets.
• Union per-view Gaussians; differentiable rendering.
• Losses: masked MSE+LPIPS; optional L_pts.
• Pose-free inference; GT poses only for masks.","• Outperforms pixelSplat and MASt3R across baselines.
• Better PSNR/SSIM/LPIPS on ScanNet++ splits.
• Finetuning with L_pts further improves quality.
• Loss masking critical; prevents Gaussian growth.
• ~4 FPS at 512x512 on RTX 2080Ti.
• Encoding 0.268s; no pose estimation needed.",,,,http://arxiv.org/abs/2408.13912,https://zotero.org/alehanderoo/items/Y2MGKL8L,https://doi.org/10.48550/arXiv.2408.13912,12/11/2025 16:41 (GMT+1)
Yes,Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting (2024),"2D features, 3D features, 3D-fusion","• Reuse 4D GS deformation for dynamic modeling
• Use pseudo-depth for Gaussian initialization
• Integrate confidence-guided loss with multi-view cues
• Replace monocular prior with multi-cam ray-voting
• Evaluate robustness under calibration drift conditions
• Add MOT state estimation atop Gaussians","• Real-time dynamic endoscopic scene reconstruction
• Overcome NeRF slow training and rendering
• Avoid need for ground-truth depth
• Handle monocular depth ambiguity and noise
• Improve geometry via confidence and normal constraints","• Strong dynamic 4D GS aligns with object-centric splats
• Monocular focus mismatches multi-cam requirement
• No occupancy, classification, or tracking modules
• No self-calibration; assumes accurate poses
• Encouraging 100 FPS for edge deployment
• Consider flow-based cues for flying objects","• 4D Gaussian Splatting with deformation fields
• Hexplane spatio-temporal encoding plus tiny MLPs
• Depth prior initialization using Depth-Anything outputs
• Confidence-guided depth and color likelihood loss
• Surface normal constraints and depth regularization
• Total variation regularization on grid features","• Endoscopic robotic surgery scenes, in-vivo porcine
• StereoMIS: 11 sequences; 800–1000 frames sampled
• EndoNeRF: two stereo prostatectomy samples
• Monocular training; stereo depth used for evaluation
• Non-rigid tissue motion present; tool occlusions
• Public datasets; licenses not specified","• Initialize Gaussians from monocular pseudo-depth and masks
• 4D GS with multi-head deformation decoders
• Camera poses assumed known; no self-calibration
• No optical flow; Depth-Anything small backbone
• Losses: color, TV, confidence, depth, surface normals
• Training on RTX4090; 4–7 minutes per sequence","• Outperforms EndoNeRF, EndoSurf, LerPlane on PSNR/SSIM/LPIPS
• 100 FPS rendering; real-time capability
• 4 GB GPU memory during inference
• Training time 4–8 minutes reported
• Ablations show each component significantly contributes",https://github.com/lastbasket/Endo-4DGS,,,http://arxiv.org/abs/2401.16416,https://zotero.org/alehanderoo/items/BVLLCK2K,https://doi.org/10.48550/arXiv.2401.16416,12/11/2025 16:40 (GMT+1)
Yes,Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation (2024),"2D features, 3D features","• Use as per-camera pseudo-depth prior for occupancy.
• Adopt percentile normalization for scale-agnostic fusion.
• Exploit scale-shift alignment across cameras.
• Distill to 2–10 step model for real-time.
• Fine-tune on Isaac Sim aerial scenes.
• Beware depth scale ambiguity, dynamic object artifacts.","• Affine-invariant monocular depth from single RGB.
• Improve zero-shot generalization across unseen domains.
• Leverage diffusion model priors for depth estimation.
• Resource-efficient fine-tuning on synthetic depth data.
• Address unknown intrinsics without metric scale.","• Monocular; no multi-view geometry or self-calibration.
• No tracking, classification, or dynamic modeling.
• Depth priors useful for track-before-detect voting.
• Potentially aids extrinsic refinement via cross-view consistency.
• Runtime heavy; ensemble unsuitable for edge deployments.
• Unclear performance on sky, small flying objects.","• Conditional latent diffusion for depth prediction.
• DDPM noise objective; DDIM sampling.
• Stable Diffusion VAE latent space preserved.
• Affine-invariant percentile normalization (2%-98%).
• Annealed multi-resolution training noise schedule.
• Test-time ensembling with scale-shift alignment.","• Training: Hypersim 54k, Virtual KITTI ~20k.
• Evaluation: NYUv2 654, KITTI 652, ETH3D 454.
• Evaluation: ScanNet 800, DIODE 771 images.
• Monocular; single camera; no view overlap.
• Annotations: dense synthetic depth; real LiDAR/Kinect depths.
• 3D motion: N/A; static single images.","• Encode RGB, depth via frozen Stable Diffusion VAE.
• Concatenate latents; fine-tune U-Net only.
• Calibration: none; affine-invariant, intrinsics-agnostic.
• DDPM training 1000 steps; DDIM inference 50 steps.
• Test-time ensembling; median fusion after scale-shift.
• 18k iters, bs32, grad-acc16, 2.5 days, RTX4090.","• Zero-shot SOTA across multiple datasets.
• NYUv2 AbsRel 5.5, δ1 96.4.
• KITTI AbsRel 9.9, δ1 91.6.
• ETH3D AbsRel 6.5, δ1 96.0.
• Multi-resolution noise improves accuracy and consistency.
• Denoising steps can reduce to ~10 with minor loss.",,,,http://arxiv.org/abs/2312.02145,https://zotero.org/alehanderoo/items/D4XCTMVH,https://doi.org/10.48550/arXiv.2312.02145,12/11/2025 16:39 (GMT+1)
Yes,Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID (2025),"2D features, explain, track","• Adopt GMC with LK flow for per-camera stabilization.
• Use CircleLoss ReID for appearance association.
• Increase input resolution for tiny flying targets.
• Benchmark buffer sizes; start around 60 frames.
• Integrate per-view tracks before multi-view 3D fusion.
• Beware thermal domain mismatch and single-camera assumptions.","• Thermal IR multi-UAV detection and tracking baseline.
• Address low-contrast, tiny targets, noisy backgrounds.
• Replace YOLOv5+DeepSORT with YOLOv12+BoT-SORT.
• Evaluate under 4th Anti-UAV Challenge metrics.
• Analyze training, buffer, resolution, ReID impacts.","• Relevant for flying targets; but strictly 2D, single-view.
• No 3D occupancy, fusion, or calibration handling.
• Optical flow CMC useful under fast camera motion.
• ReID insights may aid cross-view association.
• Track-before-detect not used; could be explored.
• Need FPS, latency, and RGB generalization evidence.","• YOLOv12 R-ELAN with FlashAttention, large kernels.
• BoT-SORT Kalman Filter with camera motion compensation.
• GMC via Lucas-Kanade optical flow and RANSAC affine.
• ReID with SBS backbones, CircleLoss or Triplet.
• Metrics: MOTA and Anti-UAV SOT accuracy formula.","• Thermal infrared videos, UAV swarms domain.
• Track 3: 200 train, 100 test sequences.
• Resolution 640×512; inputs up to 1600 during inference.
• Single-camera sequences; no multi-view overlap.
• Annotations: 2D boxes, visibility flags.
• Anti-UAV dataset, public; license N/A.","• Pipeline: YOLOv12 detector feeding BoT-SORT-ReID.
• Two-stage training: SOT pretrain, MOT finetune.
• CMC using affine GMC from LK optical flow.
• ReID: SBS-ResNet50, AGW, MGN variants evaluated.
• Losses: CE, Triplet, CircleLoss; optimizer AdamW.
• Inference tweaks: SOT lost-target reporting, no interpolation.","• BoT-SORT boosts SOT score dramatically vs detector-only.
• Best MOT MOTA 0.7609 with 1600 input.
• Image resolution increases yield ~+0.1 score.
• CircleLoss outperforms Triplet for ReID.
• Model size impact minimal; buffer 60 slightly best.
• Memory accumulation causes inference crashes; runtime FPS N/A.",https://github.com/wish44165/YOLOv12-BoT-SORT-ReID,,,http://arxiv.org/abs/2503.17237,https://zotero.org/alehanderoo/items/CIFBUNHW,https://doi.org/10.48550/arXiv.2503.17237,12/11/2025 16:37 (GMT+1)
Yes,PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction (2023),"2D features, 3D features, 3D-fusion, pose","• Reuse differentiable PnP with opacity-weighted correspondences.
• Adopt intrinsics-conditioned, single-stream token fusion.
• Replace triplane with 3D Gaussians for speed.
• Use PF-LRM for online extrinsic self-calibration.
• Beware static-scene assumption, segmentation dependence.
• Quick test on Isaac Sim multi-cam captures.","• Jointly predict poses and NeRF from unposed images.
• Handle sparse views with minimal overlap.
• Replace brittle SfM under sparse correspondences.
• Fast feed-forward reconstruction and registration.
• Category-agnostic, generalizes across datasets.","• Strong for RGB-only multi-view self-calibration.
• Lacks occupancy, classification, tracking, dynamics.
• Not tailored for flying object motion.
• Per-patch 3D-2D cues aid track-before-detect.
• Requires known intrinsics; edge latency heavy.
• Background handling, multi-object remain open.","• Single-stream transformer over image and triplane tokens.
• DINO ViT patch tokens with view/intrinsics conditioning.
• Triplane NeRF supervised via differentiable volume rendering.
• Per-patch 3D point, opacity, confidence prediction.
• EPro-PnP differentiable pose loss over correspondences.
• Losses: L2, LPIPS, point, opacity, PnP.","• Domain: object-centric static scenes.
• Training: ~1M objects, 32 views each.
• Cameras: 2–4 views, wide baselines, sparse overlap.
• Annotations: posed multi-view images, masks sometimes.
• 3D motion present? No.
• Availability/license: N/A.","• Tokenize images via DINO ViT-B/16 patches.
• Concatenate with triplane tokens; self-attention mixing.
• Intrinsics-conditioned; RGB-only inference; reference relative poses.
• Predict per-patch 3D points, opacities, confidences.
• Pose via EPro-PnP; NeRF via triplane rendering.
• Training: L2+LPIPS+point+opacity+PnP; 128 A100s.","• 14.6x lower rotation error versus baselines.
• Acc@15 often above 0.95 across datasets.
• Novel-view PSNR around 25–27 dB.
• Robust to moderate mask noise, lighting changes.
• Graceful degradation with fewer input views.
• Inference ~1.3s on single A100 GPU.",https://totoro97.github.io/pf-lrm,,,http://arxiv.org/abs/2311.12024,https://zotero.org/alehanderoo/items/DLS2N8ZL,https://doi.org/10.48550/arXiv.2311.12024,12/11/2025 16:35 (GMT+1)
Yes,UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse recentering+adapter+alignment for drifted extrinsics.
• Integrate LoRA adaptation into 4D GS backbone.
• Generate unfavorable views from favorable simulation labels.
• Replace PnP with alignment for pose-agnostic fusion.
• Require high-quality masks; evaluate segmentation sensitivity.
• Benchmark on flying-object multi-cam sequences.","• Address pose-free 3DGS under unfavorable views.
• Bridge generalization gap from favorable training views.
• Maintain multi-view Gaussian consistency without poses.
• Adapt pretrained models without new unfavorable datasets.
• Enable practical smartphone captures with unknown poses.","• Strong for pose robustness; static object assumption.
• No occupancy, classification, or tracking components.
• Recentered inputs break epipolar/pose estimation.
• Multi-object, dynamic scenes remain unaddressed.
• Could aid online extrinsic drift compensation.
• Lacks runtime/FPS; edge feasibility unknown.","• Pose-free pixel-aligned 3DGS for object-centric scenes.
• Image recentering exploits favorable-view priors.
• LoRA layers enabling efficient model adaptation.
• Adapter uses recentered positional embeddings.
• Gaussian alignment via weighted least squares scale/translation.
• Loss: MSE + LPIPS reconstruction.","• Object-centric synthetic and real object datasets.
• Train: G-Objaverse 280K assets favorable views.
• Eval: GSO 50 scenes, 32 unfavorable views/scene.
• Eval: OmniObject3D 20 scenes, masks provided.
• Context inputs N=4, large baselines, off-center objects.
• Availability: Project page; code release unspecified.","• Backbone: FreeSplatter pose-free ViT 3DGS.
• Foreground recentering via bounding box resizing.
• LoRA inserted in patchify and self-attention.
• Adapter transformer refines Gaussians using recentered embeddings.
• Gaussian alignment estimates global scale, translation via WLS.
• Training: render unfavorable contexts; MSE+LPIPS; 40K iters.","• Large gains over FreeSplatter and LucidFusion.
• GSO: PSNR 22.91 vs 16.70 baseline.
• OmniObject3D: PSNR 18.86 vs 15.81 baseline.
• Ablations: LoRA, adapter, embeddings all contribute.
• Robust under increasing unfavorable context views.
• Training time ~1.5 days on four A100.",,,,http://arxiv.org/abs/2507.22342,https://zotero.org/alehanderoo/items/JM33SP7K,https://doi.org/10.48550/arXiv.2507.22342,12/11/2025 16:33 (GMT+1)
Yes,FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse PnP on Gaussian maps for self-calibration.
• Adopt pixel-alignment loss for stable 3D prediction.
• Use feed-forward transformer to initialize 3DGS online.
• Replace SfM for drift correction in multi-cam rigs.
• Beware static-scene assumption; moving objects may degrade.
• Train with synthetic depths in Isaac Sim.","• Pose-free sparse-view 3D reconstruction from uncalibrated images.
• Predict 3D Gaussians without known intrinsics/extrinsics.
• Enable rapid focal length and camera pose estimation.
• Address LRM sensitivity to pose inaccuracies.
• Unify object-centric and scene-level reconstruction variants.","• Highly relevant for RGB-only self-calibration and 3DGS.
• No tracking/classify; extend toward 4D Gaussians for motion.
• Static assumption mismatches flying-object dynamics.
• Cross-view transformer suits track-before-detect fusion.
• Rolling-shutter and drift robustness not evaluated.
• Depth supervision only in training aligns with thesis sim.","• Feed-forward transformer with cross-view self-attention token exchange.
• Pixel-aligned Gaussian maps in a unified reference frame.
• Pose recovery via PnP-RANSAC from 2D-3D correspondences.
• Losses: L_render (MSE+LPIPS), L_align, L_pos.
• Shared focal length assumption across views; reference-view embeddings.","• Objaverse training renders, 512×512, 32 views per asset.
• Scene-level training: BlendedMVS, ScanNet++, CO3Dv2 subsets.
• Evaluation: GSO, OmniObject3D, ScanNet++, CO3Dv2.
• 24 eval views/object; 20 random, 4 structured views.
• Annotations: depths, masks for training supervision.
• Static scenes; no temporal motion; public datasets.","• ViT-style patchify with positional and view embeddings.
• L-layer self-attention aggregates multi-view image tokens.
• Linear head predicts per-pixel Gaussian parameters.
• PnP-RANSAC estimates poses and shared focal length.
• Two-stage training: L_pos pretrain; L_align+render finetune.
• Occlusion-aware masking; no classify/track heads; RGB-only inference.","• FreeSplatter-O PSNR 30.44 on GSO; 31.93 OmniObject3D.
• Beats LGM/InstantMesh despite unknown input poses.
• Pose RRE: 3.90 GSO; 11.35 OmniObject3D (object).
• Scene-level surpasses Splatt3R; strong ScanNet++/CO3Dv2 metrics.
• L_align crucial; removing drops PSNR by ~3–5 dB.
• Pose estimation within seconds; detailed runtime N/A.",https://bluestyle97.github.io/projects/freesplatter/,,,http://arxiv.org/abs/2412.09573,https://zotero.org/alehanderoo/items/8F5DZNUF,https://doi.org/10.48550/arXiv.2412.09573,12/11/2025 16:24 (GMT+1)
Yes,No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views (2025),"2D features, 3D features, 3D-fusion, pose","• Adopt pose head for online extrinsic refinement
• Use canonical Gaussians for multi-view fusion
• Add reprojection loss to stabilize self-calibration
• Scale to ≥5 views with cross-view attention
• Train on Isaac Sim with RGB-only supervision
• Evaluate EPA to decouple reconstruction from pose","• Pose-free 3DGS from sparse unposed multi-view RGB
• Eliminate GT pose supervision during training and inference
• Mitigate geometry–pose feedback instability
• Handle low-overlap, extreme viewpoint changes
• Efficient single-step feed-forward reconstruction","• Strong self-calibration aligns with drift robustness
• No tracking, classification, or occupancy outputs
• Assumes static scenes; flying objects unaddressed
• Feed-forward speed suits multi-cam online inference
• Useful backbone for 4D Gaussian dynamics extension
• Open questions: multi-view scaling, temporal consistency","• Canonical-space Gaussians relative to reference view
• Shared ViT backbone for pose and geometry
• Rendering loss: L2 + LPIPS
• Pixel-wise reprojection loss with projection π
• Pose as 10D rep; 6D rotation, 4D translation
• Self-supervised bundle-adjustment-like joint optimization","• Datasets: RE10K, ACID, DL3DV, DTU
• 2–10 unposed RGB views per sample
• Overlap bins: small, medium, large
• Annotations: RGB; poses for evaluation only
• 3D-motion present? N/A
• Publicly available; standard licenses","• Shared ViT encoder-decoder with cross-view attention
• Predict canonical Gaussians: centers, scale, rotation, SH, α
• Lightweight MLP pose head; relative to reference
• Losses: L2, LPIPS, reprojection consistency
• Initialization from MASt3R; optional intrinsics token
• Optional PnP RANSAC from Gaussian centers","• SOTA PSNR/SSIM/LPIPS on RE10K, ACID
• Strong at low-overlap, large viewpoint changes
• Pose AUC surpasses DUSt3R, MASt3R
• 0.044s reconstruction for two 256×256 views
• Ablations: reprojection loss is critical
• Zero-shot generalization to ACID, DTU",https://ranrhuang.github.io/spfsplat/,,,http://arxiv.org/abs/2508.01171,https://zotero.org/alehanderoo/items/76CZBCRU,https://doi.org/10.48550/arXiv.2508.01171,12/11/2025 16:23 (GMT+1)
Yes,Pose-free 3D Gaussian splatting via shape-ray estimation (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse ray-guided fusion for extrinsic drift robustness
• Use anchor-offset Gaussians for precise volumetric occupancy
• Integrate pose rays with track-before-detect ray-voting
• Add temporal consistency for dynamic, flying objects
• Evaluate RGB-only multi-cam overlapping central volume
• Train in sim with photometric supervision, online refinement","• Pose-free 3DGS from unposed multi-view RGB
• Reduce sensitivity to pose noise, misalignment
• Avoid explicit 3D transforms during fusion
• Generalizable feed-forward reconstruction, no per-scene optimization
• Handle sparse views and varying baselines","• Strong fit: pose-free multi-view, Gaussian representation
• Mismatch: static scenes, no tracking or classification
• Useful for online self-calibration from object motion
• Latent fusion may mitigate calibration drift
• Unknown real-time feasibility for edge inference
• Extend to 4D dynamics and explainability","• Plücker ray pose representation for per-pixel geometry
• Pose-aware cost volumes with cross-attention aggregation
• Canonical volume fusion for geometry and features
• Anchor-aligned coarse-to-fine Gaussian offsets
• Losses: MSE, LPIPS, ray regression
• Expected depth via softmax over depth hypotheses","• Domains: DTU, RealEstate10K, ACID, BlendedMVS
• DTU: 75 train, 15 test scenes
• DTU: 49 cameras, varied baselines, static objects
• RealEstate10K: 67k train, 7k test videos
• Annotations: 2D photometric; poses for evaluation only
• Motion: static scenes; datasets publicly available","• Matching transformer extracts multi-view features
• Predict patch-wise Plücker rays from features
• Build pose-aware cost volumes via ray-conditioned warping
• Fuse to canonical geometry and feature volumes
• Anchor depths predict K Gaussian offsets per pixel
• Train with photometric, LPIPS, ray regression losses","• Better PSNR/SSIM under noisy poses than g-3DGS
• Outperforms with approximately 1° pose noise scenarios
• Beats pose-free NeRFs on RealEstate10K metrics
• Ablations: pose embedding, offsets significantly help
• Cross-dataset generalization shows robustness
• Runtime, FPS, memory not reported",,,,http://arxiv.org/abs/2505.22978,https://zotero.org/alehanderoo/items/VPRRMD58,https://doi.org/10.48550/arXiv.2505.22978,12/11/2025 16:20 (GMT+1)
Yes,HISPLAT: HIERARCHICAL 3D GAUSSIAN SPLATTING FOR GENERALIZABLE SPARSE-VIEW RECONSTRUC- (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse hierarchical Gaussians for coarse-to-fine occupancy.
• Adopt EAM to refine ray-voted foreground volumes.
• Use MFM-like opacity gating for multi-scale fusion.
• Seed depth via plane-sweep on few calibrated views.
• Stress-test robustness to pose drift/noisy extrinsics.
• Integrate DINOv2 features for generalization.","• Sparse-view reconstruction from two images only.
• Single-scale 3D-GS misses structure and details.
• Introduce hierarchical Gaussians coarse-to-fine.
• Improve generalization across datasets without finetuning.","• Strong 3D-GS; static scenes, no dynamics/tracking.
• Two-view assumption; extend to ≥5 cameras.
• No classification; add temporal parameter change heuristics.
• Lacks self-calibration; explore error-driven pose refinement.
• Evaluate under extrinsic drift, rolling-shutter, motion blur.
• Missing runtime; edge feasibility unclear.","• Hierarchical Gaussian primitives: coarse skeleton, fine details.
• Error Aware Module guides compensatory fine Gaussians.
• Modulating Fusion reweights opacities via error attention.
• Cost volume plane-sweep depth for stage-one.
• Softmax depth regression over sampled planes.
• Photometric losses: MSE + LPIPS.","• RealEstate10K indoor/outdoor; 67k train, 7k test.
• ACID aerial drones; 11k train, 1.9k test.
• Two input views; three target views per scene.
• Cameras calibrated via SfM; overlap unspecified; public datasets.
• Annotations: RGB images, camera poses only.
• 3D motion presence: N/A.","• Shared U-Net + multi-view transformer backbone.
• Known poses; no self-cal; MFM opacity fusion.
• Stage1: plane-sweep cost volume for depth.
• Later stages: EAM depth offsets from error maps.
• Gaussian heads: centers unprojected; SH, covariances, opacity.
• Train 256x256, 300k iters, MSE+LPIPS, 8×4090.","• RealEstate10K: 27.21 PSNR; +0.82 over MVSplat.
• ACID: 28.75 PSNR; +0.50 over MVSplat.
• Zero-shot Replica: +3.19 PSNR vs suboptimal method.
• Zero-shot DTU: +1.05 PSNR improvement.
• Ablations: EAM+MFM unlock hierarchical gains.
• Runtime/memory numbers not provided here.",https://github.com/Open3DVLab/HiSplat,,,,https://zotero.org/alehanderoo/items/NY3V5ZX6,,12/11/2025 16:17 (GMT+1)
Yes,GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting (2024),"2D features, 3D features, 3D-fusion, pose","• Reuse pixel-aligned Gaussian regression backbone.
• Treat Gaussian opacities as occupancy prior.
• Fine-tune on Isaac Sim flying objects.
• Add online extrinsics refinement for drift robustness.
• Extend to temporal 4D Gaussians for dynamics.
• Seed with multi-cam differencing ray-votes initialization.","• Sparse-view feed-forward 3D reconstruction from posed images.
• Replace triplane NeRF with pixel-aligned Gaussians.
• Handle both object- and scene-level inputs.
• Achieve fast inference and high-quality novel views.
• Address limited resolution, slow rendering of prior LRMs.","• Strong fit for multi-view RGB-only reconstruction.
• Lacks dynamic modeling; no tracking capabilities.
• Requires accurate poses; drift sensitivity likely.
• No optical flow or track-before-detect cues.
• Promising foundation for 4D Gaussian tracking.
• Heavy compute; edge deployment challenging.","• Pixel-aligned Gaussian per-pixel along camera ray.
• Plücker ray coordinates for per-pixel pose conditioning.
• Multi-view transformer self-attention over concatenated tokens.
• Loss: MSE + VGG perceptual reconstruction.
• Distance via sigmoid mapping to near/far bounds.
• Differentiable Gaussian splatting rendering supervision.","• Objaverse objects; 730k renders; 32 views each.
• GSO, ABO evaluation; 4 inputs; 10 test views.
• RealEstate10K scenes; 80k clips; indoor/outdoor.
• 2 input views for scene model; SfM poses.
• Annotations: posed RGB only; no depths at inference.
• Public datasets; static-only; license N/A.","• Patchify RGB+Plücker; transformer encodes multi-view context.
• Decode per-pixel Gaussian params: RGB, scale, rotation, opacity, depth.
• Known intrinsics/extrinsics; no self-calibration; static-only.
• Merge per-view Gaussians; splatting renders supervision views.
• Train 256→512; FlashAttention; BF16; 64 A100 GPUs.
• Losses: MSE, VGG; near/far distance parameterization.","• +3.98dB PSNR over Triplane-LRM on GSO.
• +1.59dB PSNR over Triplane-LRM on ABO.
• +2.2dB PSNR over pixelSplat on RealEstate10K.
• Sharper details; fewer floaters; better thin structures.
• Inference ~0.23s with 2-4 views on A100.",https://sai-bi.github.io/project/gs-lrm/,,,http://arxiv.org/abs/2404.19702,https://zotero.org/alehanderoo/items/GMQKZ99W,https://doi.org/10.48550/arXiv.2404.19702,12/11/2025 16:15 (GMT+1)
Yes,PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse Fibonacci lattice for uniform spherical sampling
• Adapt Gaussian pyramid to object-centric 4D splats
• Leverage hierarchical cost volume for multi-view geometry
• Adopt cubemap renderer and deferred backprop for memory
• Add self-calibration to handle extrinsic drift
• Test with ≥3 cameras, overlapping central volume","• 4K panorama novel view synthesis from sparse inputs
• Address spherical geometry redundancy near panorama poles
• Achieve fast, memory-efficient feed-forward inference
• Enable generalization across scenes and baselines
• Handle two wide-baseline posed panoramas","• Strong 3DGS ideas; task differs from tracking/classification
• Assumes accurate poses; no drift robustness
• Two panoramas, not multi-camera synchronous rig
• Static-scene assumption; dynamic handling minimal
• No occupancy, class labels, or MOT
• Could inspire efficient 4D Gaussian processing","• Spherical 3D Gaussian pyramid with Fibonacci lattice
• Hierarchical spherical cost volume, cross-view attention
• Monocular depth features refine cost distributions
• Two-step deferred backpropagation for 4K training
• Losses: L1 depth, L2+LPIPS image
• Deferred blending mitigates moving object mismatches","• Panorama indoor/outdoor synthetic and real scenes
• 360Loc: 18 sequences, 9,334 frames
• Insta360: two sequences, ~38K frames
• Two posed panoramas, wide baseline, full overlap
• Synthetic depth GT; real: RGB-only supervision
• Dynamic elements present; datasets publicly available","• FPN with Swin Transformer cross-view attention
• Hierarchical spherical cost volume with dot-product correlations
• Integrate monocular depth features via 2D U-Nets
• Fibonacci lattice sampling; spherical 3D Gaussian pyramid
• Lightweight Gaussian heads with residual multi-level guidance
• Cubemap renderer; two-step deferred backprop; posed inputs","• SOTA WS-PSNR/SSIM across synthetic and real
• Up to 70× faster than PanoGRF
• 0.34s per 512×1024 image inference
• 4K inference on 24GB RTX 3090
• 4K training on single 80GB A100
• Fibonacci reduces Gaussians by ~36% without quality loss",https://github.com/chengzhag/PanSplat,,,http://arxiv.org/abs/2412.12096,https://zotero.org/alehanderoo/items/2VSIFYBB,https://doi.org/10.48550/arXiv.2412.12096,12/11/2025 16:13 (GMT+1)
Yes,pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (2024),"2D features, 3D features, 3D-fusion, explain, pose","• Reuse epipolar transformer for online scale/drift handling
• Adopt per-ray depth distributions for ray-voting occupancy
• Use feed-forward Gaussian prediction for object-centric priors
• Integrate rasterized 3D-GS for real-time volumetric updates
• Extend to ≥3 synchronized cameras with shared encoder
• Add motion-based self-calib; remove strict static-scene assumption","• Generalizable NVS from sparse image pairs
• Explicit 3D Gaussian radiance fields
• Resolve per-scene SfM scale ambiguity
• Overcome local minima in primitive regression
• Real-time, memory-efficient training and rendering","• Strong fit for feed-forward 3D Gaussian representation
• Static-scene assumption mismatches flying-object dynamics
• Requires known poses; no self-calib from motion
• No classification or multi-object tracking heads
• Epipolar depth distributions align with track-before-detect
• Investigate robustness to calibration drift, rolling shutter","• Epipolar transformer infers scale via depth encodings
• Discrete disparity-depth buckets per ray
• Sampled depth with reparameterization-like trick
• Set alpha equal to bucket probability
• Losses: MSE plus LPIPS
• Self-attention inpaints unmatched pixels","• RealEstate10k home walkthrough videos
• ACID aerial landscape videos
• Two reference views; sometimes three
• Poses from SfM; arbitrary scale
• Annotations: images and camera poses only
• 3D-motion present? N/A","• ResNet-50 and ViT-B/8 DINO feature encoder
• Two rounds epipolar cross-attention with depth PE
• Per-pixel probabilistic depth distribution ϕ and offsets δ
• Predict Σ and spherical harmonics S per pixel
• Union of Gaussians from both views; rasterized rendering
• Trained 300k steps, Adam, MSE+LPIPS, ~80GB VRAM","• Outperforms baselines on PSNR, SSIM, LPIPS
• ACID PSNR 28.27; RealEstate10k PSNR 26.09
• Rendering ~650× faster than next-fastest baseline
• Encode 0.102s; render 0.002s per frame
• Training memory 14.4GB; inference 3.002GB
• Ablations confirm epipolar and probabilistic sampling necessity",https://dcharatan.github.io/pixelsplat,,,http://arxiv.org/abs/2312.12337,https://zotero.org/alehanderoo/items/NLAFYLM3,https://doi.org/10.48550/arXiv.2312.12337,12/11/2025 16:10 (GMT+1)
Yes,LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation (2024),"2D features, 3D features, 3D-fusion, pose","• Leverage Gaussian splatting for compact 3D occupancy features
• Adopt Plücker ray inputs for multi-cam conditioning
• Use grid distortion to mimic cross-view inconsistency
• Apply orbital jitter to simulate extrinsic drift
• Test concatenation fusion as simple multi-view baseline
• Optional Gaussians→NeRF→mesh for shape prior","• High-res 3D generation from text or image
• Overcome low-res triplane transformer bottlenecks
• Efficient feed-forward 3D Gaussian regression
• Fuse multi-view images into consistent 3D
• Maintain seconds-level generation speed","• Good fit for multi-view, static object fusion
• Mismatch: no tracking, classification, occupancy estimation
• Assumes accurate poses; only augmentation for pose errors
• No optical flow; 2D U-Net features only
• Not evaluated on real overlapping cameras
• Unsuitable for flying-object dynamics without extensions","• 3D Gaussian splatting as scene representation
• Asymmetric U-Net for high-throughput decoding
• Plücker ray embedding for camera poses
• Cross-view self-attention for multi-view fusion
• Image, alpha MSE and LPIPS losses
• Assume static objects, fixed four-view setup","• Objaverse objects, static single-object domain
• ~80k objects filtered for quality
• 100 views/object at 512×512 renders
• Four orthogonal azimuth inputs, fixed elevation
• Annotations: RGBA images; known camera poses
• 3D motion: no; availability public; license N/A","• Two-stage: multi-view diffusion, then Gaussian regression
• Poses via Plücker rays; no self-calibration
• Asymmetric U-Net with cross-view self-attention
• Concatenate per-view Gaussians; differentiable splat rendering
• Losses: RGB MSE+LPIPS; alpha MSE
• Training: 32×A100/80G, 4 days; grid, jitter augments","• User study: 4.18 consistency, 3.95 quality
• 5 seconds to generate 3D Gaussians
• 512 training resolution improves details
• 65,536 Gaussians default per object
• Inference memory ~10 GB including diffusion
• Mesh conversion takes about one minute",,,,http://arxiv.org/abs/2402.05054,https://zotero.org/alehanderoo/items/ELYWP5VK,https://doi.org/10.48550/arXiv.2402.05054,12/11/2025 16:08 (GMT+1)
Yes,4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time (2025),"2D features, 3D features, 3D-fusion, pose","• Adopt pixel-aligned 4DGS as dynamic occupancy proxy
• Use posed RGB-only; ensure accurate multi-cam calibration
• Convert Gaussians into voxel occupancy for tracking
• Fine-tune on flying-object Isaac Sim renders
• Leverage temporal marginal p(t) for track-before-detect
• Add test-time pose refinement for drift","• Generalizable 4D reconstruction from sparse multi-view timestamps
• Address inefficiencies of optimization and generative 4D methods
• Directly predict per-pixel 4D Gaussian primitives
• Render any novel view-time from few inputs","• Strong match to RGB-only, multi-view dynamics
• No classification or MOT; add downstream heads
• Assumes accurate poses; drift robustness unaddressed
• No optical flow; consider auxiliary flow cues
• 4DGS parameters are interpretable for explainability
• Training heavy; inference moderate, still GPU-bound","• Unified 4D Gaussian Splatting space-time ellipsoids
• Pixel-aligned Gaussians conditioned by Plücker rays
• Conditional 3DGS from 4DGS via Gaussian conditioning
• Alpha compositing with temporal marginal weighting p(t)
• Loss: MSE plus perceptual (VGG) reconstruction
• Transformer tokens across views and time","• Objaverse-derived animated objects; dynamic, articulated domain
• 24-frame sequences per object; some eval 48 frames
• Multiple camera setups: canonical, rotating, random
• Supervision: posed RGB views only; intrinsics/extrinsics provided
• 3D motion present; interpolation with missing timestamps
• Dataset curation described; license details N/A","• Patchify RGB+Plücker+time into per-view tokens
• Transformer regresses per-pixel 4DGS parameters
• Pixel-aligned depth via near–far interpolation
• Optional free Gaussian tokens for sparse coverage
• Differentiable 4DGS rasterization with alpha blending
• Curriculum training; MSE+perceptual losses; large-scale GPUs","• Outperforms GS-LRM under sparse multi-view inputs
• PSNR ~32 on Consistent4D alternating views
• Robust to missing frames; strong interpolation ability
• 24 frames rendered under 1.5s on A100
• Larger models improve quality significantly
• Performance drops with very long input sequences",https://4dlrm.github.io/,,,http://arxiv.org/abs/2506.18890,https://zotero.org/alehanderoo/items/3U2YC456,https://doi.org/10.48550/arXiv.2506.18890,12/11/2025 15:46 (GMT+1)
Yes,Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats (2025),"2D features, 3D features, 3D-fusion, pose","• Repurpose hybrid Mamba2+Transformer for multi-view fusion
• Adopt opacity L1 and pruning for efficiency
• Use Plücker ray embeddings for camera-aware tokens
• Risk: requires accurate poses; no self-calibration
• Risk: static-only; no dynamics, classification, tracking
• Quick-win: finetune on Isaac Sim multi-cam RGB","• Instant wide-coverage 3D GS from 32 high-res images
• Overcome long-sequence quadratic transformer bottlenecks
• Match optimization quality with feed-forward speed
• Enable 360° scene reconstruction in about one second
• Scale to 250K tokens efficiently","• Good fit for feed-forward 3D GS from RGB
• Mismatch: no occupancy, dynamics, MOT, classification
• Relies on fixed poses; drift robustness untested
• Multi-view overlap leveraged; 32-view scalability notable
• Design takeaways: token merging, opacity sparsity, depth prior
• Open: pose noise tolerance, FOV variation, rolling-shutter effects","• Hybrid Mamba2 SSM and transformer global attention
• Per-pixel Gaussians aligned with Plücker ray embeddings
• Rendering loss: MSE + perceptual (λ=0.5)
• Scale-invariant depth regularization via DepthAnything disparity
• L1 opacity regularization encourages sparsity and pruning","• Real indoor/outdoor scenes: DL3DV, Tanks&Temples, RealEstate10K
• DL3DV-10K: ~10,510 videos, 200–300 keyframes each
• 32 input views selected for wide 360° coverage
• Camera poses from COLMAP; no GT depth at train
• Scenes mostly static; dynamic motion not targeted
• Public datasets; licenses per dataset","• Input 32 RGB + Plücker rays; patch tokenization
• {7 Mamba2 + 1 Transformer}×3 with global attention
• Bi-directional Mamba2 scans; FlashAttentionV2 for transformers
• Token merging 2×2 reduces tokens 4×; dim expands
• Decode per-pixel GS; prune low-opacity Gaussians
• Losses: MSE, perceptual, L1 opacity, scale-invariant depth","• 1s reconstruction for 32×960×540 on A100 80GB
• Comparable PSNR to 3DGS 30k; 24.10 vs 23.60
• Post-opt 10 steps: PSNR 25.60 in 37s
• ~800× faster than optimization-based GS methods
• Opacity loss cuts visible Gaussians to ~33–40%
• Training peak memory ~53GB at 960×540",,,,http://arxiv.org/abs/2410.12781,https://zotero.org/alehanderoo/items/ARUSFJYN,https://doi.org/10.48550/arXiv.2410.12781,12/11/2025 15:44 (GMT+1)
Yes,L4GM: Large 4D Gaussian Reconstruction Model (2024),"2D features, 3D features, 3D-fusion, pose","• Reuse 4D Gaussians as object-centric volumetric occupancy.
• Adopt temporal self-attention for multi-view temporal consistency.
• Replace diffusion views with real overlapping cameras.
• Train interpolation head for smoother flight trajectories.
• Use Plücker pose embedding; test drift robustness.
• Finetune on Isaac Sim flying objects.","• Feed-forward 4D reconstruction from monocular video.
• Avoid slow optimization and SDS fragility.
• Leverage synthetic multiview animated dataset.
• Achieve temporal and view consistency at speed.","• Strong fit for 4D Gaussians; no tracking.
• Single-object, static-camera assumption mismatches multi-camera.
• No classification head; add temporal classifier.
• No self-calibration; evaluate extrinsic drift.
• Could fuse real multi-cam instead of generative views.
• Edge runtime promising; memory footprint unspecified.","• Per-frame 3D Gaussian Splatting as 4D.
• Cross-view and temporal self-attention layers.
• Plücker ray camera pose embeddings.
• Per-timestep multiview rendering loss.
• RGB LPIPS+MSE and mask MSE losses.
• Assumes static camera, moving object.","• Objaverse-4D synthetic animated objects.
• 44K objects, 110K animations, 12M videos.
• 48 views per 1s clip, 24 FPS.
• Training uses 4 input and 4 supervision cameras.
• RGB and alpha masks provided.
• 3D motion present; availability unclear.","• Generate initial multiviews of frame one via ImageDream.
• Azimuth alignment using LGM render-and-match.
• Replicate multiviews across timesteps; static camera assumption.
• U-Net with cross-view and temporal self-attention.
• Pose encoded with Plücker embeddings.
• Train LPIPS+MSE RGB, mask MSE; T=8, V=4.","• SOTA on Consistent4D: LPIPS 0.12, CLIP 0.94.
• FVD 691.87; surpasses baselines by large margins.
• Inference runtime ~3s per 64-frame video.
• 4080 16G GPU feasible; 65k Gaussians per frame.
• Temporal attention reduces flicker; pretraining crucial.
• Interpolation 0.065s per step; boosts FPS.",https://research.nvidia.com/labs/toronto-ai/l4gm,,,http://arxiv.org/abs/2406.10324,https://zotero.org/alehanderoo/items/Q9MEVA6J,https://doi.org/10.48550/arXiv.2406.10324,12/11/2025 15:42 (GMT+1)
Yes,Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse metric gauge for drift-tolerant self-calibration
• Adopt state tokens for temporal scale consistency
• Leverage retargeting+flow losses for motion learning
• Adapt Gaussian fusion for dynamic/static separation
• Replace human priors; train on flying objects
• Stress-test under moving cams and extrinsic drift","• Feed-forward 4D from uncalibrated sparse-view videos
• Enable novel-view and novel-time synthesis
• Maintain metric-scale without ground-truth calibration
• Achieve interactive speed and memory efficiency
• Improve temporal consistency across frames","• Strong fit: 4D Gaussians, feed-forward, uncalibrated
• Mismatch: human-specific, single subject, stationary cams
• Dense 3D motion aids track-before-detect cues
• No classification or multi-object tracking
• Constant-velocity may fail for agile flyers
• RGB-only inference; compatible with thesis","• Metric gauge regularization for scale alignment
• State tokens for temporal cross-attention
• Retargeting loss for self-supervised motion
• Occlusion-aware optical flow loss via SEARAFT
• Constant-velocity motion assumption
• Gaussian fusion MLP with dual consistency factor","• Humans; DNA-Rendering train: 2,078 sequences
• Eval: DNA test 10 IDs; Genebody full
• MetaHuman4D: 11 IDs, 7 motions, 48 views
• Input: 4 static cams, ~45° spacing, overlap
• Annotations: synthetic GT meshes, dense motions
• Availability/license: project page; license N/A","• Three-stage: static, streaming, motion+fusion
• Pose from VGGT; uncalibrated with metric gauge
• Backbones: VGGT, DPT; state-token cross-attention
• SEARAFT flow prior; occlusion-aware weighting
• 3D Gaussian assets; dual-consistency fusion MLP
• Losses: Lcam, photometric, L_matching, L_fusion","• Outperforms GPS-Gaussian by +2.90 dB PSNR
• Beats AnySplat by up to +2.28 dB
• Surpasses L4GM by +12.57 dB PSNR
• Metric scale error 0.0264 m vs MoGe-2
• Runtime 224 ms; 4.45 FPS base
• 44–88 FPS with 10–20x interpolation",https://zhenliuzju.github.io/huyingdong/Forge4D,,,http://arxiv.org/abs/2509.24209,https://zotero.org/alehanderoo/items/K3WEGJGA,https://doi.org/10.48550/arXiv.2509.24209,12/11/2025 15:38 (GMT+1)
Yes,WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving (2025),"2D features, 3D features, 3D-fusion, classify, pose","• Reuse latent Gaussian decoder for feed-forward 4D splats
• Adopt Plücker ray encoding with known extrinsics
• Add online self-calibration to handle extrinsic drift
• Replace segmentation with track-before-detect foreground cues
• Convert splats to occupancy probabilities in central volume
• Evaluate on flying-object classes; add temporal classifiers","• Bridge generative video and 4D reconstruction for driving
• Enable controllable multi-track novel-view video synthesis
• Improve 3D consistency across viewpoints and time
• Avoid per-scene optimization; feed-forward 4D Gaussians
• Support multi-modal conditioning for controllability","• Strong overlap with 4D Gaussian splatting design
• No occupancy, tracking, or class labels provided
• Calibration drift unaddressed; risky for outdoor rigs
• Depth supervision from foundation model; replace with sim GT
• Multi-view cross-attention likely stabilizes overlap cameras
• Flying-object dynamics untested; altitude coverage uncertain","• 4D-aware latent diffusion with cross-view attention
• Rectified flow training with interpolated latent states
• Plücker ray encoding integrates camera geometry
• Gaussian parameterization: center, rotation, scale, opacity, color
• Losses: L1 RGB, LPIPS, depth L1, BCE seg
• 4D aggregation via ego-pose transformations","• Urban driving scenes, nuScenes dataset
• 1,000 scenes; 700 train, 150 val
• Six-view cameras; multi-view video clips
• Annotations: boxes, BEV sketches, captions, dynamic masks
• 3D motion: vehicles, pedestrians present
• Public dataset; license details N/A","• Multi-condition inputs: BEV layout, boxes, trajectory, text
• Cross-view DiT with ControlNet; rectified flow sampling
• Latents include RGB, metric depth, dynamic masks
• Gaussian decoder with Plücker rays; pixel-aligned 3D splats
• Static/dynamic separation; 4D aggregation; fast splatting
• Enhanced diffusion refinement; RGB-only inference; no LiDAR","• SOTA FVD/FID on nuScenes multi-view generation
• Best novel-view FID/FVD across ±1–4 m shifts
• 4D aggregation and refinement significantly improve metrics
• Downstream BEV mIoU 38.49, mAP 29.34
• Inference 16.1 minutes per 17x6 video, 22GB memory
• Gaussian decoder ~0.84s; 8-step rectified flow",https://wm-research.github.io/worldsplat/,,,http://arxiv.org/abs/2509.23402,https://zotero.org/alehanderoo/items/LBE9APKP,https://doi.org/10.48550/arXiv.2509.23402,12/11/2025 15:36 (GMT+1)
Yes,Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation (2025),"2D features, 3D features, 3D-fusion","• Adopt 4D Gaussians for dynamic/static separation.
• Use uncertainty masking for multi-view inconsistency repair.
• Leverage asymmetric U-Net as feed-forward 4D predictor.
• Replace diffusion refinement with multi-cam consensus fusion.
• Map Gaussian density to voxel occupancy estimates.
• Train on Isaac Sim multi-cam data.","• High-fidelity 4D content from monocular videos.
• Ensure spatial-temporal consistency across views and time.
• Enhance fine details beyond diffusion outputs.
• Generalize across text/image/video conditioned inputs.
• Reduce artifacts: flicker, misalignment, distortions.","• Strong fit: 4D Gaussians, multi-view feature fusion.
• Mismatch: no tracking/classification modules included.
• Assumes fixed calibration; no drift self-calibration.
• Generative diffusion unsuitable for real-time inference.
• Limited evidence on fast flying objects.
• Mask-feedback loop valuable for robustness ideas.","• 4D Gaussian splatting representation per frame.
• Asymmetric U-Net for multi-view feature encoding.
• Splatter Image backprojection to 3D Gaussians.
• Uncertainty masking using DINOv2 features.
• Video diffusion inpainting masked inconsistencies.
• Losses: MSE, LPIPS; AdamW, cosine schedule.","• Consistent4D videos; ObjaverseDy test sequences.
• Internet images curated for image-to-4D.
• Training: 80K Objaverse objects, 100 views each.
• Four orthogonal views synthesized per frame.
• Annotations: rendered RGB supervision; SAM masks.
• 3D motion present; availability public; license N/A.","• Multi-view video synthesis with MV-Adapter and enhancer.
• Asymmetric U-Net encodes multi-view spatial/depth cues.
• Splatter Image backprojects to per-frame 3D Gaussians.
• Uncertainty masks via DINOv2-based predictor network.
• Video diffusion inpaints masks; feedback updates Gaussians.
• Assumes known cameras; no self-calibration at inference.","• SOTA on Consistent4D: LPIPS 0.09, CLIP-S 0.97.
• Consistent4D: FVD-F 390.85, FVD-V 282.79.
• ObjaverseDy: LPIPS 0.112, CLIP-S 0.939.
• Image-to-4D: LPIPS 0.12, PSNR 19.2.
• Ablations: masking/training reduce FVD significantly.
• Runtime and memory usage: N/A.",https://visual-ai.github.io/splat4d,,,http://arxiv.org/abs/2508.07557,https://zotero.org/alehanderoo/items/TUWXZGU6,https://doi.org/10.48550/arXiv.2508.07557,12/11/2025 15:32 (GMT+1)
Yes,4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos (2025),"2D features, 3D features, 3D-fusion, pose","• Adopt feedforward 4DGS module for dynamics.
• Use velocity, lifespan as MOT priors.
• Apply density control for efficient multi-view.
• Extend to synchronized multi-camera rolling windows.
• Add online extrinsic refinement for drift.
• Voxelize Gaussians to occupancy features.","• Feedforward 4D reconstruction from posed monocular videos.
• Avoid slow per-scene optimization for dynamics.
• Unify static and dynamic via 4D Gaussians.
• Scale to long videos with efficient training.
• Improve real-world generalization over synthetic-trained models.","• Strong 4DGS fit; lacks multi-camera ingestion.
• No explicit object classification or MOT.
• Relies on accurate poses; drift unaddressed.
• RGB-only inference aligns with thesis constraints.
• Temporal attributes helpful for flying object tracking.
• DINO+Plücker align with track-before-detect rays.","• 4D Gaussian Splatting with temporal attributes.
• 2DGS geometry; time-varying opacity and orientation.
• Timestamp-aware Plücker ray tokenization.
• Losses: MSE, LPIPS, SSIM, depth, normals.
• Regularize velocity, angular velocity, lifespan.
• Multi-level spatiotemporal attention for efficiency.","• Real-world posed monocular videos across devices.
• 128-frame subsequences used for supervision windows.
• Single camera; known intrinsics/extrinsics required.
• Pseudo-supervision: expert depth and normals.
• Dynamic scenes with nonrigid motion present.
• Datasets: Aria, ARKitTrack, COLMAP-curated sets.","• Frames+poses → DINOv2 features → transformer fusion.
• Assumes accurate calibration; no self-calibration.
• Decode pixel-aligned 4DGS via MLP heads.
• Prune low-activation Gaussians; densify space-time.
• Multi-level temporal attention reduces complexity.
• Train with image, depth, normal, regularization losses.","• On-par SoM accuracy; three orders faster.
• Outperforms L4GM on real-world generalization.
• Emergent motion segmentation and optical flow.
• 80% Gaussian reduction; 5× rendering speed-up.
• 25 ms/frame rendering on A100 80G.
• Consistent world-aligned 4DGS over 64 frames.",,,,http://arxiv.org/abs/2506.08015,https://zotero.org/alehanderoo/items/D23BBPSR,https://doi.org/10.48550/arXiv.2506.08015,12/11/2025 15:29 (GMT+1)
Yes,PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis (2025),"2D features, 3D features, 3D-fusion, classify, explain","• Adapt Gaussian head for object-centric 4D splats.
• Use probabilistic heads for uncertainty-aware classification.
• Leverage DPO with trajectory preferences for motion.
• Borrow SAM/CoTracker ranking for training signals.
• Skip MPM; drive dynamics from multi-view ray-votes.
• Benchmark speed vs real-time multi-cam constraints.","• Optimization-free 4D Gaussian synthesis from sparse views.
• Jointly infer geometry and physical properties.
• Avoid SDS and per-scene optimization instability.
• Enable fast, physically plausible simulations.
• Provide large dataset with physics annotations.","• Strong 4D-GS ideas; not multi-cam tracking.
• Single-view, generative; no occupancy estimation.
• Physics simulation incompatible with real-time tracking.
• Material classification may inform class head.
• Preference learning from tracks promising for MOT.
• Unclear robustness to calibration drift.","• Probabilistic 3DGS and physics parameterization.
• MPM dynamics; constitutive models: Neo-Hookean, FCR, Drucker-Prager.
• Couple deformation gradient to Gaussian covariance via polar decomposition.
• Joint NLL and photometric reconstruction losses.
• Direct Preference Optimization for non-differentiable alignment.","• PhysAssets: 24k+ 3D objects.
• Four-view renders per asset.
• Single static camera for simulations.
• Annotated materials, E, ν, density.
• GT guiding videos; trajectory labels via SAM/CoTracker.
• Dataset released; license unspecified.","• Generate four views using MVDream.
• U-Net encoder fuses views and camera embeddings.
• Requires camera parameters; no self-calibration.
• Dual heads: Gaussian and Physics distributions.
• Sampled ψ,θ initialize MPM; render 4D.
• DPO fine-tuning; 12h, 8×A800, AdamW.","• Outperforms SDS baselines on CLIP sim.
• Average CLIP sim 0.2748 vs 0.2291.
• UPR improves; pretrain-only averages 57%.
• Generates 50-frame videos in under one minute.
• Inference claimed under 30 seconds sometimes.
• Generalizes to in-the-wild images.",https://hihixiaolv.github.io/PhysGM.github.io/,,,http://arxiv.org/abs/2508.13911,https://zotero.org/alehanderoo/items/W6IKF8YR,https://doi.org/10.48550/arXiv.2508.13911,12/11/2025 15:21 (GMT+1)
Yes,Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos (2025),"2D features, 3D features, 3D-fusion, pose","• Reuse bullet-time 3DGS as dynamic 4D backbone
• Adopt Plücker embeddings and ViT tokenization
• Integrate interpolation supervision into training
• Use NTE-like module for fast motions
• Add self-calibration for extrinsic drift (not covered)
• Extend to multi-camera overlapping inputs at inference","• Real-time dynamic scene reconstruction from monocular videos
• Generalizable feed-forward model for dynamic scenes
• Avoid per-scene optimization and external depth priors
• Handle fast motion and unseen timestamps
• Unify static and dynamic reconstruction settings","• Strong for dynamic 3DGS; no tracking or classification
• Monocular focus; multi-view possible but not emphasized
• Relies on accurate poses; drift robustness unaddressed
• No occupancy grids; Gaussians approximate occupancy
• No optical flow or track-before-detect cues
• Good design: bullet-time conditioning and interpolation supervision","• Bullet-time conditioning via timestamp embeddings
• Pixel-aligned 3D Gaussian Splatting representation
• Plücker camera pose embeddings for geometry
• ViT transformer backbone, 24 attention blocks
• RGB-space losses: MSE and LPIPS (λ=0.5)
• Interpolation supervision for temporal consistency","• DyCheck: 7 scenes, 3 synchronized cameras
• NVIDIA Dynamic Scene: 9 scenes, 12 cameras
• DAVIS monocular clips with SLAM poses
• Static pretrain: RE10K, Objaverse, MVImgNet, DL3DV
• Dynamic train: Kubric, PointOdyssey, DynamicReplica, Spring
• PANDA-70M: 40K clips with DROID-SLAM trajectories","• Patch tokens plus pose and time embeddings
• Requires known poses; no self-calibration
• ViT backbone with FlashAttention-3, FlexAttention
• Pixel-aligned unprojection to 3D Gaussians
• NTE predicts intermediate frames; KV-Cache acceleration
• Curriculum: static pretrain, dynamic co-train, long context","• DyCheck: competitive SSIM/LPIPS; 0.98s reconstruction
• NVIDIA: PSNR 25.82, third overall
• Beats 4D-GS, Casual-FVS PSNR by ~5%
• Render speed 115 FPS at 480×270
• 12-view 256×256 inference in 150 ms
• Memory under 10 GB on A100",,,,http://arxiv.org/abs/2412.03526,https://zotero.org/alehanderoo/items/FH3KS8WV,https://doi.org/10.48550/arXiv.2412.03526,12/11/2025 15:16 (GMT+1)
Yes,Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels (2024),"2D features, 3D features, pose","• Reuse DGS warping for 4D Gaussian modeling.
• Adopt warped-state normal regularization.
• Adapt field initialization for multi-cam self-calib.
• Replace flow with DINOv2 registration cues.
• Test on Isaac Sim flying objects.
• Use continuous SE(3) fields for drift correction.","• Single-video to high-fidelity 4D reconstruction.
• Handle non-rigidity and frame distortion.
• Preserve geometry and appearance consistency.
• Initialize warping fields and camera poses.
• Enable text-to-4D via video diffusion.","• Strong 4D GS; no tracking or classification.
• Single-view; extend to multi-view overlap.
• No occupancy output; surface-centric surfels.
• Useful self-calibration cues from motion.
• Runtime, memory, real-time feasibility unclear.
• Robustness to drift, rolling-shutter needs verification.","• Dynamic Gaussian surfels representation.
• Continuous SE(3) warping fields, conditioned by time, u.
• Dual quaternion blend skinning over bones.
• Warped-state normal consistency regularization.
• Dual-branch rotation/scale refinements.
• Volume rendering photometric reconstruction loss.","• Generated videos of animals, objects.
• Thirty single-video sequences evaluated.
• Monocular camera; unknown trajectory.
• No manual annotations; RGB only.
• Non-rigid 3D motion present.
• Project page; dataset generation unspecified.","• Neural SDF init, bone-based backward warping.
• Cycle-consistent forward field inversion.
• Self-calibrated camera poses from RGB frames.
• DINOv2 features for registration; no optical flow.
• DGS rasterization with SH color, alpha compositing.
• Trained ~1 hour on one A800 GPU.","• Outperforms SCGS by ~2.5 PSNR average.
• Best SSIM/LPIPS across 30 videos.
• Refinement reduces flicker, improves detail.
• Warped normal regularization improves geometry.
• Initialization boosts accuracy over no-init baselines.
• Training time ~1 hour; inference speed N/A.",,,,http://arxiv.org/abs/2405.16822,https://zotero.org/alehanderoo/items/FPBSKZ3F,https://doi.org/10.48550/arXiv.2405.16822,12/11/2025 15:13 (GMT+1)
Yes,Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation (2021),"3D features, 3D-fusion, pose","• Adopt SE(3)-equivariant descriptors for 3D features
• Replace depth with RGB multi-view feature encoders
• Use descriptor matching for cross-view association
• Pretrain occupancy-like fields in Isaac Sim
• Add online extrinsic refinement from object motion
• Distill optimization into feed-forward pose heads","• Few-shot category-level manipulation from demonstrations
• Overcome 2D keypoint limitations for 6-DoF tasks
• Learn SE(3)-equivariant 3D descriptors self-supervised
• Jointly solve correspondence and pose estimation","• Manipulation-focused; not targeting flying object tracking
• Depth reliance conflicts with RGB-only inference
• No multi-object tracking or classification provided
• Equivariance aids pose generalization, not calibration drift
• Energy landscapes provide interpretable alignment cues
• Per-pose optimization likely too slow realtime","• SE(3)-equivariance via Vector Neurons architecture
• Descriptors from occupancy network multi-layer activations
• Energy minimization for point and pose matching
• Translation invariance using point cloud centering
• L1 descriptor distance for optimization objective","• Simulation using ShapeNet mugs, bowls, bottles
• Ten demonstrations per task in experiments
• Four calibrated depth cameras; fused point clouds
• Annotations: demonstrations; no manual keypoints
• 6-DoF object motion throughout tasks
• Project website available; code link unstated","• Pretrain occupancy network for 3D reconstruction
• Extract hierarchical spatial descriptors per 3D coordinate
• SO(3) equivariance via Vector Neurons layers
• Pose descriptors from query points near contacts
• Optimize ADAM to match pose descriptors
• Fuse four depth cameras into point cloud","• Outperforms DON on overall pick-and-place success
• Robust to arbitrary SE(3) test poses
• All-layer descriptors best in ablations
• Query point scale strongly influences performance
• Real robot executions validate generalization
• Runtime and memory metrics: N/A",https://yilundu.github.io/ndf/,,,http://arxiv.org/abs/2112.05124,https://zotero.org/alehanderoo/items/ZS2NNILY,https://doi.org/10.48550/arXiv.2112.05124,12/11/2025 15:11 (GMT+1)
Yes,"3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities (2025)","2D features, 3D features, 3D-fusion, explain, pose","• Adopt 4DGS for flying object dynamics
• Use optical-flow regularization for motion cues
• Leverage adaptive control to reduce floaters
• Integrate StopThePop for view-consistent blending
• Explore self-calib via SLAM/3DGS-Calib ideas
• Prototype track-before-detect via multi-view differencing","• Survey 3D Gaussian Splatting literature and techniques
• Bridge tasks with shared technical modules
• Identify efficiency, realism, generalization challenges
• Provide taxonomy of 3DGS components and improvements
• Highlight opportunities across dynamic and semantic 3DGS","• Survey aligns with object-centric 4D splatting
• No LiDAR assumption; some works require it
• Multi-camera overlap broadly supported techniques
• Calibration drift addressed by SLAM, calibration modules
• Occupancy prediction remains underexplored
• Tracking/classification require additional heads","• 3D Gaussian density with covariance reparameterization
• Sigma=R S S^T R^T via quaternion rotation
• EWA splatting and alpha blending for rendering
• Nine modules: init, attributes, splatting, regularization, training
• Losses: SDS, rigidity, depth-normal, multi-view consistency
• Adaptive control: clone, split, prune strategies","• Survey; no single dataset evaluated
• Covers static, dynamic, relightable scenes
• Multi-view camera settings broadly discussed
• Annotations vary; semantics, depth, flow referenced
• 3D motion covered in dynamic 4DGS
• Availability: awesome3DGS GitHub list","• Summarizes 3DGS pipeline: initialize, splat, optimize
• Pose handled via SfM, SLAM, calibration works
• 2D features: ORB, CLIP, DINO, ControlNet
• 3D fusion: explicit Gaussians, mesh/SDF hybrids
• Training: SDS, optical-flow, rigidity, depth-normal
• Adaptive control and compression strategies reviewed","• 3DGS enables ≥30 FPS 1080p rendering
• DISTWAR achieves 2.44× gradient acceleration
• Hardware clustering yields 10.7× render speedup
• StopThePop: 1.6× faster, 2× memory saved
• Generalizable 3DGS limited synthesis range
• Dynamic 4DGS improves temporal consistency",https://github.com/qqqqqqy0227/awesome3DGS,,,https://ieeexplore.ieee.org/document/10870258/,https://zotero.org/alehanderoo/items/Z7XG34SF,https://doi.org/10.1109/TCSVT.2025.3538684,12/11/2025 15:07 (GMT+1)
Yes,Survey on Monocular Metric Depth Estimation (2025),"2D features, 3D features, explain","• Use ZoeDepth/DepthPro as per-camera depth priors.
• Ensure RGB-only inference; no LiDAR dependencies.
• Calibrate scale across cameras using shared targets.
• Enforce temporal smoothing for flying object stability.
• Avoid diffusion depth for real-time tracking.
• Evaluate intrinsics-robust models for drift tolerance.","• Review MMDE progress and trends.
• Highlight gap in recent surveys.
• Emphasize absolute-scale, zero-shot depth.
• Analyze datasets and benchmarks.
• Discuss efficiency, blurriness, generalization.","• Monocular, not multi-view; no epipolar fusion.
• Helpful for occupancy priors; not mandatory.
• No tracking/classification; integrate externally.
• Temporal consistency and flicker remain concerns.
• No Gaussian Splatting or 4D dynamics discussed.
• Lacks calibration drift self-correction methods.","• Monocular metric depth from single RGB.
• Scale-invariant and scale-shift-invariant losses.
• Adaptive binning: AdaBins, LocalBins, BinsFormer.
• CRF regularization: NeW CRFs.
• Log-depth parameterization, FOV conditioning.
• Intrinsics-free via pseudo-spherical, geometric invariance.","• Domains: indoor and outdoor scenes.
• Datasets: KITTI, NYU, nuScenes, TartanAir.
• Cameras: predominantly monocular RGB.
• Annotations: metric and relative depth maps.
• 3D motion present in driving datasets.
• Availability: mostly open benchmarks.","• Categorizes single-pass, patch-based, diffusion models.
• Intrinsics handling: canonical mapping, embeddings, spherical outputs.
• Backbones: CNNs, ViTs; multi-scale fusion.
• Depth heads: regression, adaptive bins, CRF refinement.
• Training: multi-dataset, semi/unsupervised, synthetic supervision.
• No multi-view fusion; monocular inference only.","• ZoeDepth strong zero-shot metric generalization.
• DepthPro balances detail and speed effectively.
• Patch-based improves sharpness; adds latency.
• Diffusion sharper structure; high compute cost.
• UniDepth handles unknown intrinsics robustly.
• Datasets lacking standardization hinder fair comparison.",,,,http://arxiv.org/abs/2501.11841,https://zotero.org/alehanderoo/items/2IXWVQ84,https://doi.org/10.48550/arXiv.2501.11841,12/11/2025 15:04 (GMT+1)
Yes,Vision-Based Multi-Future Trajectory Prediction: A Survey (2025),"2D features, explain, track","• Adapt PEC to 3D goal-conditioned flight endpoints.
• Use grid-sampling recurrent decoder for 3D occupancy rollout.
• Leverage diffusion for diverse 3D future hypotheses.
• Apply LDS/BOSampler for K diverse 3D tracks.
• Evaluate with 3D minADE/minFDE, KDE-NLL.
• Risk: methods assume 2D, single-view, no calibration drift.","• Survey of vision-based multi-future trajectory prediction.
• Taxonomies: frameworks, datasets, evaluation metrics.
• Compares models on standard MTP benchmarks.
• Analyzes uncertainty: accuracy, diversity, social acceptance, explainability.
• Identifies metric and dataset limitations.","• Limited multi-view, calibration, or 3D focus.
• Frameworks transferable to 3D flying trajectories.
• Anchors could be 3D goal spheres or waylines.
• Grid sampling aligns with occupancy prediction needs.
• Language-guided explainability promising for classifying intentions.
• Verify compatibility with RGB-only, rolling-shutter, drift.","• Seq2seq with social, temporal, scene encoders.
• MTP frameworks: noise, anchor, recurrent.
• Generative models: GAN, CVAE, NF, DDPM.
• Losses: variety loss, adversarial, KL, occupancy.
• Metrics: lower-bound, probability-aware, distribution-aware.
• Sampling: beam search, LDS, BOSampler.","• Domains: pedestrians, vehicles; road-user motion.
• #sequences/frames: N/A.
• #cameras/layout/overlap: N/A.
• Annotations: 2D trajectories; occasional HD maps/LiDAR.
• 3D-motion present? Rare; mostly 2D planar.
• Availability: public benchmarks; standard licenses.","• Feature extraction: LSTM/GRU/Transformer/CNN/GNN.
• Scene interaction from images, segmentation, HD maps.
• Pose strategy: N/A; calibration not addressed.
• 2D backbones; no optical flow emphasis.
• 3D fusion method: N/A.
• Training: variety loss, adversarial, KL, diffusion sampling.","• STGlow tops pedestrian minADE/minFDE among MTP.
• YNet outperforms PECNet via scene-constrained endpoints.
• Diverse sampling significantly boosts lower-bound metrics.
• ForkingPath precision and PTU ~50% maxima.
• Heatmap PEC memory-heavy; acceleration via LED.
• Runtime/FPS/memory numbers: N/A.",,,,https://ieeexplore.ieee.org/document/10937765/,https://zotero.org/alehanderoo/items/Q7LI4WQ5,https://doi.org/10.1109/TNNLS.2025.3550350,12/11/2025 15:01 (GMT+1)
Yes,A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic segmentation (2024),"2D features, 3D features, classify, explain","• Borrow multi-view projection for RGB-only multi-cam fusion.
• Adopt transformer attention for temporal multi-view aggregation.
• Use contrastive pretraining with Isaac Sim synthetic data.
• Repurpose classification heads for flying-object categories.
• Avoid LiDAR-specific operators; keep 2D feature space.
• Evaluate range-point-voxel fusion analogs for occupancy.","• Survey deep learning for 3D point clouds.
• Focus classification and semantic segmentation tasks.
• Bridge gaps in overlooked mesh and hybrid methods.
• Provide taxonomy, datasets, metrics, comparisons.
• Discuss challenges and future research directions.","• Survey emphasizes point clouds; inference requires LiDAR depth.
• Projection-based MVCNN aligns with multi-camera RGB.
• No occupancy prediction, 3D MOT, or tracking.
• No calibration drift or self-calibration discussion.
• 4D Gaussian or dynamic NeRF absent.
• Limited runtime/FPS; edge feasibility unclear.","• Taxonomy across mesh, projection, volumetric, point, hybrid.
• Supervised vs unsupervised learning paradigms.
• Metrics: OA, mAcc, F1, mIoU, IoU equations.
• Assumptions: permutation invariance, local-global aggregation.
• Attention, graph Laplacian, kernel convolutions described.","• Domains: indoor/outdoor LiDAR, RGB-D, UAV photogrammetry.
• Datasets: ModelNet, ScanObjectNN, S3DIS, SemanticKITTI.
• #sequences/frames: varies; some sequential LiDAR.
• #cameras/layout/overlap: N/A; sensor-specific.
• Annotations: per-point semantics, object classes.
• Availability: public benchmarks; licenses vary.","• Categorizes mesh, projection, volumetric, point, hybrid pipelines.
• No calibration or pose estimation covered.
• 2D backbones: MVCNN, Res2Net, harmonized pooling.
• 3D fusion: point-voxel hybrids, range-point-voxel fusion.
• Heads: classification, semantic segmentation; no tracking heads.
• Training: supervised, self/self-supervised; contrastive, generative.","• PointView-GCN OA 95.4% on ModelNet40.
• PTv2 tops ModelNet40 OA and mAcc.
• RangeFormer strong mIoU on SemanticKITTI.
• FG-Net leads Semantic3D mean accuracy.
• RepSurf-U best OA on ScanObjectNN mesh-based.
• Runtime details sparse; some real-time claims.",,,,http://arxiv.org/abs/2405.11903,https://zotero.org/alehanderoo/items/K95NMI3N,https://doi.org/10.1007/s00138-024-01543-1,12/11/2025 14:57 (GMT+1)
Yes,Evaluating Alternatives to SFM Point Cloud Initialization for Gaussian Splatting (2024),"3D features, 3D-fusion, pose","• Replace COLMAP with NerfAcc 5k point init
• Add NeRF depth distillation to GS training
• Use existing RGB-only poses; avoid LiDAR
• Benchmark under pose drift and overlap changes
• Adapt for object-centric 3D splats of flyers
• Quick-win: try large-box random init","• Remove SfM dependence for 3DGS initialization
• Leverage NeRF to bootstrap Gaussian Splatting
• Improve random initialization quality and stability
• Provide faster pipeline than COLMAP
• Evaluate with SLAM-estimated cameras","• Relevant to GS without SfM; static-centric focus
• No classify/track; add MOT/classifier modules
• Assumes accurate poses; self-calib not addressed
• Could bootstrap central-volume occupancy splats
• Depth guidance may mitigate pose drift sensitivity
• Test on fast, low-texture flying targets needed","• 3DGS alpha compositing color equation
• NeRF termination probability S(t;r) for surfaceness
• Inverse-CDF sampling for NeRF-to-GS points
• Differentiable GS depth rendering formulation
• L1 depth distillation loss with scheduled weight","• Mip-NeRF 360 indoor/outdoor multi-view scenes
• OMMO large-scale drone video scenes
• Replica, TUM sequences with SLAM poses
• #frames/#cameras: N/A
• Annotations: RGB only; photometric supervision
• Availability: listed project pages; mixed licenses","• Pretrain NerfAcc INGP 5k–30k iterations
• Sample 3D points via NeRF inverse-CDF
• Initialize GS with 500k points, SH colors
• Optional depth distillation into GS training
• Random init: large 50×50×50 box baseline
• Poses from COLMAP or ORB-SLAM3 monocular","• NeRF init often ≥ COLMAP PSNR on benchmarks
• Random large-box beats prior random baseline
• Best results: NeRF init + depth supervision
• SLAM poses faster than COLMAP, lower PSNR
• A6000: 5k NeRF ≈30s pretrain
• ORB-SLAM3 40ms/f, NerfAcc depth ≈300ms/f",https://theialab.github.io/nerf-3dgs,,,http://arxiv.org/abs/2404.12547,https://zotero.org/alehanderoo/items/9RLL9IID,https://doi.org/10.48550/arXiv.2404.12547,12/11/2025 14:55 (GMT+1)
Yes,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2021),"2D features, classify, explain","• Use ViT as 2D backbone per camera.
• Pretrain on sim RGB with masked patches.
• Smaller patches for small flying objects.
• Share weights across views; fuse downstream.
• Use attention maps for explainability.
• Risk: large-scale pretraining required for robustness.","• Pure transformer for image classification.
• Replace CNNs with patch-based Transformer.
• Scale pretraining to 14M–300M images.
• Claim: less compute than SOTA CNNs.
• Gap: limited transformers in vision at scale.","• Strong 2D features; no 3D, multi-view support.
• Attention maps useful for interpretability.
• Consider hybrid CNN+ViT for small datasets.
• No temporal modeling; add optical flow later.
• Patch size impacts small object sensitivity.
• Open: performance on aerial, sparse backgrounds.","• Image patch tokenization, 16x16 default patch size.
• Learnable class token for classification.
• Learned 1D positional embeddings with interpolation.
• Transformer encoder: MSA, MLP, GELU, LayerNorm.
• Supervised cross-entropy; label smoothing, weight decay.
• Preliminary masked patch prediction self-supervision.","• Natural images; single-image classification.
• ImageNet: 1.3M; ImageNet-21k: 14M; JFT: 303M.
• Single camera; no multi-view overlap.
• Annotations: class labels only.
• 3D motion: N/A; static images.
• I21k public; JFT private; VTAB benchmarks.","• Split images into patches; linear embed patches.
• Add positional embeddings; prepend class token.
• Stack Transformer blocks: MSA, MLP, LayerNorm.
• Classification head; no detection or tracking heads.
• Calibration: N/A; single-view RGB only.
• Training: Adam, label smoothing, high-res fine-tuning.","• 88.55% ImageNet top-1; 90.72% ReaL.
• 94.55% CIFAR-100; 77.63 VTAB.
• Outperforms BiT with less pretrain compute.
• Hybrids help at small compute budgets.
• ViT memory-efficient; large per-core batch sizes.
• Inference speed comparable to ResNets on TPUv3.",https://github.com/google-research/vision_transformer,,,http://arxiv.org/abs/2010.11929,https://zotero.org/alehanderoo/items/P8IP2F85,https://doi.org/10.48550/arXiv.2010.11929,12/11/2025 14:48 (GMT+1)
Yes,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification (2021),"2D features, classify, explain","• Use CrossViT-S/18 as per-camera 2D backbone.
• Adopt CLS-agent cross-attention for multi-scale fusion.
• Prototype cross-view CLS-to-patch fusion module.
• Add temporal adapters for motion classification cues.
• Profile FPS/memory under 5-camera pipeline.
• Risk: single-image design; no 3D/MOT support.","• Improve ViT with multi-scale token fusion.
• Address missing multi-scale representations in transformers.
• Reduce fusion attention complexity to linear time.
• Increase ImageNet accuracy with modest compute.","• Limited direct fit to 3D flying-object pipeline.
• Agent-token concept promising for cross-view fusion.
• Offers attention maps for basic explainability.
• No calibration drift handling or epipolar cues.
• No track-before-detect, occupancy, or MOT.
• Good 2D backbone; integrate cautiously.","• CLS token as cross-branch agent.
• Cross-attention: CLS queries other-branch patches.
• Linear-time fusion vs quadratic all-attention.
• Standard ViT MSA+FFN per branch encoders.
• Positional embeddings added to all tokens.","• ImageNet1K: 1.28M train, 50k val.
• Single-view RGB images, no multi-camera.
• Annotations: class labels only.
• No 3D motion; static images.
• Code and models publicly available.","• Dual-branch ViT with small/large patch tokens.
• Cross-attention fusion between branches via CLS.
• Compared fusions: all, CLS, pairwise, cross-attention.
• Classification head using fused CLS tokens.
• Training: 300 epochs, AdamW, strong augmentation.
• Compute: 32 GPUs, batch 4096, cosine schedule.","• Top-1 up to +2% over DeiT.
• Cross-attention best among fusion schemes.
• Throughput: 640 img/s (CrossViT-15 @224).
• FLOPs 5.6–9.5G; params 26.7–44.3M.
• Transfer competitive on five downstream datasets.
• Linear fusion lowers compute vs all-attention.",https://github.com/IBM/CrossViT,,,http://arxiv.org/abs/2103.14899,https://zotero.org/alehanderoo/items/V66RCSDS,https://doi.org/10.48550/arXiv.2103.14899,12/11/2025 14:46 (GMT+1)
Yes,Point Transformer V3,3D features,"• Use PTv3 as 3D feature backbone
• Apply on 3DGS/4DGS reconstructed point clouds
• Integrate with multi-view RGB via MVS/NeRF-to-points
• Add classifier/tracker heads for flying objects
• Downsample dense 3DGS points to PTv3-friendly sparsity
• Benchmark receptive fields for small airborne targets","• Bridge accuracy-efficiency trade-off in point transformers
• Enable scaling receptive fields efficiently
• Replace costly KNN and RPE operations
• Achieve SOTA across 3D tasks","• Strong 3D backbone; no multi-camera RGB support
• No self-calibration or pose estimation
• No explicit dynamic/static separation
• Promising after 3D volumetric reconstruction
• Serialization design may aid 4D dynamics
• Lacks explainability tools (Grad-CAM/TREx)","• Scaling principle: simplicity enables performance via scale
• Serialization via space-filling curves (Z-order, Hilbert)
• Patch attention with windowed dot-product
• Enhanced conditional positional encoding (xCPE)
• Pre-norm transformer blocks with LayerNorm","• Indoor datasets: ScanNet, ScanNet200, S3DIS
• Outdoor datasets: nuScenes, SemanticKITTI, Waymo
• %sequences/frames: N/A
• %cameras/layout/overlap: LiDAR only, N/A overlap
• Annotations: semantics, instances, 3D boxes
• 3D motion present: Waymo multi-frame detection","• Serialize points with Z-order/Hilbert and variants
• Group patches; pad; apply patch attention
• Patch interaction: Shift Dilation/Patch/Order, Shuffle Order
• xCPE sparse conv before attention
• U-Net backbone, grid pooling, pre-norm blocks
• Heads: CenterPoint detection; PPT joint pretraining","• ScanNet test: +3.7 mIoU over PTv2
• S3DIS 6-fold: +4.2 mIoU vs PTv2
• nuScenes val: +2.0 mIoU vs SphereFormer
• SemanticKITTI val: +3.0 mIoU vs SphereFormer
• Waymo single-frame: +3.3 mAPH over FlatFormer
• 3.3x faster, 10.2x less memory than PTv2",https://github.com/Pointcept/PointTransformerV3,,,https://ieeexplore.ieee.org/document/10658198/,https://zotero.org/alehanderoo/items/IJS7J85C,https://doi.org/10.1109/CVPR52733.2024.00463,15/10/2025 18:06 (GMT+2)
Yes,Mamba4D: Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models,"3D features, 3D-fusion, classify","• Reuse Mamba temporal backbone for 4DGS feature volumes.
• Replace transformers with Mamba for efficiency.
• Inject 4D positional encoding into 3DGS tokens.
• Evaluate cross-temporal scanning on multi-view fused volumes.
• Risk: relies on point clouds, not RGB inputs.
• Quick-win: benchmark speed/memory gains in 3D tracker.","• Efficient 4D point cloud video backbone.
• Handle irregular spatial distribution, temporal inconsistencies.
• Reduce transformer quadratic complexity on long sequences.
• Disentangle spatial-temporal modeling for better dynamics.","• Strong 4D fusion ideas; input modality mismatch.
• No multi-camera overlap handling or self-calibration.
• No tracking heads or probabilistic 3D trajectories.
• Useful SSM blocks for long-range dynamics.
• Could adapt to 3D occupancy grids.
• Lacks explainability methods; no Grad-CAM/TREx.","• State Space Models (SSMs) with linear complexity.
• Mamba blocks for data-dependent sequence modeling.
• Disentangled intra-frame spatial and inter-frame temporal modules.
• 4D positional encoding: x,y,z,t via MLP.
• Temporal pooling for short-term dynamics aggregation.
• Spatio-temporal scanning strategies ensure causality.","• MSR-Action3D; action recognition; video-level labels.
• HOI4D; action segmentation; frame labels.
• HOI4D: 2971 train, 892 test scenes.
• 150 frames/sequence; 2048 points/frame.
• Synthia4D; semantic segmentation; per-point labels.
• #cameras/overlap: N/A; 3D motion present.","• Anchor frame selection with temporal stride.
• FPS sampling of spatial anchor points.
• KNN point tubes across short clips.
• Intra-frame Spatial Mamba with temporal max-pooling.
• Inter-frame Temporal Mamba with scanning strategies.
• 4D positional encoding fused into tokens.","• +10.4% accuracy on MSR-Action3D.
• +0.7 F1 on HOI4D action segmentation.
• +0.19 mIoU on Synthia4D segmentation.
• 87.5% GPU memory reduction.
• 5.36× faster runtime on long sequences.
• Cross-temporal scanning best, 92.68% accuracy.",https://github.com/IRMVLab/Mamba4D,,,,https://zotero.org/alehanderoo/items/EQYTSDBM,,15/10/2025 18:04 (GMT+2)
Yes,Rethinking Point Cloud Data Augmentation: Topologically Consistent Deformation,"3D features, classify","• Use SinPoint on multi-view reconstructed 3D points
• Augment 4DGS Gaussians with topology-preserving offsets
• Apply SSF for flying object classifier pretraining
• Combine Markov affine augmentations; constrain |Aω|≤1
• Risk: deformations may harm rigid motion dynamics
• Quick test: augment pseudo-LiDAR of drones/birds","• Improve point cloud DA diversity with topology preserved
• Address distortion in existing mix/deform augmentations
• Theorize why augmentation improves performance via variance
• Introduce homeomorphic sine mapping for consistency
• Expand distribution via Markov chain augmentations","• No multi-camera, pose, or tracking support
• Strong for 3D feature augmentation pretraining
• May help sparse, noisy MVS point clouds
• Needs temporal-consistent extension for tracking
• Parameter tuning crucial to avoid folding
• Effect on occupancy fields or splats unclear","• Theorem: augmentation increases dataset variance
• Homeomorphism ensures topological consistency post-deformation
• Sine mapping homeomorphism when -1≤Aω≤1
• Residual deformation: P' = P + g(P)
• Jacobian analysis for orientation preservation and folding risk","• Domains: synthetic objects, real scanned objects, scenes
• Datasets: ModelNet40, ReducedMN40, ScanObjectNN
• Part segmentation: ShapeNetPart; scenes: S3DIS, SemanticKITTI
• Cameras/layout/overlap: N/A
• Annotations: class labels, part labels, semantic labels
• 3D motion present: none; public datasets","• SinPoint SSF/MSF sine-based homeomorphic deformations
• Markov chain combining affine augmentations
• Applied to point cloud backbones: PointNet++, DGCNN
• Pose strategy: N/A; calibration-free not addressed
• 2D backbones/optical flow: N/A
• Training: SGD, baseline losses; minimal runtime overhead","• SOTA on ScanObjectNN: 90.2 OA
• +5.9% average OA on real datasets
• Improved robustness under noise, rotation, scaling, dropout
• Fast training: 10x faster than PointAugment
• Consistent gains across many backbones
• Scenes: +7.6% mIoU on SemanticKITTI",https://github.com/CSBJian/SinPoint,,,,https://zotero.org/alehanderoo/items/ZAG9LXJY,,15/10/2025 18:01 (GMT+2)
Yes,DG-MVP,"2D features, 3D-fusion, classify","• Adapt Depth Pooling to multi-camera feature fusion.
• Use MMP for robust per-object descriptors.
• Apply occlusion/density augmentations to training data.
• Risk: assumes clean per-object segmentations.
• Risk: no temporal or tracking capabilities.
• Quick win: max-pool per-camera logits across views.","• 3D DG for point cloud classification.
• Address synthetic-to-real domain shift.
• Point-based backbones discard useful points.
• Robustness to occlusion, missing points needed.","• Not focused on flying, dynamic targets.
• Single-object classification, no tracking.
• Multi-view fusion concept transferable to cameras.
• No pose or self-calibration methods.
• Add explainability, e.g., Grad-CAM on ResNet18.
• Consider temporal fusion and 4DGS integration.","• Multi-view depth projections alleviate missing points.
• Depth Pooling: max across six views.
• Multi-scale Max Pooling captures local/global features.
• Dual-branch CE losses with weights.
• Augmentations simulate holes and non-uniform density.","• PointDA-10: ModelNet, ShapeNet, ScanNet.
• Sim-to-Real: ModelNet/ShapeNet → ScanObjectNN.
• Six orthogonal projections per object.
• Class labels only; single-object samples.
• No motion; static shapes.
• Public datasets; standard academic licenses.","• Project point clouds to six depth images.
• ResNet18 extracts per-view 2D features.
• Depth Pooling fuses multi-view features.
• MMP strips aggregate multi-scale features.
• Dual branches: avg-pool head, MMP head.
• Adam, lr 0.001, bs 32, 160–200 epochs.","• 55.29% avg on PointDA-10; beats SUG.
• Outperforms several UDA without target access.
• 70.92% M→SO; 66.74% M→SOB.
• Depth Pooling outperforms average and view pooling.
• MMP boosts S→S* OA by 3.73%.
• ResNet18 best among tested backbones.",,,,http://arxiv.org/abs/2504.12456,https://zotero.org/alehanderoo/items/NKQTZ4SY,https://doi.org/10.48550/arXiv.2504.12456,15/10/2025 17:59 (GMT+2)
Yes,Point Cloud Upsampling Using Conditional Diffusion Module with Adaptive Noise Suppression,"3D features, 3D-fusion","• Use ANS to denoise 3DGS or voxel features.
• Adapt conditional diffusion to refine occupancy volumes.
• Integrate TT for multi-scale 3D feature aggregation.
• Replace CD/EMD with diffusion-based training.
• Beware static-shape training; lacks temporal dynamics.
• Quick test: ANS on fused multi-view point clouds.","• Robust point cloud upsampling under noise.
• Address CD/EMD sensitivity to outliers.
• Improve feature dominance and structure preservation.
• Leverage diffusion denoising for upsampling.","• No multi-camera RGB; limited thesis alignment.
• Valuable for 3D denoising and feature fusion.
• No pose, classification, or tracking addressed.
• Targets static shapes, not flying objects.
• Could post-process 3DGS geometry refinement.
• Requires adding 2D semantics and temporal cues.","• Conditional DDPM guides densification from noise.
• Reverse process predicts noise; MSE training.
• No CD/EMD; inherent point correspondences.
• ANS weights neighborhoods via attention mechanisms.
• TT combines cyclic, ancestral, local-global attention.","• PUGAN, PU1K synthetic shape patches.
• 24k PUGAN, 69k PU1K training patches.
• Inputs 256 points; ground truth 1,024.
• Tests: 27 PUGAN, 127 PU1K shapes.
• Cameras and overlap: N/A.
• 3D motion: none; static objects.","• Two-branch CP-Net and DP-Net architecture.
• ANS removes noisy neighbors via learned weights.
• TT decoder uses cyclic and ancestral operations.
• Local and global attention for feature fusion.
• Cross-attention between CP-Net and DP-Net.
• Losses: diffusion noise MSE; CP-Net reconstruction MSE.","• SOTA CD/HD on PUGAN 4× and 16×.
• PU1K 4×: CD 0.155, HD 1.588.
• Strong robustness to Gaussian noise across levels.
• Best under random and Laplace noise.
• Ablations confirm ANS and TT effectiveness.
• Runtime and memory: N/A.",https://github.com/Baty2023/PDANS,,,,https://zotero.org/alehanderoo/items/LQMWZ8Y2,,15/10/2025 17:56 (GMT+2)
Yes,PointNeXt,"3D features, classify","• Reuse training strategies in 3D feature backbones.
• Adopt ∆p normalization in point encoders.
• Integrate InvResMLP for 3D point modules.
• Beware point-cloud requirement; pipeline starts from images.
• Quick-win: test color drop, label smoothing, AdamW.
• Evaluate on per-object 3DGS point samples.","• Revisit PointNet++ training and scaling strategies.
• Claim training dominates recent gains over architecture.
• Propose PointNeXt with InvResMLP scaling.
• Boost classification and segmentation performance significantly.
• Offer faster inference than recent SOTA.","• Strong for 3D features; no multi-view support.
• No pose self-calibration; not camera-based.
• No temporal modeling; unsuitable for tracking.
• Helpful backbone after 3D reconstruction stage.
• Radius normalization insight transferable to 4DGS.
• Need validation on flying, sparse objects.","• Set Abstraction with hierarchical feature learning.
• Relative position normalization by neighborhood radius.
• Inverted residual bottleneck MLP design.
• Separable MLPs to reduce computation.
• Cross-entropy with label smoothing loss.
• AdamW optimizer with cosine decay.","• Benchmarks: S3DIS, ScanNet, ScanObjectNN, ModelNet40, ShapeNetPart.
• #scans/objects: S3DIS 271 rooms; ScanNet 1201/312/100.
• #cameras/layout/overlap: N/A.
• Annotations: per-point semantics, object labels, parts.
• 3D motion present?: No, static scenes.
• Availability/license: Public benchmarks; license N/A.","• Improved augmentations: resampling, color drop, auto-contrast, height.
• Optimization: label smoothing, AdamW, cosine scheduler.
• Receptive field tuning: radius search, ∆p normalization.
• Architecture: InvResMLP, residuals, separable MLPs, stem.
• Tasks: classification, segmentation, part segmentation.
• Pose, optical flow, multi-view fusion: N/A.","• ScanObjectNN OA: 77.9→86.1 without architecture changes.
• PointNeXt-S OA: 87.7, 10× faster than PointMLP.
• S3DIS 6-fold mIoU: up to 74.9.
• Color drop adds +5.9 mIoU on S3DIS.
• Residual removal drops mIoU by 6.5.
• Throughput higher than Point Transformer, similar FLOPs.",https://github.com/guochengqian/pointnext,,,http://arxiv.org/abs/2206.04670,https://zotero.org/alehanderoo/items/FB4IQPSJ,https://doi.org/10.48550/arXiv.2206.04670,15/10/2025 17:49 (GMT+2)
Yes,Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning,"3D features, classify","• Borrow complete-to-partial distillation for 4DGS features
• Use frame-level contrastive temporal supervision
• Simulate partial views via occlusion-aware renderings
• Distill from complete multiview reconstructions to partial RGB
• Beware point-cloud to RGB domain gap
• Test on flying-object 3D tracking benchmarks","• Self-supervised 4D point cloud representation learning
• Leverage unlabeled dynamic sequences efficiently
• Unify geometry and motion representations
• Overcome static-only 3D pretraining limitations
• Improve downstream 4D segmentation and recognition","• Not multi-camera; single-sensor point clouds
• No flying-object focus
• Strong temporal feature learning design
• Asymmetric distillation seems broadly applicable
• Lacks pose self-calibration components
• No 3D tracking heads included","• Teacher-student knowledge distillation across time
• Complete-to-partial 4D distillation paradigm
• Frame-level contrastive losses: L_geo, L_time
• Asymmetric teacher weaker than student
• Temporal windowed prediction heads","• HOI4D indoor scenes; actions, semantics
• 2971 train, 892 test sequences
• Sequences: 150–300 frames per sequence
• Single viewpoint point clouds; no multi-camera overlap
• Annotations: action labels, semantic masks
• 3D motion present; public datasets","• Generate partial-view sequences via camera trajectory sampling
• Occlusion-based visible point sampling per frame
• Student: 4D Conv + transformer
• Teacher: 4D Conv only, asymmetric
• Frame-level contrastive distillation, window size three
• Predict adjacent frames via dynamic heads","• +2.5% HOI4D action segmentation accuracy
• +1.0% HOI4D semantic segmentation mIoU
• +1.0% Synthia 4D semantic mIoU
• +2.1% MSR-Action3D accuracy
• Asymmetric teacher superior to symmetric
• Natural trajectories outperform random sampling",https://github.com/dongyh20/C2P,,,https://ieeexplore.ieee.org/document/10204798/,https://zotero.org/alehanderoo/items/PZ4LQFRJ,https://doi.org/10.1109/CVPR52729.2023.01694,15/10/2025 17:47 (GMT+2)
Yes,Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos,"3D features, 3D-fusion, classify, explain","• Use 4D attention for dynamic 3D feature aggregation.
• Insert after 4DGS to fuse temporal object splats.
• Avoid explicit tracking; rely on video-level attention.
• Risks: needs reliable 3D points from RGB.
• Test on synthetic flying objects, multi-view reconstructions.
• Benchmark classification heads for aerial object categories.","• Model raw point cloud videos without point tracking.
• Handle inconsistent points across frames robustly.
• Avoid voxelization's compute and latency overhead.
• Exploit self-attention for global spatiotemporal correlation.
• Support 3D action recognition and 4D segmentation.","• No multi-view pose estimation; calibration-free not addressed.
• No 2D features or optical flow integration.
• Strong for 3D-temporal fusion without tracking.
• Potentially complements 4DGS dynamic feature modeling.
• Not focused on occupancy or 3D MOT.
• Scalability to long sequences and memory unclear.","• Point 4D convolution over spatiotemporal local neighborhoods.
• Kernel generated from continuous 4D displacements function.
• Video-level multi-head self-attention across all local areas.
• 4D coordinate embedding added to local features.
• Residual, LayerNorm, ReLU within transformer blocks.
• Losses unspecified; metrics: accuracy, mIoU.","• MSR-Action3D: 567 videos, 23K frames.
• NTU60: 56K videos, 3 cameras; NTU120: 114K videos.
• Synthia4D: 6 driving videos, stereo RGB-D reconstruction.
• Annotations: action labels; per-point semantic classes.
• 3D motion present; both objects and cameras moving.
• Multi-camera overlap/availability/license: N/A.","• Subsample frames (stride), FPS points, search spatial neighbors.
• Point 4D convolution with radii rt, rs.
• Embed 4D coordinates; apply video-level multi-head self-attention.
• Action: max-pool global feature, MLP classifier.
• Segmentation: feature propagation via inverse-distance interpolation.
• Training: SGD 50 epochs, lr 0.01 with decay.","• Outperforms baselines on NTU60/120 accuracy.
• MSR-Action3D: up to 90.94% accuracy.
• Synthia4D mIoU: 83.16% using 3 frames.
• Video-level attention > frame-level attention significantly.
• Runtime reduced 1903ms vs 3DV-PointNet++.
• Best performance with temporal radius rt=1.",,,,https://ieeexplore.ieee.org/document/9578674/,https://zotero.org/alehanderoo/items/LUG95JVY,https://doi.org/10.1109/CVPR46437.2021.01398,15/10/2025 17:45 (GMT+2)
Yes,"Multi-Modal UAV Detection, Classification and Tracking Algorithm -- Technical Report for CVPR 2024 UG2 Challenge","2D features, 3D features, 3D-fusion, classify, pose, track","• Reimport keyframe selection and soft-vote.
• Adapt Kalman tracking, AR smoothing for RGB triangulation.
• Replace LiDAR pipeline with 3DGS fusion.
• Add optical flow cues in keyframe ranking.
• Evaluate multi-camera overlap constraints explicitly.
• Integrate explainability (Grad-CAM) for classification.","• Multi-modal UAV detection, classification, 3D tracking.
• Robust under extreme weather and long ranges.
• Fuse cameras, Lidars, and radar effectively.
• Overcome small target visibility in images.
• Unsupervised 3D pose without reliable visual depth.","• Strong for flying objects; non-RGB pose mismatch.
• Stereo cameras unused for 3D triangulation.
• No self-calibration; extrinsics assumed known.
• No Gaussian Splatting or 3D feature volumes.
• Temporal fusion ideas transferable to RGB-only.
• Need multi-view association across overlapping cameras.","• Late fusion across heterogeneous sensors.
• Sequence fusion via cosine feature similarity.
• ROI-based keyframe selection using YOLOv9-e.
• Attention LSTM for temporal point features.
• Kalman filtering and AR trajectory completion.
• Polynomial bias correction for center regression.","• Domain: anti-UAV MMUAD challenge dataset.
• 102 train, 16 val, 59 test sequences.
• Sensors: stereo fisheye, two Lidars, mmWave radar.
• Unsynchronized, varying FoVs, partial overlap.
• Annotations: UAV type, 3D pose points.
• Availability/license: dataset N/A; code available.","• Two pipelines: image classification, 3D pose.
• No self-calibration; rely provided sensor rigs.
• EfficientNet-B7 classifier with soft-vote aggregation.
• YOLOv9-e zero-shot ROI and keyframes.
• Lidar/radar clustering, attention LSTM, MLP heads.
• Kalman filter tracking; AR completion; B-spline smoothing.","• Rank 1 on UG2+ MMUAD challenge.
• Pose MSE 2.21375, best among teams.
• Classification accuracy 0.8136, top performance.
• Detector acc 0.9998, recall 0.9184.
• Center regression MSE reduced 0.27→0.05.
• Runtime/memory: N/A.",https://github.com/dtc111111/Multi-Modal-UAV,,,http://arxiv.org/abs/2405.16464,https://zotero.org/alehanderoo/items/9B48MD7W,https://doi.org/10.48550/arXiv.2405.16464,13/10/2025 23:57 (GMT+2)
Yes,YOLOv5 Drone Detection Using Multimodal Data Registered by the Vicon System,2D features,"• Use YOLOv5 as 2D detector baseline
• Adopt MCD for detection-to-tracking evaluation
• Pretrain on AirSim, finetune on real multi-view
• Add epipolar constraints for multi-view association
• Beware marker-based labeling domain bias
• Add temporal cues: optical flow, tracklet smoothing","• Precise 2D drone detection for 3D tracking
• Multi-camera synchronized scenes with overlapping views
• Automatic labeling via Vicon 3D–2D projection
• Propose MCD metric for tracking suitability
• Evaluate YOLOv5/v8 on real and synthetic data","• Relevant multi-camera, overlapping indoor setup
• Only 2D detection; no 3D fusion/tracking
• No calibration-free extrinsics estimation
• Indoor domain; outdoor generalization uncertain
• MCD metric helpful for 3D tracking
• No explainability or Gaussian splatting components","• YOLOv5 architecture: CSPDarknet, PANet, SPPF
• Transfer learning from COCO weights
• 3D-to-2D projection using camera extrinsics
• MCD: centers' distance with thresholds
• mAP: 11-point interpolation and smoothing","• Real HML indoor lab scene
• HML: 4 cameras, ~18k frames, 25 fps
• AirSim: 8 cameras, ~40k images, 25 fps
• Overlapping views, synchronized multi-camera setup
• Vicon MoCap 3D ground truth
• 3D motion present; datasets freely available","• Multi-camera recording with Vicon synchronization
• Calibrated extrinsics; no self-calibration
• Marker cross estimates drone pose and size
• Project 3D bbox to 2D for labels
• Train YOLOv5 with COCO pretraining
• Albumentations, NMS; evaluate mAP and MCD","• AirSim mAP@0.1:0.9 equals 0.89
• AirSim mAP@0.5:0.95 equals 0.83
• HML mAP@0.1:0.9 equals 0.41
• HML mAP@0.5:0.95 equals 0.09
• HML MCD ≈6.78 px at threshold 10
• YOLOv8 higher mAP; worse HML MCD",,,,https://www.mdpi.com/1424-8220/23/14/6396,https://zotero.org/alehanderoo/items/J9KS66HG,https://doi.org/10.3390/s23146396,13/10/2025 23:55 (GMT+2)
Yes,Generating synthetic data for deep learning-based drone detection,"2D features, classify","• Use pipeline to pretrain 2D detectors.
• Generate synchronized multi-camera overlapping sequences.
• Export depth/pose labels to aid 3D modules.
• Balance color/size distributions; avoid tiny targets initially.
• Evaluate sim2real transfer; fine-tune on real.
• Produce segmentation masks for 2D instance extraction.","• Labeled drone data scarce, expensive, restricted.
• Propose Unreal+AirSim pipeline for synthetic drone data.
• Study effects of shape, color, size.
• Evaluate training strategies for YOLOv5 detection.","• Helps data scarcity; limited to 2D detection.
• No multi-view overlap, pose, 3D fusion.
• Useful for flying-object appearance diversity modeling.
• Adopt size filtering to reduce FPs.
• Need synchronized multi-camera, depth, timing outputs.
• Verify transfer to real, birds vs drones disambiguation.","• Assumes synthetic realism can benefit detection generalization.
• Uses mAP@0.5 and mAP@0.5:0.95.
• Defines FNR, FPR, FDR formulas.
• Objectness (confidence) loss monitored during training.","• Domain: drone detection in urban 3D scenes.
• ~41,000 RGB images across campaigns.
• Five fixed cameras; overlap unstated.
• Nine drone CAD models, multiple colors.
• Annotations: bounding boxes, automatic labels.
• Availability/license: N/A.","• Unreal Engine 4.25 + Microsoft AirSim simulation.
• Drone control module generates trajectories.
• Data module captures parallel RGB/IR/segmentation.
• Five fixed camera positions; calibration N/A.
• YOLOv5l fine-tuned from COCO, 640x640.
• Ablations: color, shape, size; remove small boxes.","• Color/shape significantly affect detection performance.
• Some models easier than others; Model A hardest.
• Adding color variants reduces FNR on matching sets.
• Removing small objects increases mAP notably.
• Size mismatch increases FPR and multiple boxes.
• Unknown models generalize poorly versus known.",,,,http://aip.scitation.org/doi/abs/10.1063/5.0180345,https://zotero.org/alehanderoo/items/EMEHLWW8,https://doi.org/10.1063/5.0180345,13/10/2025 23:51 (GMT+2)
Yes,Multiview Detection with Feature Perspective Transformation,"2D features, 3D-fusion, classify, pose","• Reuse feature-map perspective projection for multiview fusion.
• Extend projection to 3D voxels for flying objects.
• Replace ground-plane assumption with height-discretized volumes.
• Add self-calibration from dynamics; relax calibration dependence.
• Incorporate optical flow for temporal 2D features.
• Use 3D dilated convolutions for volumetric spatial aggregation.","• Robust multiview pedestrian detection under heavy occlusion.
• Improve multiview aggregation beyond anchor-based features.
• Replace CRF spatial reasoning with CNN convolution.
• Provide synthetic dataset to analyze occlusions.","• Strong multiview fusion; mismatched ground-plane for flying.
• Requires calibrated cameras; thesis seeks calibration-free.
• No tracking or classification beyond occupancy.
• Idea generalizes to 3D feature volumes.
• Coordinate channels helpful for geometric reasoning.
• Evaluate robustness to calibration noise and height variation.","• Anchor-free feature sampling via perspective transformation matrices.
• Ground-plane BEV assumption, z=0 occupancy modeling.
• Large-kernel dilated convolutions approximate CRF spatial aggregation.
• L2 regression on Gaussian-smoothed occupancy heatmaps.
• Combined loss with head/foot heatmap supervision.
• Coordinate channels encode ground-plane positions.","• Wildtrack: 400 frames, 7 calibrated cameras.
• MultiviewX: 400 frames, 6 calibrated cameras.
• Overlap: 3.74 and 4.41 camera coverage averages.
• Annotations: ground-plane occupancy grid cells.
• 3D motion across frames; no temporal labels.
• Availability: code and MultiviewX on GitHub.","• ResNet-18 shared backbone for per-view feature maps.
• Requires known intrinsics/extrinsics; not calibration-free.
• Project feature maps to ground plane using calibrations.
• Concatenate projected maps with XY coordinate channels.
• Dilated large-kernel convolutions aggregate spatial neighbors.
• L2 loss on POM and head/foot heatmaps.","• Wildtrack MODA 88.2, +14.1 over prior SOTA.
• MultiviewX MODA 83.9, +8.7 over Deep-Occlusion.
• Feature-projection beats anchors and result-projection.
• Large kernels improve MODA by +11.3 and +6.7.
• Memory constraints require downsampling; reasonable accuracy maintained.
• Single-view loss adds modest gains (−1.2% without).",https://github.com/hou-yz/MVDet,,,http://arxiv.org/abs/2007.07247,https://zotero.org/alehanderoo/items/RWX29XED,https://doi.org/10.48550/arXiv.2007.07247,13/10/2025 23:48 (GMT+2)
Yes,Lifting Multi-View Detection and Tracking to the Bird’s Eye View,"2D features, 3D features, 3D-fusion, pose, track","• Reuse bilinear sampling for early 3D fusion.
• Adopt BEV temporal association for 3D tracking.
• Extend to full 3D volume, not ground-plane.
• Integrate self-calibration from moving targets.
• Evaluate depth splatting for cross-scene robustness.","• Improve MTMC detection/tracking via BEV lifting.
• Compare parameter-free and parameterized lifting strategies.
• Unify pedestrian and vehicle multi-view tracking.
• Early temporal fusion for robust association.
• Address cross-scene generalization challenges.","• Strong multi-view overlap handling aligns with thesis.
• Ground-plane assumption mismatches flying objects.
• No 3D occupancy; vertical dimension collapsed.
• Requires calibrated cameras; no self-calibration shown.
• Temporal BEV tracking design highly applicable.
• Explore triangulation gating for aerial 3D volume.","• Static cameras with large FOV overlap assumption.
• Pinhole camera model, perspective homography (Eq. 1).
• Voxel querying with bilinear sampling (Eq. 2).
• Depth splatting with discrete depth distributions.
• CenterNet-style BEV heatmap and offset heads.
• Losses: Focal, L1, Smooth L1; Kalman filter.","• Wildtrack: 7 cams, 400 frames, 2 fps, 1080p.
• MultiviewX: 6 cams, 400 frames, synthetic, 1080p.
• Overlap strong; average 3.7–4.4 views per target.
• Annotations: BEV grid centers; ground-plane occupancy.
• Synthehicle: 3–8 cams, varied weather, classes three.
• Public datasets; code available; calibrations provided.","• Encode per-camera RGB; project features to shared BEV.
• Lifting: homography, bilinear sampling, depth splatting, deformable attention.
• Temporal aggregation via history BEV concatenation.
• Heads: BEV heatmap, xy offset, motion offset.
• Calibrated pose K,R,t; extrinsic noise augmentation.
• Training: Adam, one-cycle, ImageNet init, grad accumulation.","• SOTA detection on Wildtrack, MultiviewX (MODA).
• SOTA tracking IDF1, MOTA across pedestrian datasets.
• Bilinear sampling competitive; depth splatting strong.
• Deformable attention overfit; underperformed on small data.
• Poor cross-scene generalization on Synthehicle.
• Runtime/memory: N/A.",https://github.com/tteepe/TrackTacular,,,https://ieeexplore.ieee.org/document/10678065/,https://zotero.org/alehanderoo/items/WTLRDNJB,https://doi.org/10.1109/CVPRW63382.2024.00071,13/10/2025 23:46 (GMT+2)
Yes,Stacked Homography Transformations for Multi-View Pedestrian Detection,"2D features, 3D-fusion, classify, pose","• Reuse SHOT-like stacked-plane BEV projection module
• Adapt planes to volumetric 3D for flying objects
• Integrate soft selection with optical flow features
• Use calibration-light homography from minimal labels
• Evaluate multi-camera overlap robustness for aerial targets
• Combine with 4DGS for dynamic object volumes","• Robust multi-view pedestrian BEV occupancy detection
• Establish accurate 3D correspondences across views
• Efficient alternative to costly 3D volumetric projection
• Overcome misalignment of pure ground-plane projection","• Strong multi-view feature alignment; good fusion baseline
• Ground-plane assumption limits flying-object applicability
• No tracking or 3D probabilistic state estimation
• No Gaussian Splatting; effectively 2.5D approach
• Selection map visualizations aid partial explainability
• Extend to voxels or 4DGS for airborne targets","• SHOT approximates 3D projection by stacked homographies
• Soft selection predicts per-pixel height likelihoods
• Pinhole model; homography H_i^k formulation
• MSE losses for head/foot and BEV maps","• WILDTRACK: 7 static cams, heavy overlap
• 12x36 m scene; 400 annotated frames
• MultiviewX: 6 cams; synthetic Unity dataset
• 1080x1920 frames; 2 FPS annotations
• BEV occupancy labels; no identity tracking
• Public datasets; licenses unspecified; code supplementary","• ResNet‑18 dilated backbone per view
• Predict head/foot heatmaps for supervision
• Stack D homographies to multiple height planes
• Softmax selection masks weight per homography
• Project features to BEV, concatenate, classify occupancy
• Calibration-light via H0; SGD training details provided","• SOTA MODA: 90.2 WILDTRACK, 88.3 MultiviewX
• +2.0 MODA over MVDet; +4.4 MultiviewX
• Higher recall; similar precision vs baselines
• Larger D improves; D=6 OOM
• Less reliant on large-kernel cross-view fusion
• Generalizes across unseen cameras/scenes split",,,,https://ieeexplore.ieee.org/document/9710265/,https://zotero.org/alehanderoo/items/2SMBPD9V,https://doi.org/10.1109/ICCV48922.2021.00599,13/10/2025 23:43 (GMT+2)
Yes,GS-CPR,"2D features, 3D features, 3D-fusion, pose","• Use GS-CPR to refine camera extrinsics self-calibrated
• Train 3DGS background; mask dynamics when building
• Apply ACT for cross-camera exposure differences
• Use MASt3R for robust cross-view RGB matching
• Use 3DGS depth to seed occupancy initialization
• Validate on overlapping multi-camera scenes","• Test-time camera pose refinement using 3DGS
• Replace iterative NeRF refinement with one-shot
• Avoid training descriptors; use MASt3R RGB matching
• Improve APR/SCR pose accuracy and robustness
• Address exposure mismatch via adaptive color transform","• Strong fit for pose calibration of RGB cameras
• Lacks multi-camera fusion, but adaptable
• No flying object classification or tracking
• Requires decent initial pose proximity
• Prebuilt 3DGS dependency may limit online deployment
• Consider incremental 3DGS and multi-view matching","• 3DGS for novel view synthesis and depth
• Exposure-adaptive affine color transform MLP
• 2D-3D matches via rendered depth and correspondences
• PnP with RANSAC for pose estimation
• Relative pose variant with scale alignment","• Indoor 7Scenes, 12Scenes; outdoor Cambridge Landmarks
• Exact frames counts N/A
• Single RGB queries; prebuilt 3DGS per scene
• SfM ground-truth poses for training 3DGS
• Dynamic objects filtered during 3DGS training","• Initial pose from APR/SCR estimator
• Render RGB and depth with 3DGS+ACT
• Match query and render using MASt3R
• Build 2D-3D correspondences from depth
• Solve PnP+RANSAC for refined pose
• GS-CPRrel: relative pose, scale from depth","• SOTA on 7Scenes, 12Scenes median errors
• Significant gains for DFNet, Marepo, ACE
• Outperforms NeRF-based methods in accuracy
• One-shot runtime ~0.18–0.19 seconds per query
• ACT improves outdoor Cambridge performance
• MASt3R matcher best in ablations",https://xrim-lab.github.io/GS-CPR/,,,http://arxiv.org/abs/2408.11085,https://zotero.org/alehanderoo/items/XPE3JGI6,https://doi.org/10.48550/arXiv.2408.11085,13/10/2025 23:38 (GMT+2)
Yes,GO-N3RDet,"2D features, 3D features, 3D-fusion, classify","• Reuse PEOM for pose-error tolerant multi-view fusion
• Adapt OOM to 4DGS density consistency
• Integrate DIS-like sampling in GS ray marching
• Replace NeRF with 4DGS dynamic density field
• Add optical flow for dynamic object separation
• Evaluate on multi-camera overlapping flying targets","• Improve multi-view RGB 3D object detection
• Address missing 3D positional modeling in 2D features
• Enhance scene geometry perception for detection
• Fuse views into geometry-aware voxel features","• Strong multi-view fusion; lacks dynamic or tracking
• Requires known poses; not calibration-free
• Indoor static datasets; no flying objects
• Concepts transferable to occupancy in 4DGS
• Ray-distance weighting promising for noisy overlap
• Need association/tracking heads for 3D MOT","• Positional information embedded voxel optimization (PEOM)
• Double importance sampling along camera rays (DIS)
• Opacity Optimization Module enforcing multi-view consistency
• Distance-weighted opacity averaging across viewpoints
• Composite loss: L_cls, L_loc, L_c, L_d, L_opacity
• Projection using camera intrinsics and extrinsics","• Indoor scenes: ScanNet, ARKitScenes datasets
• 1,513 scenes; 2.5M frames; 1.6K rooms; 5K scans
• Multi-view RGB frames; known camera parameters
• 3D bounding box annotations, object classes
• Mostly static scenes; minimal 3D motion
• Public datasets; standard academic licenses","• ResNet backbone extracts per-view 2D features
• Project voxels onto images using known poses
• PEOM predicts offsets; max-pools multi-view features
• NeRF branch with DIS, multi-view OOM
• Distance-weighted opacity adjusts voxel features
• 3D detection head predicts boxes, categories","• SOTA on ScanNet mAP@0.25: 58.6
• ARKITScenes mAP@0.25: 44.7; large gain
• Matches CN-RMA accuracy with 14x fewer epochs
• PEOM, DIS, OOM each improve mAP notably
• Opacity adjustment adds +3.4 mAP@0.25
• Smaller grid [40,40,16] reduces compute",https://github.com/ZechuanLi/GO-N3RDet,,,https://ieeexplore.ieee.org/document/11094479/,https://zotero.org/alehanderoo/items/DUTVYKWP,https://doi.org/10.1109/CVPR52734.2025.02534,13/10/2025 23:36 (GMT+2)
Yes,Multi-view Tracking Using Weakly Supervised Human Motion Prediction,"2D features, 3D-fusion, track","• Reuse multi-view ground-plane fusion concept.
• Adapt flow prediction to 3D voxel/4DGS grid.
• Integrate flow-aware association into 3D MOT.
• Add self-calibration; replace calibration with pose estimation.
• Evaluate with flying-object datasets, overlapping cameras.
• Quick-win: flow constraints regularize detection/tracks.","• Occlusions break single-view people tracking.
• Need multi-view temporal consistency enforcement.
• Predict motion to infer per-frame presence.
• Fuse views while keeping spatial consistency.
• Outperform SOTA on WILDTRACK, PETS2009.","• Strong multi-view fusion; assumes ground-plane, not 3D volume.
• Not calibration-free; requires known extrinsics.
• Flow-before-detection idea suits 3D occupancy flow.
• Extend to 3D grid with occupancy conservation.
• Consider optical flow features for fast movers.
• Verify robustness with asynchronous cameras, varying frame rates.","• Grid world ground-plane discretization.
• People conservation flow constraint (Eq.1).
• Non-overlap per-cell constraint (Eq.2).
• Temporal cycle-consistency constraint (Eq.3).
• Detection reconstructed from flows (Eq.5).
• Losses: L_det, L_cycle, motion reweighting λ_r.","• Domain: multi-view pedestrian scenes.
• WILDTRACK: 400 frames, 7 cameras.
• PETS2009 S2L1: 795 frames, 7 cameras.
• Overlapping FoV, calibrated intrinsics/extrinsics.
• Annotations: foot positions, ground-plane maps.
• 3D motion limited; datasets publicly available.","• ResNet-34 per-view feature extraction.
• Known calibration; project to ground-plane P.
• Temporal aggregation between t and t+1.
• Multiscale spatial aggregation across views.
• Predict 9-direction human flow per cell.
• Association: KSPFlow, muSSPFlow; losses L_det, L_cycle.","• WILDTRACK detection: MODA 91.9, Prec 96.4, Rec 95.7.
• WILDTRACK tracking: MOTA 91.3, IDF1 93.5 (muSSP).
• PETS S2L1 tracking: MOTA 93.3 (muSSPFlow).
• Flow-based costs outperform KSP, muSSP baselines.
• Ablations confirm flow, multiscale, cycle-consistency benefits.
• Runtime/memory not reported.",https://github.com/cvlab-epfl/MVFlow,,,https://ieeexplore.ieee.org/document/10030506/,https://zotero.org/alehanderoo/items/KLMMV46M,https://doi.org/10.1109/WACV56688.2023.00163,13/10/2025 23:33 (GMT+2)
Yes,ST-4DGS,"2D features, 3D features, 3D-fusion, pose","• Reuse spatial-temporal density control for dynamic-object compactness.
• Leverage RAFT-based motion masks for 2D segmentation seeds.
• Adopt 4DGS backbone for 3D dynamic occupancy reconstruction.
• Replace COLMAP with self-calibration for calibration-free pose.
• Integrate semantic features into Gaussians for classification.
• Test on fast flying objects; tune rigidity assumptions.","• Dynamic novel view synthesis with real-time efficiency.
• Maintain spatial-temporal consistency in dynamic scenes.
• Prevent 4D Gaussian compactness degeneration and floaters.
• Improve quality over 4DGS and plane-based methods.
• Enable persistent rendering under modest object motion.","• Strong multi-view dynamic fusion aligns with thesis.
• Lacks classification and 3D tracking modules.
• Assumes known poses; calibration-free needed for thesis.
• Designed for modest motion; flying speed uncertain.
• Optical-flow reliance may struggle with sky backgrounds.
• Good static/dynamic separation via motion-aware splitting.","• 4DGS with spatial-temporal deformation field F.
• Motion-aware shape regularization decouples motion and shape.
• Local rigid regularization: L_loc=L_rig+L_rot.
• Temporal warping loss L_tem using RAFT flows.
• Anisotropic term L_ani constrains Gaussian scale ratios.
• Overall loss: L=λcLc+λTVLTV+Lm.","• Datasets: DyNeRF, ENeRF-Outdoor, Dynamic Scene.
• DyNeRF: 21 cams, 300 frames, kitchen.
• ENeRF: 18 cams, 1200 frames; sample 100 frames.
• Dynamic Scene: 12 cams, 100–200 frames.
• Annotations: none; images only; code available.
• 3D motion present; camera overlap/layout: N/A.","• Initialize 3D Gaussians via COLMAP SfM.
• 4D Gaussian Splatting with spatial-temporal deformation.
• Motion-aware regularization: L_loc, L_tem, L_ani.
• Density control: geometry pruning, motion-aware splitting.
• Requires COLMAP poses; not calibration-free.
• RAFT optical flow; coarse-to-fine; 3K+30K iters.","• Outperforms 4DGS on DyNeRF: PSNR 32.67 vs 31.03.
• ENeRF-Outdoor: PSNR 26.74, better than 4K4D 25.81.
• Dynamic Scene: PSNR 28.66, large gains over baselines.
• Rendering speed: 37fps native, 335fps accelerated.
• Training time ~2.7h; storage smaller than 4DGS, 4K4D.
• Ablations confirm benefits of each component.",https://github.com/wanglids/ST-4DGS,,,https://dl.acm.org/doi/10.1145/3641519.3657520,https://zotero.org/alehanderoo/items/J8LAJZAY,https://doi.org/10.1145/3641519.3657520,13/10/2025 23:30 (GMT+2)
Yes,Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting,"2D features, 3D features, 3D-fusion, pose","• Adopt SPE-based self-calibration for extrinsics estimation.
• Initialize 4DGS from optimized structural points.
• Use motion masks for static/dynamic separation.
• Validate robustness on flying-object, low-texture backgrounds.
• Extend to multi-camera overlapping views for redundancy.
• Relax constant focal assumption or pre-normalize intrinsics.","• Self-calibrate cameras for dynamic 4D Gaussian Splatting.
• Eliminate COLMAP dependence and lengthy preprocessing.
• Handle extreme rotations with minimal translations.
• Avoid unreliable off-the-shelf depth/flow supervision.
• Scale to long monocular videos robustly.","• Strong fit for calibration-free pose estimation.
• Monocular focus mismatches multi-camera requirement.
• No object classification or 3D tracking provided.
• Motion masks requirement may limit aerial scenarios.
• Design informs static/dynamic separation and 4DGS fusion.
• Unclear performance with distant sky backgrounds.","• Structural Points Extraction with edges, gradients, motion masks.
• CoTracker-assisted correspondences for robust 2D-3D mapping.
• Joint optimization of camera and 3D structural points.
• Projection, distance, depth regularization losses for calibration.
• Canonical and deformation fields for dynamic 4DGS.
• RGB loss: L1 plus D-SSIM mixed.","• NeRF-DS: 7 videos, 400–800 frames each.
• DAVIS: 40 short sequences, 50–80 frames.
• Nvidia: 9 scenes, originally 12 cameras.
• Input used: monocular RGB; motion masks provided.
• Dynamic 3D motion present across datasets.
• Public datasets; licensing details N/A.","• SPE selects structural points via edges and gradients.
• Mask dynamic regions using motion masks.
• Track candidate points with CoTracker.
• Jointly optimize poses and 3D points.
• Optimize canonical/deformation 4DGS with Adaptive Density Control.
• No depth/flow; RGB L1+D-SSIM; A100 GPU.","• Camera poses comparable to COLMAP on NeRF-DS.
• Outperforms RoDynRF in ATE and RPE.
• Works on DAVIS extreme geometry where COLMAP fails.
• Comparable PSNR/SSIM to Deformable-3DGS.
• Fewer floating points; better geometry consistency.
• Total optimization time around five hours.",https://github.com/fangli333/SC-4DGS,,,http://arxiv.org/abs/2406.01042,https://zotero.org/alehanderoo/items/ECBRITYM,https://doi.org/10.48550/arXiv.2406.01042,13/10/2025 23:28 (GMT+2)
Yes,Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation,"3D features, 3D-fusion, pose","• Adopt hybrid 3D-4DGS for static/dynamic separation
• Use temporal-scale thresholding in 4DGS pipeline
• Integrate unified rasterizer for mixed Gaussians
• Avoid opacity resets for dynamic stability
• Model background as 3DGS before 4D objects
• Plan calibration-free pose estimation replacement","• 4DGS wastes parameters on static regions
• Accelerate dynamic scene training significantly
• Maintain high-fidelity rendering quality
• Reduce memory via adaptive static modeling","• Good fit for static/dynamic separation needs
• Matches multi-view overlap; not calibration-free
• No classification, tracking, or explainability
• Relies on COLMAP poses; potential mismatch
• Accelerates 4DGS-based 3D feature volume building
• Unclear on fast flying object robustness","• Hybrid 3D-4D representation with temporal thresholding
• Alpha compositing for Gaussian rendering
• 4D quaternion-based rotation parameterization
• Time-conditioned mean/covariance slicing equations
• Iterative classification during densification stages","• N3V: six multi-view dynamic scenes
• 18–21 cameras with overlapping views
• Five 10s clips; one 40s sequence
• Technicolor: 16-camera 4×4 array, 50 frames
• No annotations; photometric supervision only
• 3D motion present; datasets publicly available","• Initialize with COLMAP poses; fully 4D start
• Temporal-scale threshold τ classifies static/dynamic
• Convert static 4D Gaussians to 3D
• Separate densification/pruning for 3D and 4D
• Unified CUDA rasterizer blends 3D and 4D
• No opacity resets; 6k iters N3V, batch 4","• Training 3–5× faster than 4DGS baselines
• N3V: 32.25 PSNR, 208 FPS, 273MB
• 10s clips train in ~12 minutes (RTX4090)
• 40s clip: 52m train, lowest LPIPS
• Fewer Gaussians; significant memory reduction
• Tau trades quality, speed, and storage",,,,http://arxiv.org/abs/2505.13215,https://zotero.org/alehanderoo/items/C9FJA26B,https://doi.org/10.48550/arXiv.2505.13215,13/10/2025 23:26 (GMT+2)
Yes,4D Gaussian Splatting,"2D features, 3D features, 3D-fusion, pose","• Use 4DGS for dynamic 3D feature volume.
• Leverage temporal variance for static/dynamic separation.
• Integrate SAM-based 4D feature segmentation.
• Apply compression for memory-constrained deployments.
• Risk: needs accurate multi-camera poses, overlap.
• Quick test: drones in overlapped multi-view RGB.","• Real-time dynamic scene novel view synthesis.
• Avoid global tracking under complex motions.
• Capture spatiotemporal correlations compactly and explicitly.
• Reduce storage of dynamic Gaussian representations.
• Extend to segmentation and urban scenes.","• Strong fit for multi-view dynamic 3D modeling.
• Lacks calibration-free pose; add self-calibration.
• No classification/tracking heads; extend features.
• Flying objects feasible; test cluttered backgrounds.
• Emergent flow aids 3D tracking formulation.
• Explainability absent; consider Grad-CAM on features.","• Native 4D Gaussian primitives with anisotropic covariances.
• Factorization p(u,v,t)=p(t)p(u,v|t) for rendering.
• 4D rotations via dual quaternion left-right decomposition.
• 4D spherindrical harmonics for temporal appearance.
• Differentiable splatting rasterization with depth sorting.
• Losses: photometric L1, DSSIM; urban regularizers.","• Plenoptic Video: six multi-view real scenes.
• Technicolor: five light-field scenes, 4x4 cameras.
• Waymo-NOTR: 32 driving clips, five cameras.
• D-NeRF synthetic monocular object videos.
• No manual annotations; photometric supervision only.
• 3D motion present; datasets publicly available.","• Initialize Gaussians from COLMAP points or random.
• Requires calibrated poses; not calibration-free.
• No 2D backbone; optional SAM masks for segmentation.
• Multi-view photometric fitting with 4D splatting.
• 4DSH for time-varying view-dependent color.
• Losses: L1, DSSIM; LiDAR, rigid, sparse, covt.","• Best PSNR/LPIPS on Plenoptic Video benchmark.
• 114 fps real-time rendering, surpassing baselines.
• Technicolor top PSNR 34.03; compact 1.6MB/scene.
• 95% storage reduction with minimal quality drop.
• Waymo urban scenes: superior reconstruction and NVS.
• Ablations: 4D rotation, 4DSH significantly help.",https://fudan-zvg.github.io/4d-gaussian-splatting,,,http://arxiv.org/abs/2412.20720,https://zotero.org/alehanderoo/items/BNT46XCZ,https://doi.org/10.48550/arXiv.2412.20720,13/10/2025 23:21 (GMT+2)
Yes,4DGC,"3D features, 3D-fusion","• Reuse motion grid for dynamic 4DGS tracking prior.
• Apply RD-aware training to SH coefficients.
• Use Gaussian compensation for new dynamic regions.
• Integrate streamable 4DGS into multi-camera pipeline.
• Risks: needs dense multi-view, known poses.
• Quick-win: compress per-object 4DGS feature volumes.","• Efficient compression for dynamic 4D Gaussian FVV.
• Reduce storage and transmission for streaming.
• Incorporate rate-distortion in end-to-end training.
• Exploit motion to reduce temporal redundancy.
• Overcome slow rendering and model redundancy.","• Strong fit for 4DGS, multi-view dynamic modeling.
• Mismatch: no classification, tracking, or occupancy outputs.
• No self-calibrated pose; relies on provided extrinsics.
• Could aid static/dynamic separation via motion grid.
• Unclear handling of fast flying objects.
• Verify performance with overlapping RGB-only cameras.","• Motion-aware dynamic Gaussian representation across frames.
• Multi-resolution motion grid with shared MLPs.
• Sparse compensated Gaussians for new regions.
• Differentiable quantization modeling quantization noise.
• Tiny implicit entropy model for bitrate estimation.
• Rate-distortion loss with L1 and SSIM terms.","• Domains: real-world free-viewpoint dynamic scenes.
• Datasets: N3DV, MeetRoom, Google Immersive.
• 300 frames per N3DV scene evaluated.
• Multi-view; one view held-out for testing.
• 3D motion present: yes, dynamic sequences.
• Availability/license: N/A.","• Two-stage sequential modeling and compression pipeline.
• Pose: assumed known; calibration-free not addressed.
• No 2D backbones; no optical flow used.
• 3DGS with motion grid transforms, compensated Gaussians.
• Differentiable quantization and implicit entropy coding.
• Loss: L1+SSIM color, rate terms, RD trade-off.","• Best RD across datasets versus baselines.
• ~16x compression over 3DGStream at similar quality.
• PSNR 31.58 dB, SSIM 0.943 on N3DV.
• Rendering speed 168 FPS; decoding 0.09s.
• Training time 0.83 min; faster than TeTriRF.
• Supports variable bitrates; streamable performance.",,,,http://arxiv.org/abs/2503.18421,https://zotero.org/alehanderoo/items/IGA7XQXV,https://doi.org/10.48550/arXiv.2503.18421,13/10/2025 23:19 (GMT+2)
Yes,1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering,"3D features, 3D-fusion","• Adopt spatial-temporal score pruning for 4DGS objects.
• Use key-frame masks to accelerate 4D reconstruction.
• Expect dependence on calibrated multi-view cameras.
• Not providing classification or tracking heads.
• Test on flying-object scenes with fast motion.
• Evaluate long-interval masks for small fast movers.","• Reduce 4DGS storage and rendering latency.
• Identify and remove temporal redundancy.
• Prune short-lifespan Gaussians without quality loss.
• Filter inactive Gaussians during rasterization.
• Achieve 1000+ FPS dynamic scene rendering.","• Accelerates dynamic 4DGS; helpful for feature volumes.
• No self-calibration; requires known poses.
• No object segmentation, classification, or tracking.
• Potentially beneficial for dynamic-static separation.
• Unclear multi-camera overlap assumptions.
• Assess suitability for airborne, sparse texture targets.","• 4D Gaussian primitives with spatiotemporal covariance.
• Alpha compositing for view-dependent blending.
• Conditional 3D Gaussian from 4D covariance.
• Spatial-temporal variation score for pruning.
• Second derivative of temporal opacity as stability.
• Key-frame mask sharing across adjacent frames.","• N3V dynamic scenes, real-world cooking sequences.
• 300 frames per scene at half-resolution.
• D-NeRF synthetic monocular videos, eight sequences.
• #cameras/layout/overlap: N/A.
• Annotations: N/A.
• 3D motion present across all scenes.","• Train baseline 4DGS on input views.
• Pose: relies on provided calibrated poses; calibration-free N/A.
• Compute spatial-temporal variation score per Gaussian.
• Prune 80–85% Gaussians by score.
• Key-frame temporal filtering with mask sharing.
• 4DGS rasterization for multi-view fusion.","• 41× storage reduction on N3V versus 4DGS.
• 9× faster rasterization; 1000+ FPS achieved.
• PSNR change on N3V: −0.04 dB.
• PSNR +0.38 dB on D-NeRF.
• Active Gaussian overlap IoU >80% within 20 frames.
• Rendering memory 1.62GB; training GPU 10.54GB.",,,,http://arxiv.org/abs/2503.16422,https://zotero.org/alehanderoo/items/W9LAHMAJ,https://doi.org/10.48550/arXiv.2503.16422,13/10/2025 23:12 (GMT+2)
Yes,Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting,"3D features, 3D-fusion, pose","• Use 4DGS for static/dynamic separation via temporal variance.
• Convert Gaussian opacity to occupancy probability volumes.
• Leverage p(t) filtering for long video efficiency.
• Integrate 2D segmentation for per-object 4D volumes.
• Estimate poses self-calibrated; then train 4DGS jointly.
• Extract flows from Gaussian trajectories for tracking cues.","• Real-time photorealistic dynamic scene representation and rendering.
• Overcome 6D plenoptic coupling and interference issues.
• Scale to complex dynamics without explicit deformation fields.
• End-to-end training across variable-length videos.
• Support monocular and multi-view dynamic novel view synthesis.","• Strong fit for multi-view, overlapping dynamic reconstructions.
• Lacks object classification, explicit multi-object tracking.
• Requires accurate camera poses; not calibration-free currently.
• Flying objects feasible after 2D segmentation integration.
• Temporal densification, variance masking are valuable ideas.
• Verify robustness to rapid, sparse flying trajectories.","• Represent spacetime with unbiased 4D Gaussian primitives.
• 4D rotations using paired quaternions model dynamics.
• Factorization p(u,v,t)=p(t)p(u,v|t) guides rendering.
• Conditional 3D Gaussian projected via Jacobian approximation.
• 4D Spherindrical Harmonics for time-evolving appearance.
• Photometric rendering loss; densification and pruning strategy.","• Plenoptic Video: six real, 10-second multi-view scenes.
• D-NeRF: eight synthetic monocular dynamic videos.
• Waymo segments: five cameras; used three frontal views.
• Overlapping views enable multi-camera novel view synthesis.
• Annotations: calibrated poses; RGB; optional LiDAR depth.
• 3D motion present; datasets publicly available.","• Initialize Gaussians via COLMAP point cloud.
• Not calibration-free; relies on known poses.
• 4D Gaussian splatting with tile-based rasterizer.
• 4DSH models view-dependent, time-evolving color.
• Densification with temporal splitting; µ_t gradient indicator.
• Training uses rendering loss; batched time sampling.","• Best PSNR/LPIPS on Plenoptic Video benchmark.
• Reaches 114 FPS; real-time high-resolution rendering.
• Outperforms monocular baselines on D-NeRF metrics.
• 4D rotation, 4DSH ablations show significant gains.
• Emergent optical flow aligns with scene dynamics.",https://github.com/fudan-zvg/4d-gaussian-splatting,Notero%20Advanced/tmpa40mleev.png,,http://arxiv.org/abs/2310.10642,https://zotero.org/alehanderoo/items/DRLU87ZC,https://doi.org/10.48550/arXiv.2310.10642,13/10/2025 23:08 (GMT+2)
Yes,MEGA,"3D features, 3D-fusion","• Adopt DC-AC color for per-object 4DGS
• Use L_opa pruning to compress dynamic volumes
• Integrate deformation field for complex flying motions
• Combine with 4DGS-based static/dynamic decomposition
• Add self-calibration module for camera pose estimation
• Evaluate with optical-flow features for temporal cues","• 4DGS demands massive memory for dynamics
• Reduce 4DGS storage without quality loss
• Eliminate SH redundancy in color attributes
• Increase Gaussian utilization across time viewpoints
• Maintain real-time rendering speeds","• Strong fit for multi-view dynamic 3D fusion
• No classification, tracking, or occupancy outputs
• Requires calibrated cameras; not calibration-free
• Does not target flying objects specifically
• Useful as memory-efficient 4DGS backbone
• Test robustness outdoors, sparse overlap conditions","• DC-AC color decomposition replaces 4D SH
• Temporal-viewpoint aware MLP color predictor F_phi
• Deformation predictor F_theta expands Gaussian action range
• Opacity entropy loss L_opa encourages binary sparsity
• Follows 4DGS slicing equations for G_3D(x,t)","• Technicolor Light Field dynamic indoor scenes
• Five scenes; storage reported for 50 frames
• 4x4 rig, synchronized, dense overlap
• Neu3DV: six scenes, 18–21 cameras
• 300 frames, half-resolution evaluation
• No annotations; photometric supervision only","• Inputs: multi-view synchronized RGB only
• Assumes known camera intrinsics/extrinsics; not self-calibrated
• Replace SH with DC color plus AC predictor
• Per-Gaussian deformation conditioned on time, view
• Losses: L1, SSIM, opacity entropy
• Training: FP16, densification, zip delta storage","• 190× storage reduction on Technicolor dataset
• 125× storage reduction on Neu3DV dataset
• +1.2 dB PSNR over 4DGS Technicolor
• Real-time rendering maintained across datasets
• Participation ratio increased to about 75%
• Fewer Gaussians with L_opa and deformation",https://github.com/Xinjie-Q/MEGA,,,http://arxiv.org/abs/2410.13613,https://zotero.org/alehanderoo/items/SN5A25CG,https://doi.org/10.48550/arXiv.2410.13613,13/10/2025 23:06 (GMT+2)
Yes,Is Space-Time Attention All You Need for Video Understanding?,"2D features, classify, explain","• Use TimeSformer as per-camera 2D temporal backbone.
• Extract token features for multi-view 3D fusion.
• Employ attention rollout for explainability probes.
• Pretrain on large image/video before fine-tuning.
• Risk: single-view bias, lacks 3D geometry.
• Quick win: distill features into 3DGS pipeline.","• Convolution-free video classification with self-attention.
• Evaluate space-time attention schemes for videos.
• Improve efficiency on long clips and resolutions.
• Achieve state-of-the-art action recognition accuracy.","• Mismatch: no multi-camera, no 3D tracking.
• Useful: strong 2D temporal features per camera.
• Divided attention suggests separate time/space modeling.
• Attention maps aid explainability alignment.
• Need adaptation for small flying object detection.
• Consider multi-view token fusion extensions.","• Vision Transformer extended to space-time tokens.
• Divided attention: temporal then spatial attention.
• Learnable spatiotemporal positional embeddings.
• QKV self-attention with softmax scaling.
• Cross-entropy classification over video classes.","• Kinetics-400/600, SSv2, Diving-48, HowTo100M.
• Single-view RGB clips; no multi-camera overlap.
• Action labels only; no 3D annotations.
• Contains human motion; 3D motion not annotated.
• Public datasets; standard academic licenses.
• K400: 240K train, 20K val videos.","• Patch embeddings from frames; Transformer encoder blocks.
• Divided temporal-then-spatial self-attention per block.
• Classification token with MLP head.
• Pretraining on ImageNet-1K/21K.
• No flow, pose, or 3D fusion.
• Attention rollout for attention visualization.","• Divided attention outperforms joint and axial schemes.
• K400 top-1 up to 80.7%.
• SSv2 top-1 up to 62.4%.
• Lower inference cost: 0.59 TFLOPs baseline.
• Faster inference than SlowFast on K400.
• HowTo100M top-1 up to 62.6%.",https://github.com/facebookresearch/TimeSformer,,,http://arxiv.org/abs/2102.05095,https://zotero.org/alehanderoo/items/4ZSZE3PX,https://doi.org/10.48550/arXiv.2102.05095,13/10/2025 23:04 (GMT+2)
Yes,A Framework Combining 3D CNN And Transformer For Video-Based Behavior Recognition,"2D features, classify","• Reuse 3D CNN+Transformer as per-camera temporal encoder
• Augment with optical flow for motion cues
• Integrate encoder features into 3DGS object volumes
• Add explainability (Grad-CAM) for feature attribution
• Beware single-camera assumptions and classification-only design
• Benchmark runtime on multi-camera, real-time constraints","• Improve video behavior recognition accuracy and efficiency
• Combine 3D CNN locality with Transformer global context
• Address 3D CNN limited long-range modeling
• Reduce Transformer computational overhead in videos
• Provide lightweight, unified fusion architecture","• Good for 2D temporal features; lacks 3D geometry
• No multi-camera overlap, pose, or tracking
• Fusion design adaptable to per-camera encoders
• Needs dynamic/static separation and 3D fusion
• Unclear generalization to flying object dynamics
• Runtime on edge multi-cam remains unreported","• 3D CNN captures local spatiotemporal patterns
• Transformer self-attention models long temporal dependencies
• Positional encodings for sequence ordering
• Weighted fusion alpha blends CNN and Transformer features
• Scaled dot-product attention equation provided
• 3D convolution operation formally defined","• Domain: violence detection in surveillance videos
• RWF-2000: 2000 labeled clips
• Hockey Fight dataset size: N/A
• Single-camera videos, no multi-view overlap
• Annotations: clip-level violent/non-violent labels
• 3D motion: N/A; License: N/A","• Eight-layer 3D CNN with 3x3x3 kernels
• Six 3D pooling layers for downsampling
• Two Transformer blocks on flattened tokens
• Learned weighted fusion with residual connections
• MLP head for classification outputs
• Training: AdamW, CE loss, 200 epochs, 224x224","• 96.7% accuracy on Hockey Fight
• 93.56% accuracy on RWF-2000
• ROC-AUC: 0.9652 (Hockey), 0.9481 (RWF-2000)
• Outperforms LSTM and standalone 3D CNN
• Smoother convergence, earlier stabilization
• Ablations validate complementary module strengths",,,,,https://zotero.org/alehanderoo/items/BU79SFAK,,13/10/2025 22:59 (GMT+2)
Yes,Video Transformer Network,"2D features, classify, explain","• Use as per-camera 2D temporal feature extractor.
• Aggregate frame embeddings for 4DGS object volumes.
• Use attention weights for temporal saliency gating.
• Replace with ViT-B backbone for stronger features.
• Beware: no geometry, calibration, or 3D outputs.
• Quick-win: pretrain on aerial/flying-object videos.","• Replace 3D ConvNets with transformer video recognition.
• Handle long videos in single pass efficiently.
• Maintain accuracy with faster train/inference runtime.
• Avoid multi-view, clip-average inference inefficiency.
• Generic framework using any 2D backbone.","• Strong 2D temporal module; no 3D occupancy support.
• Single-camera design; no overlapping multi-camera fusion.
• Helpful for dynamic/static separation via attention saliency.
• Could seed classifiers for flying-object categories.
• Needs integration with self-calibration and 4DGS.
• Evaluate explainability via attention rollout, not Grad-CAM.","• Longformer sliding-window attention, global CLS token.
• Linear-complexity attention enables thousands of tokens.
• 2D spatial encoder produces per-frame embeddings.
• MLP head on CLS for classification.
• Cross-entropy loss; attention dropout, positional embeddings.","• Kinetics-400: 234k train, 19,760 val videos.
• Moments-in-Time v2: 727k train, 30.5k val.
• Single-camera internet videos; no multi-view overlap.
• Annotations: video-level action labels only.
• 3D motion present, no 3D ground-truth.
• Code available; dataset licenses follow originals.","• 2D backbone: ResNet, DeiT, ViT variants.
• Temporal encoder: Longformer, window size 32.
• No optical flow; RGB frames only.
• MLP classification head on CLS token.
• Training: SGD, augmentation, clip sampling 16/32 frames.
• Inference: full-video or multi-view protocols.","• ViT-B-VTN top-1 78.6–79.8 on Kinetics-400.
• MiT v2 top-1 37.4, state-of-the-art RGB.
• 16.1x faster training end-to-end vs baselines.
• 5.1x faster validation runtime; 1.5x fewer GFLOPs.
• Full-video inference maintains accuracy; baselines drop ~8%.
• Fine-tuning backbone gains ~7% top-1 accuracy.",https://github.com/bomri/SlowFast/tree/master/projects/vtn,,,https://ieeexplore.ieee.org/document/9607406/,https://zotero.org/alehanderoo/items/SKHJMTCM,https://doi.org/10.1109/ICCVW54120.2021.00355,13/10/2025 22:57 (GMT+2)
Yes,Wonder3D,"2D features, 3D features, 3D-fusion, pose","• Use normals as robust per-view 2D features.
• Adopt geometry-aware weighting in multi-view fusion.
• Incorporate outlier-dropping in 4DGS optimization.
• Share multi-view attention across cameras.
• Risk: static, single-object; no dynamics.
• Quick test: normals aiding occupancy voxelization.","• Efficient single-image to 3D reconstruction.
• Improve multi-view geometric consistency.
• Preserve high-fidelity geometric details.
• Reduce SDS per-shape optimization time.
• Robust mesh extraction from sparse views.","• Strong single-object geometry; weak for dynamics.
• No self-calibration; assumes known poses.
• SDF fusion ideas transferable to 4DGS.
• Normals strengthen texture-geometry disentanglement.
• Not for tracking, classification, explainability.
• Extend to video, multi-camera, flying objects.","• Cross-domain diffusion modeling normals and colors jointly.
• Domain switcher conditions UNet per domain.
• Multi-view attention for cross-view dependencies.
• Cross-domain attention aligns geometry and appearance.
• SDF optimization with geometry-aware normal loss.
• Regularizers: eikonal, sparsity, smoothness; mask BCE, RGB MSE.","• Objaverse LVIS subset; synthetic object renders.
• ≈30k objects; six views per object.
• Six fixed cameras; overlapping around object.
• Rendered normals, colors, masks; meshes available.
• No 3D motion; static scenes.
• Public datasets; license N/A.","• Input: image, CLIP embedding, camera poses.
• Generate multi-view normals and colors via diffusion.
• Multi-view attention enforces cross-view consistency.
• Cross-domain attention couples normals and colors.
• Fuse using instant-NGP SDF with geometry-aware losses.
• Predefined poses; no self-calibration; no flow.","• Best CD 0.0199; IoU 0.6244 on GSO.
• PSNR 26.07; SSIM 0.924; LPIPS 0.065.
• 2–3 minutes per textured mesh.
• Cross-domain attention improved geometric consistency.
• Multi-view attention improved rear-view realism.
• Normal fusion removed holes, reduced noise.",https://www.xxlong.site/Wonder3D/,,,http://arxiv.org/abs/2310.15008,https://zotero.org/alehanderoo/items/BPHEFCTH,https://doi.org/10.48550/arXiv.2310.15008,13/10/2025 22:54 (GMT+2)
Yes,GeoWizard,"2D features, 3D features","• Use depth/normal as priors for 3DGS initialization.
• Adopt scene decoupler for domain-aware geometry cues.
• Leverage cross-domain attention for multi-task consistency.
• Integrate as per-camera background geometry estimator.
• Add Grad-CAM for explainability if needed.
• Beware scale ambiguity across multi-camera views.","• Monocular depth and normal estimation from single images.
• Overcome dataset diversity and label quality limitations.
• Exploit diffusion priors for geometry prediction.
• Joint estimation to improve consistency and detail.
• Resolve scene layout ambiguity across domains.","• Strong monocular priors; no multi-view or tracking.
• Useful for static/dynamic separation via background depth.
• No pose self-calibration or multi-camera fusion.
• No optical flow or temporal consistency modeling.
• Flying objects performance unclear; requires dedicated tests.
• Consider cross-view scale alignment constraints.","• Latent diffusion conditioned on image and CLIP embeddings.
• Joint modeling of p(d,n) with unified U-Net.
• Geometry switcher controlling depth and normal domains.
• Cross-domain geometric self-attention ensuring consistency.
• Scene distribution decoupler with one-hot indicators.
• v-prediction loss and multi-resolution noise scheduling.","• Domains: indoor, outdoor, background-free objects.
• Training samples: approximately 0.28M images.
• #cameras: single-view; no multi-camera overlap.
• Annotations: depth, normals; some normals from depth.
• 3D motion: none; static imagery only.
• Availability/license: N/A.","• Fine-tune SD v2 latent U-Net with CLIP conditioning.
• Pose/self-calibration: N/A.
• Geometry switcher toggles depth or normal.
• Cross-domain geometric self-attention aligns modalities.
• Scene distribution decoupler: indoor/outdoor/object indicators.
• v-prediction, multi-resolution noise; Adam, 20k steps, 8×A100.","• SOTA zero-shot depth across multiple benchmarks.
• SOTA zero-shot normals with finer details.
• Improved depth-normal geometric consistency metrics.
• Better MonoSDF reconstruction precision and recall.
• Robust generalization to unreal, in-the-wild images.
• Training: 2 days, 8×A100; inference speed N/A.",,,,http://arxiv.org/abs/2403.12013,https://zotero.org/alehanderoo/items/R2ARVCTA,https://doi.org/10.48550/arXiv.2403.12013,13/10/2025 22:51 (GMT+2)
Yes,SPMTrack,"2D features, classify, explain, track","• Reuse TMoE in 2D feature backbones
• Adopt target state token for temporal memory
• Compare LoRA vs TMoE for efficiency
• Integrate attention maps for explainability
• Risk: single-object, monocular; no 3D support
• Quick win: evaluate on UAV flying-object videos","• Adaptive relation modeling for tracking with MoE
• Overcome background-foreground attention interference
• Integrate spatio-temporal context beyond image pairs
• Parameter-efficient fine-tuning with minimal trainable parameters
• Achieve SOTA accuracy with smaller backbones","• Strong temporal modeling; no multi-view support
• Lacks 3D occupancy or fusion modules
• MoE routing may transfer to multi-camera tokens
• Extend heads to 3D state estimation
• Consider optical flow features for dynamics
• Open: throughput, stability under long-term occlusions","• Task-specific MoE (TMoE) across attention and FFN
• Dense expert routing with shared frozen expert
• Compression expert reduces dimension before routed experts
• Spatio-temporal target state token for memory
• Binary cross-entropy and Generalized IoU losses
• Type embeddings for foreground/background/search tokens","• Domain: single-object 2D tracking videos
• LaSOT 280 test; GOT-10K 180 test
• TrackingNet 511 test; UAV123 123 sequences
• Monocular single camera; no multi-view overlap
• Annotations: 2D bounding boxes only
• 3D motion presence: N/A; code available on GitHub","• One-stream ViT backbone with TMoEBlocks
• No pose; single-camera, calibration-dependent
• Backbones: ViT-B/L/G pretrained on DINOv2
• Spatio-temporal memory via target state token
• Decoupled doubleMLP heads: center cls, box reg
• Optimization: AdamW, cosine LR, 170 epochs","• SOTA LaSOT AUC 74.9 (ViT-B)
• SOTA TrackingNet AUC 86.1 (ViT-B)
• GOT-10K AO 76.5; competitive to larger models
• Trainable params reduced ~80% vs ARTrackV2
• TMoE + spatio-temporal add +2.0 AUC total
• Attention maps show better background suppression",https://github.com/WenRuiCai/SPMTrack,,,http://arxiv.org/abs/2503.18338,https://zotero.org/alehanderoo/items/KWUKIU78,https://doi.org/10.48550/arXiv.2503.18338,13/10/2025 22:49 (GMT+2)
Yes,Track Anything,"2D features, track","• Use SAM+XMem for per-camera mask propagation.
• Generate pseudo-masks for dynamic object segmentation.
• Feed masks to 4DGS object reconstruction.
• Add optical flow for temporal consistency checks.
• Not suitable for multi-view calibration or 3D tracking.
• Evaluate on flying-object videos per camera.","• Enable interactive VOS without heavy annotation.
• Integrate SAM with temporal tracking.
• Address SAM's poor temporal consistency in videos.
• Reduce human effort via minimal clicks.","• Useful for 2D masks; lacks 3D capabilities.
• No multi-camera overlap handling.
• Could bootstrap dynamic/static separation per view.
• Consider automating click prompts across frames.
• Long-term memory weaknesses may hurt long scenes.
• Unclear performance on small, fast flying objects.","• Leverages SAM prompts as segmentation prior.
• Uses XMem's memory-based temporal correspondence.
• One-pass interactive tracking with human correction.
• Assumes single-view, 2D segmentation objective.
• No explicit learning; zero-training inference.","• DAVIS-2016 val; DAVIS-2017 test-dev datasets.
• Natural, diverse short and long videos.
• Single-camera videos; no multi-view overlap.
• Pixel-wise mask annotations for VOS.
• 3D motion not modeled; 2D sequences only.
• Code and demos publicly available.","• Step1: SAM click-based initialization masks.
• Step2: XMem propagation using memory stores.
• Step3: SAM refinement using probes, affinities prompts.
• Step4: Human corrective clicks during failures.
• No calibration, flow, or 3D fusion.
• One-pass inference; no additional training.","• 88.4 J&F on DAVIS-2016 val.
• 73.1 J&F on DAVIS-2017 test-dev.
• Competitive with MiVOS, below XMem on metrics.
• Robust to deformation, scale, camera motion.
• Struggles on long-term memory, complex structures.
• Runtime and memory not reported.",https://github.com/gaomingqi/Track-Anything,,,http://arxiv.org/abs/2304.11968,https://zotero.org/alehanderoo/items/YF4DIQZQ,https://doi.org/10.48550/arXiv.2304.11968,13/10/2025 22:47 (GMT+2)
Yes,Segment and Track Anything,"2D features, track","• Use SAM for per-camera instance masks.
• Use Grounding-DINO prompts for flying objects.
• Replace DeAOT with multi-view 3D tracker.
• Feed masks to 4DGS object reconstruction.
• Evaluate CMR-like logic for new entrants.
• Beware single-view bias, no geometry.","• Unified video segmentation and tracking framework.
• Bridge SAM to temporally coherent video.
• Support multimodal interaction: click, box, text.
• Handle new objects appearing mid-video.","• Strong 2D segmentation; lacks 3D reasoning.
• No multi-camera overlap or calibration.
• Useful for initialization and 2D features.
• Flying small objects may challenge SAM.
• Consider optical flow to refine temporal coherence.
• Verify performance on aerial drone viewpoints.","• Combine SAM, DeAOT, Grounding-DINO modules.
• DeAOT uses identification embedding association.
• Hierarchical Gated Propagation Module for memory.
• CMR rule for new object detection.
• No explicit losses described.","• Domains: VOS, assorted application videos.
• DAVIS-2016 Val, DAVIS-2017 Test.
• Single-camera monocular videos; no multi-view overlap.
• Pixel-level mask annotations.
• 3D motion not modeled; 2D tracking only.
• Code available; datasets public benchmarks.","• Interactive: SAM prompts generate keyframe masks.
• Text: Grounding-DINO boxes feed SAM.
• Tracker: DeAOT propagates multi-object masks.
• Automatic: segment-everything every n frames.
• CMR compares masks to add new IDs.
• Training details/losses: N/A.","• DAVIS-2016 Val Avg 92.0 with clicks.
• DAVIS-2017 Test Avg 79.2 performance.
• Competitive with state-of-the-art DeAOT baselines.
• Qualitative robustness across diverse domains.
• Runtime claims fast; numbers not provided.",https://github.com/z-x-yang/Segment-and-Track-Anything,,,http://arxiv.org/abs/2305.06558,https://zotero.org/alehanderoo/items/VQF99KVH,https://doi.org/10.48550/arXiv.2305.06558,13/10/2025 22:45 (GMT+2)
Yes,Exploring Temporally-Aware Features for Point Tracking,"2D features, track","• Use Chrono as per-camera temporally-aware 2D encoder.
• Replace flow with Chrono correlation cues.
• Seed 2D tracks for 4DGS dynamic segmentation.
• Triangulate multi-view tracks for 3D flying objects.
• Beware single-view training; lacks geometric constraints.
• Evaluate cross-view feature consistency and robustness.","• Improve point tracking with temporally-aware features.
• Address reliance on heavy iterative refiners.
• Leverage pretrained backbones for robust real-world tracking.
• Enable accurate refiner-free point tracking.","• Excellent 2D temporal encoding; lacks 3D fusion.
• No self-calibration or multi-camera modeling.
• Useful for static/dynamic separation via stable tracks.
• Missing classification and explainability modules.
• Consider cross-view adapters for multi-camera overlap.
• Test on fast flying objects, motion blur.","• Adapt DINOv2 with temporal adapters between transformer blocks.
• Local temporal attention over window N=13 frames.
• Cosine similarity correlation with soft-argmax localization.
• Huber loss supervises 2D point positions.
• Residual design preserves pretrained spatial semantics.","• Generic videos: synthetic and real domains.
• TAP-Vid-RGB-Stacking: 50 synthetic videos.
• TAP-Vid-Kinetics: 1,189 YouTube videos.
• TAP-Vid-DAVIS: 30 challenging videos.
• Single-camera; no multi-view overlap.
• Annotations: 2D tracks, visibility; license N/A.","• DINOv2 ViT backbone plus temporal adapters each block.
• 2D Conv downsample, 1D temporal attention, Conv up.
• Temporal window N=13; stride s=4.
• Matching by cosine correlation; soft-argmax prediction.
• Train on Kubric Panning MOVi-E with Huber loss.
• AdamW, LR 1e-4, 100k iters, 4 A100 GPUs.","• SOTA position accuracy among backbones on TAP-Vid.
• +20.6%p vs TSM-ResNet-18 on DAVIS.
• Over 90% <δ4 accuracy on DAVIS strided.
• 12.5× throughput vs TAPIR with minor accuracy drop.
• 1D attention beats 1D/3D conv temporal aggregation.
• Features improve LocoTrack metrics across datasets.",https://cvlab-kaist.github.io/Chrono/,,,,https://zotero.org/alehanderoo/items/4NRZGGLA,,13/10/2025 22:44 (GMT+2)
Yes,End-to-End Flow Correlation Tracking with Spatial-Temporal Attention,"2D features, track","• Reuse flow-guided temporal aggregation per camera.
• Adopt spatial-temporal attention gating before 3D fusion.
• Use PNR-based update gating for stability.
• Risk: flow unreliable for fast flying objects.
• Quick-win: replace FlowNet with RAFT.
• Quick-win: compare no-temporal-attention versus full gating.","• Improve tracking using temporal motion cues from flow.
• DCF trackers ignore inter-frame information.
• End-to-end joint flow and tracking learning.
• Handle occlusion, deformation, illumination changes.","• No multi-view, no 3D occupancy; limited relevance.
• Strong 2D temporal feature design transferable.
• Attention weighting inspires multi-camera fusion gating.
• Flow warping may fail under large parallax.
• Extend to multi-object association remains open.
• Evaluate on aerial, high-speed, low-texture targets.","• DCF formulated as differentiable correlation filter layer.
• L2 loss to Gaussian response, regularization lambda.
• Fourier-domain solution for filter optimization.
• Optical flow warping via bilinear sampling.
• Spatial-temporal attention weights with cosine similarity.
• Channel reweighting akin to squeeze-excitation.","• General single-object video tracking domain.
• OTB2013: 50; OTB2015: 100 sequences.
• VOT2015/2016: 60 sequences each.
• Monocular videos; single camera; no overlap.
• 2D bounding box annotations.
• 3D motion/flying objects: N/A.","• FeatureNet, FlowNet, warping, spatial-temporal attention, CF layer.
• Pose strategy: none; calibration-free irrelevant.
• 2D conv backbone; FlowNet for optical flow.
• Temporal aggregation of warped features; no 3D fusion.
• Response map head; PNR-gated model updates; scale pyramid.
• SGD training; T=6 frames; lr1e-5; 12 FPS.","• OTB2013 AUC 0.689; precision 0.921; state-of-art.
• OTB2015 AUC 0.655; precision 0.881; top-ranked.
• VOT2015 EAO 0.3405; ranks first.
• VOT2016 EAO 0.334; ranks first.
• Ablations: >6% gains using flow aggregation.
• Runtime 12 FPS on GTX TITAN X.",https://github.com/zhengzhugithub/FlowTrack,,,https://ieeexplore.ieee.org/document/8578162/,https://zotero.org/alehanderoo/items/HPDSJY4K,https://doi.org/10.1109/CVPR.2018.00064,13/10/2025 22:38 (GMT+2)
Yes,Probing the 3D Awareness of Visual Foundation Models,"2D features, 3D features","• Use DINOv2 for 2D geometric features
• Avoid CLIP for geometry-sensitive modules
• Add view-consistency regularization or multiview training
• Validate wide-baseline matching on overlapped cameras
• Combine depth/normal probes with 4DGS volumes
• Explore diffusion features for semantic bootstrapping","• Evaluate VFMs' intrinsic 3D awareness
• Gap: lack of 3D benchmarks
• Probe single-view surface understanding
• Probe multiview cross-view consistency
• Use frozen features, minimal probes","• Limited for flying-object dynamics; static datasets
• Useful to choose 2D backbones
• Wide-baseline weakness risks multi-camera setups
• No pose self-calibration; relies on GT poses
• No classification/tracking modules evaluated
• Consider temporal cues, optical flow integration","• 3D awareness: surface encoding, cross-view consistency
• Map to depth, normals, correspondence tasks
• Frozen-feature probing assumption
• AdaBins depth formulation, scale-invariant loss
• Uncertainty-aware normals loss (Bae et al.)
• Nearest-neighbor correspondence, Lowe's ratio filtering","• NYUv2 indoor scenes; aligned depth, normals
• NAVI object-centric images; meshes, depth, normals
• ScanNet pairs; indoor multiview with known poses
• SPair71k keypoints; semantic correspondence classes
• Multiview overlap ensured by viewpoint bins
• Public datasets; code released; permissive licenses","• Extract dense features from diverse backbones
• Train multiscale DPT-like decoder probes
• Calibration: use dataset intrinsics and poses
• Zero-shot correspondence via cosine nearest neighbors
• Filter matches with Lowe's ratio test
• AdamW, 10 epochs, cosine LR, combined losses","• DINOv2 excels at depth and normals
• StableDiffusion competitive on single-view geometry
• CLIP/SigLIP poor at geometric cues
• Large viewpoint consistency degrades sharply
• Semantic matches outperform geometric wide-baseline
• Runtime/memory not reported",https://github.com/mbanani/probe3d,,,http://arxiv.org/abs/2404.08636,https://zotero.org/alehanderoo/items/UAB3D5SF,https://doi.org/10.48550/arXiv.2404.08636,13/10/2025 22:32 (GMT+2)
Yes,You Only Look Once,"2D features, classify","• Use YOLO for per-camera 2D detections
• Fine-tune on UAV/bird flying objects
• Seed 2D features from YOLO detections
• Evaluate multi-view consistency for fusion
• Risk: small objects, distant targets
• Add separate 3D tracking and calibration modules","• Unify detection into single end-to-end network
• Replace slow proposal-based pipelines
• Real-time detection with high accuracy
• Reduce background false positives via global context","• 2D-only; no multi-view or 3D tracking
• Strong real-time per-camera detection baseline
• Small flying objects likely challenging
• End-to-end simplicity informs module unification
• Needs instance segmentation for 4DGS integration
• Consider explainability add-ons if needed","• Detection as regression on S×S grid
• Predict B boxes and C class probabilities
• Confidence equals Pr(Object) × IOU
• Sum-squared error with λcoord, λnoobj
• Leaky ReLU activations; sqrt(width,height) trick
• Non-max suppression for duplicate removal","• PASCAL VOC 2007/2012 natural images
• #images unspecified in paper
• Single-view images; no multi-camera overlap
• 2D bounding boxes with class labels
• No 3D motion; still images
• Public benchmark datasets; standard licenses","• Resize input to 448×448
• Single CNN predicts boxes and classes
• No pose or calibration; single-RGB input
• Backbone inspired by GoogLeNet; pretrained ImageNet
• Assign responsibility via highest IOU
• Loss weights λcoord=5, λnoobj=0.5; augmentation","• 45 FPS, 63.4% mAP on VOC2007+2012
• Fast YOLO: 155 FPS, 52.7% mAP
• Fewer background errors than Fast R-CNN
• Localization errors dominate YOLO mistakes
• Combining with Fast R-CNN reaches 75.0% mAP
• Generalizes better to artwork domains",https://github.com/pjreddie/darknet,,,http://arxiv.org/abs/1506.02640,https://zotero.org/alehanderoo/items/W7I9BWTC,https://doi.org/10.48550/arXiv.1506.02640,13/10/2025 21:09 (GMT+2)
Yes,Video Swin Transformer,"2D features, 3D features, classify","• Use Video Swin for per-camera spatiotemporal features.
• Export features to 4DGS reconstruction module.
• Fine-tune on flying-object datasets.
• Add flow branch if motion cues needed.
• Enforce multi-view consistency during training.
• Apply Grad-CAM on attention maps.","• Improve video recognition efficiency and accuracy.
• Introduce local spatiotemporal attention windows.
• Utilize image-pretrained Swin for videos.
• Surpass global-attention Transformers' trade-off.
• Achieve SOTA on K400, K600, SSv2.","• Strong 2D/temporal features, no 3D fusion.
• No pose, calibration, or multi-view handling.
• Useful backbone before 3DGS feature fusion.
• LR scheduling trick transferable to fine-tuning.
• Windowed attention suits long videos efficiently.
• Needs adaptation for aerial, small flying objects.","• Spatiotemporal locality inductive bias.
• 3D shifted-window self-attention mechanism.
• Hierarchical Transformer with patch merging.
• 3D relative position bias in attention.
• Joint spatiotemporal attention within windows.
• Cross-entropy classification loss, AdamW optimizer.","• Kinetics-400: ~240k train, 20k val.
• Kinetics-600: ~370k train, 28.3k val.
• SSv2: 168.9k train, 24.7k val.
• Single monocular videos; no multi-view overlap.
• Action labels only; clip-level supervision.
• Public datasets; academic research licenses.","• 3D patch tokens from video frames.
• 3D shifted-window multi-head self-attention.
• Four-stage hierarchical Swin backbone.
• ImageNet-21K/1K pretraining; lower backbone LR.
• Relative position bias; stochastic depth; RandAugment.
• Inference with multi-view sampling, score averaging.","• K400 top-1 84.9%, SOTA.
• K600 top-1 86.1%, SOTA.
• SSv2 top-1 69.6%, surpasses MViT.
• 3D shifted windows: +0.7% top-1.
• Temporal window 8 vs 16: -0.3, -17% FLOPs.
• Smaller pretraining data than ViViT-H.",https://github.com/SwinTransformer/Video-Swin-Transformer,,,http://arxiv.org/abs/2106.13230,https://zotero.org/alehanderoo/items/3SUGK9SJ,https://doi.org/10.48550/arXiv.2106.13230,13/10/2025 20:58 (GMT+2)
Yes,ImVoxelGNet,"2D features, 3D features, 3D-fusion, classify, pose","• Reuse learnable projection-neighborhood expansion module
• Adopt GIP occupancy gating before 4DGS fusion
• Replace averaging with uncertainty-weighted voxel fusion
• Add self-calibration for calibration-free multi-camera setups
• Test on flying-object datasets with overlapping cameras
• Integrate temporal cues or optical flow features","• Improve RGB-only multi-view 3D detection accuracy
• Address pixel underuse in back-projection mapping
• Enhance geometric perception via implicit occupancy modeling
• Reduce dependency on depth or point clouds","• Strong multi-view voxel fusion; no temporal modeling
• Requires known calibration; not self-calibrated
• Static indoor scenes; no flying objects
• No tracking; detection-only classification head
• Useful occupancy prior for 3D occupancy prediction
• Consider replacing voxels with 4DGS features","• Pinhole projection linking voxels and image pixels
• Frustum masks and multi-view voxel averaging
• Learnable neighborhood feature expansion near projections
• Implicit voxel occupancy gating via 3D convolutions
• Voxel-space 3D detection with convolutional heads","• Indoor ScanNetV2 multi-view RGB scenes
• 1,201 train, 312 val scans
• 50 views per scene during testing
• Intrinsics/extrinsics provided; overlapping cameras
• Axis-aligned 3D bounding boxes
• Static scenes; no 3D motion","• Pretrained 2D backbone with FPN feature maps
• Back-project features to 3D voxels using calibration
• Multi-view voxel fusion via masked averaging
• Learnable eight-neighbor feature expansion near projections
• GIP predicts voxel occupancy, gates voxel features
• 3D conv detection head; AdamW, 12 epochs","• +2.1–2.2 mAP@0.25 over ImVoxelNet
• Combined 8-L+GIP best: 48.1 mAP@0.25
• Improved noise robustness versus baseline
• Minimal params overhead: +51K parameters
• Inference ~0.19s/image on GTX 1070
• Ablations favor learnable kernels over fixed",https://github.com/xug-coder/ImVoxelGNet,,,https://dx.plos.org/10.1371/journal.pone.0320589,https://zotero.org/alehanderoo/items/TWAS8TJT,https://doi.org/10.1371/journal.pone.0320589,13/10/2025 20:33 (GMT+2)
Yes,Multi-camera real-time three-dimensional tracking of multiple flying animals,"2D features, 3D features, 3D-fusion, pose, track","• Reuse EKF+NNSF for 3D MOT module.
• Adopt self-calibration from moving LED/fly trajectories.
• Use background subtraction for dynamic object proposals.
• Integrate entry/exit logic with 3DGS object pipelines.
• Add optical flow and semantic segmentation for features.
• Validate latency, occlusion robustness in multi-camera rigs.","• Real-time multi-camera 3D tracking of flying animals.
• Markerless, low-latency tracking with multiple targets.
• Robust tracking over large volumes with occlusions.
• Gap: lacked inexpensive, scalable, real-time 3D trackers.","• Strong fit: multi-view, flying, real-time 3D tracking.
• Missing: 3D occupancy volume, classification, explainability.
• Background subtraction may fail in dynamic backgrounds.
• Identity persistence limited; re-identification needed.
• Useful for pose/triangulation and association design.
• Check code availability, license, synchronization complexity.","• Bayesian MAP state estimation with Markov assumption.
• EKF with constant-velocity motion model.
• Projective camera observation model with Gaussian noise.
• NNSF data association per-target independence.
• Mahalanobis gating along camera rays.
• Entry/exit modeling for target birth and death.","• Domains: flies, hummingbirds in arenas.
• Systems: 5, 11, 4 camera setups.
• Overlapping multi-view around central volume.
• 60–200 fps; 18 h tracking session.
• Markerless, no manual labels; 3D motion present.
• Software availability: open-source components; data N/A.","• Background subtraction 2D features per camera.
• Features: u,v, area, brightness, orientation, eccentricity.
• Two-step calibration: DLT plus multi-camera self-calibration.
• Centralized triangulation plus EKF tracking.
• NNSF association; gating by distance, area, Mahalanobis.
• Initialization via multi-view hypotheses; track termination thresholds.","• Median 3D latency approximately 39 ms.
• Reprojection error typically under one pixel.
• 3D accuracy within 4% of physical measures.
• Robust to occlusions; alternating single-view updates.
• Tracks multiple animals simultaneously in real time.
• Reduced storage via saving 2D features only.",,,,https://royalsocietypublishing.org/doi/10.1098/rsif.2010.0230,https://zotero.org/alehanderoo/items/2JQ5A75G,https://doi.org/10.1098/rsif.2010.0230,11/10/2025 2:35 (GMT+2)
Yes,Fully Sparse 3D Occupancy Prediction,"2D features, 3D features, 3D-fusion, classify","• Reuse sparse voxel decoder for 3D occupancy volume
• Adopt mask-guided sparse sampling for 2D-3D interaction
• Use RayIoU-style metric for depth-consistent evaluation
• Augment temporal fusion with optical flow features
• Add static/dynamic separation for moving-object focus
• Assess robustness to pose noise; explore self-calibration","• Speed up camera-only 3D occupancy prediction
• Exploit scene sparsity to reduce compute
• Eliminate dense 3D features and global attention
• Propose RayIoU for fair depth-consistent evaluation
• Achieve real-time performance without accuracy loss","• Strong camera-only occupancy; no tracking provided
• Relies on dataset calibration; no self-calibration
• Multi-camera overlap aligns with thesis setup
• Not focused on aerial/flying objects
• Sparse design could seed 4DGS feature volumes
• Ray-based metrics helpful for moving-object depth","• Coarse-to-fine sparse voxel decoder
• Mask transformer with sparse queries
• Mask-guided sparse sampling cross-attention
• RayIoU ray-casting mIoU formulation
• Losses: BCE, focal, dice, BCE mask
• Hungarian matching for assignment","• Autonomous driving, Occ3D-nuScenes dataset
• 1000 videos; 700/150/150 split
• Six surround cameras with overlap
• Voxel-level semantic occupancy annotations
• Dynamic 3D motion present; multi-frame inputs
• Public benchmark; code and metric released","• ResNet-50 + FPN multi-view 2D features
• Sparse voxel decoder reconstructs class-agnostic occupancy
• Temporal warping of sampled 3D points
• Mask transformer predicts sparse masks and labels
• Top-k/threshold pruning; class-weighted BCE supervision
• Assumes known camera poses; not calibration-free","• 34.0 RayIoU at 17.3 FPS with 8 frames
• 35.1 RayIoU using 16 frames
• Outperforms FB-Occ under weaker settings
• Sparse decoder ~4x faster than dense baselines
• Mask-guided sampling boosts accuracy and speed
• Optimal sparsity ~5% voxels (~32k)",https://github.com/MCG-NJU/SparseOcc,,,http://arxiv.org/abs/2312.17118,https://zotero.org/alehanderoo/items/FQIMMNEE,https://doi.org/10.48550/arXiv.2312.17118,11/10/2025 2:27 (GMT+2)
Yes,Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction,"2D features, 3D features, 3D-fusion, classify","• Reuse sparse occupancy-guided 3D volume construction.
• Adopt 3D deformable lifting for voxel features.
• Integrate multi-view attention for view weighting.
• Use box-based occupancy pseudo-labels for supervision.
• Risk: needs accurate poses; static-scene assumption.
• Quick test: feed 4DGS with sparse voxel seeds.","• Improve multi-view indoor 3D object detection.
• Address limited voxel receptive fields in lifting.
• Reduce dense volume computational overhead.
• Avoid dependence on ground-truth geometry.
• Handle occlusion and view contribution adaptively.","• Strong for multi-view fusion; indoor static scenes.
• No tracking or dynamic separation implemented.
• Not calibrated-free; relies on reliable poses.
• Ideas transferable to 4DGS feature lifting.
• Open: robustness to pose noise, moving targets.
• Open: performance with sparse, non-overlapping cameras.","• Geometry- and context-aware 3D deformable attention.
• Multi-view attention for adaptive view weighting.
• Sparse coarse-to-fine volume construction.
• Occupancy prediction supervised by 3D boxes.
• Detection loss: centerness, IoU, focal.
• Occupancy binary cross-entropy loss.","• ScanNet: 1,201 train, 312 test scenes.
• ScanNet200: 200 classes, extended categories.
• ARKitScenes: 4,498 train, 549 test.
• Per scene: 40 train, 100 test views.
• Posed images; overlapping multi-view subsets.
• Annotations: 3D bounding boxes; static scenes.","• ResNet-50 FPN backbone extracting 2D features.
• DepthNet with plane sweeping cost volumes.
• 3D deformable attention for intra-view lifting.
• Multi-view attention for inter-view fusion.
• Sparse coarse-to-fine occupancy-guided refinement.
• Requires known intrinsics/extrinsics; no self-calibration.","• SOTA on ScanNet mAP@0.25 61.2, @0.5 35.2.
• Beats MVSDet by +5.0 @0.25, +3.9 @0.5.
• Lower training memory: 20GB vs 35GB.
• Higher FPS: 1.46 vs 0.87.
• Occupancy loss adds +6.7 @0.25 mAP.
• 3D deformable+MV attention boosts performance.",https://github.com/RM-Zhang/SGCDet,,,http://arxiv.org/abs/2507.18331,https://zotero.org/alehanderoo/items/ATHKG55F,https://doi.org/10.48550/arXiv.2507.18331,11/10/2025 2:26 (GMT+2)
Yes,Multimodal dataset for indoor 3D drone tracking,"2D features, 3D-fusion, pose, track","• Use dataset to train multi-view drone detectors.
• Adopt epipolar matching for cross-view association.
• Implement linear triangulation baseline for tracking.
• Leverage ArUco inside-out for self-pose calibration.
• Test PSO for multi-drone joint optimization.
• Exploit provided masks for dynamic-static separation.","• Provide multimodal indoor 3D drone tracking dataset.
• Address scarcity of multicamera, synchronized drone tracking data.
• Support both outside-in and inside-out localization approaches.
• Enable evaluation of multi-drone 3D tracking methods.
• Include real and simulated sequences for benchmarking.","• Strong fit: flying, multi-camera, synchronized indoor tracking.
• No occupancy grids or Gaussian Splatting provided.
• Classification absent; focus solely on localization.
• Real setup uses only four static cameras.
• Good source for calibration and association strategies.
• Consider adding calibration-free extrinsics via dynamics.","• Pinhole camera model with P=K[R|t] projection.
• Overdetermined linear system solved via pseudoinverse.
• Epipolar geometry for cross-camera association.
• Particle Swarm Optimization for 3D state estimation.
• P3P with ArUco for camera pose estimation.
• Background subtraction GMM for silhouette extraction.","• Indoor lab hall with marked walls.
• 31 sequences: 18 real, 13 simulated.
• Four static real cameras, eight simulated viewpoints.
• 25 fps, 1924x1082 real, 3840x2160 FPV.
• Annotations: Vicon 3D, 2D boxes, ArUco coordinates.
• 3D motion present; CC BY on Zenodo.","• YOLOv5 detection trained from mocap-projected boxes.
• Linear triangulation from multiple calibrated cameras.
• Epipolar matching to associate multi-drone detections.
• PSO optimizing 3D positions using silhouette overlaps.
• Inside-out pose via ArUco detection and P3P.
• Calibration via OpenCV; intrinsics from specs.","• Triangulation mean errors: 0.075–0.685 m.
• Swaps increase mean; medians remain low.
• PSO errors: 0.068–0.510 m across configs.
• More particles/iterations reduce PSO error.
• FPV ArUco position error: 0.042–0.054 m.
• Orientation differences moderate; yaw variance higher.",,,,https://www.nature.com/articles/s41597-025-04521-y,https://zotero.org/alehanderoo/items/QJHNYJZ7,https://doi.org/10.1038/s41597-025-04521-y,11/10/2025 2:25 (GMT+2)
Yes,Multi-camera multi-target drone tracking systems with trajectory-based target matching and re-identification,"2D features, 3D-fusion, pose, track","• Reuse trajectory cross-correlation for inter-camera ID association.
• Adopt EKF+flow for motion state priors.
• Assure overlapping FOV and time sync between cameras.
• Calibrate extrinsics or add self-calibration module.
• Mitigate background noise with robust detection/segmentation.
• Replace YOLOv3 with modern small-object detector.","• Multi-camera tracking and localization of drones in real time.
• Inter/intra-camera re-identification for consistent target IDs.
• Uses cross-correlation of trajectory features for matching.
• Addresses gap in multi-camera aerial re-identification.","• Strong fit for flying, multi-view tracking needs.
• No self-calibration; extrinsics assumed known.
• No occupancy or Gaussian Splatting components.
• Classification limited; focus on re-identification.
• Exploit trajectory features as 3D tracking prior.
• Open: synchronization, calibration drift, scale ambiguity.","• Trajectory signature X=f(Q, turning angle, curvature, pace).
• Cross-correlation maximization for inter-camera track matching.
• Mean-shift histogram distance with KL/Bhattacharyya metrics.
• EKF state estimation for motion prediction.
• Triangulation using camera positions, orientations, directional cosines.
• Normalized correlation threshold 0.7 for acceptance.","• Domain: drones and pedestrians.
• Two drone tests; three-camera human test.
• Cameras: two stationary for drones; three views for humans.
• Overlapping FOV with slight disparity.
• Annotations: N/A; detector outputs used.
• Availability: YouTube videos; dataset/license N/A.","• Hybrid detection: YOLOv3-Tiny and motion-based blob detection.
• Optical flow and EKF for single-camera tracking.
• Intra-camera association: nearest distance with RANSAC compensation.
• Inter-camera re-id: cross-correlate trajectory feature signals.
• Pose: assumes known camera positions and pan/tilt orientations.
• 3D fusion: least-squares triangulation of target distances.","• Successful intra/inter-camera re-id for single drone.
• Two-drone re-id degraded under background noise.
• Human multi-camera re-id consistent across three views.
• 3D coordinates obtained from two-camera setup.
• No quantitative metrics; qualitative demonstrations only.
• Real-time feasibility shown with simple hardware.",,,,https://ieeexplore.ieee.org/document/9476845/,https://zotero.org/alehanderoo/items/BA8GVBH4,https://doi.org/10.1109/ICUAS51884.2021.9476845,11/10/2025 2:21 (GMT+2)
Yes,Handcrafted and Deep Trackers,"2D features, track","• Reuse ECO-style multi-resolution 2D features.
• Adopt STRCF/SRDCF regularization for robust 2D cues.
• Fuse optical flow for motion-aware 2D features.
• Use Siamese matching for cross-view association seeds.
• Benchmark 2D trackers on flying-object videos.
• Treat outputs as priors for 4DGS reconstruction.","• Survey recent visual object tracking approaches.
• Compare handcrafted versus deep features in trackers.
• Evaluate 24 trackers across multiple benchmarks.
• Propose OTTC benchmark combining OTB and TC-128.
• Identify trends and gaps in tracking research.","• Focuses on monocular 2D; no multi-view overlap.
• No self-calibration or 3D occupancy methods.
• Useful for 2D temporal feature design.
• Siamese and DCF insights transferable to pre-processing.
• Lacks explainability and 3D tracking evaluations.","• DCF theory with FFT correlation theorem.
• MOSSE minimizes sum-squared error in Fourier.
• SRDCF introduces spatial regularization penalties.
• ECO uses GMM for appearance diversity.
• Siamese similarity learning with cross-correlation.","• OTB2013: 50 sequences; OTB2015: 100 sequences.
• TC-128: 128 sequences; OTTC: 186 sequences.
• VOT2017: 60 sequences with resets.
• Single-camera videos; overlapping views: N/A.
• Annotations: 2D boxes; no 3D motion labels.","• Categorize trackers: CFTs and non-CFTs.
• Evaluate using OPE precision and success AUC.
• Report VOT2017 EAO, accuracy, robustness.
• Use default params; speed measured on shared hardware.
• Backbones include VGG; features HOG, CN, CNN.
• Optical flow used in DMSRDCF motion features.","• Regularized CFTs outperform other categories.
• ECO and CCOT achieve top precision and success.
• Deep features generally surpass handcrafted features.
• HC+deep feature fusion further improves performance.
• KCF fastest; SiameseFC strong real-time.
• OTTC is more challenging than prior benchmarks.",http://bit.ly/2TV46Ka,,,https://dl.acm.org/doi/10.1145/3309665,https://zotero.org/alehanderoo/items/LDI7MPZJ,https://doi.org/10.1145/3309665,11/10/2025 2:17 (GMT+2)
Yes,Analysis Based on Recent Deep Learning Approaches Applied in Real-Time Multi-Object Tracking,"2D features, track","• Adopt PMBM/Kalman for probabilistic 3D tracking.
• Integrate Siamese ReID across overlapping RGB cameras.
• Leverage optical flow to stabilize motion affinities.
• Fuse high-quality detections into 4DGS feature volumes.
• Use TrackR-CNN masks for dynamic/static separation.
• Note: no self-calibration or 3D occupancy guidance.","• Review deep learning online MOT methods 2015–2020.
• Summarize detection quality and association focused approaches.
• Assess high-speed, low-compute tracking trade-offs.
• Modeling uncertainty and CNN affinity methods reviewed.
• Highlight multi-camera MOT progress and limitations.
• Identify challenges: occlusion, similarity, small objects.","• Primarily 2D MOT; limited 3D applicability.
• Multi-camera lacks overlapping, calibration-free specifics.
• Association/filtering insights useful for 3D tracking.
• No flying-object datasets; mostly pedestrians/vehicles.
• No Gaussian Splatting or volumetrics discussed.
• Extend methods with multi-view 3D geometry.","• Tracking-by-detection with DCNN feature learning.
• CLEAR MOT MOTA and MOTP formulations.
• Graph, min-cost flow association models.
• PMBM, GMPHD, GLMB Bayesian filters.
• Kalman filter with Hungarian assignment.
• Siamese and quadruplet metric learning.","• 95 papers across surveillance and driving domains.
• Datasets: MOT15/16/17, KITTI, UA-DETRAC, PETS09.
• Mostly monocular; some multi-camera network studies.
• Annotations: 2D boxes, IDs; occasional masks.
• 3D motion rarely; mostly ground-plane pedestrians.
• Availability: public benchmarks; review, no new dataset.","• PRISMA-guided search; 95 deep MOT works.
• Four themes: detection, speed, uncertainty, affinity.
• Detectors: Faster R-CNN, SSD, YOLO variants.
• Associations: MHT, CRF, LSTM, ReID, Siamese.
• Filters: Kalman, PMBM, GMPHD; correlation filters.
• Losses: triplet, softmax; occasional optical flow usage.","• Detection quality strongly drives tracking performance.
• Speedups often reduce accuracy; occlusions remain problematic.
• ReID/Siamese help IDs, struggle in dense crowds.
• Multi-camera identity consistency remains challenging.
• Reported runtimes: 0.01–1.9 seconds per frame.
• Accuracy ranges roughly 14% to 86%.",,,,https://ieeexplore.ieee.org/document/9359743/,https://zotero.org/alehanderoo/items/UUH6T2R9,https://doi.org/10.1109/ACCESS.2021.3060821,11/10/2025 2:15 (GMT+2)
Yes,"Multi-camera Multi-drone Detection, Tracking and Localization with Trajectory-based Re-identification","2D features, 3D-fusion, classify, pose, track","• Reuse trajectory-based re-ID for multi-view association.
• Adopt self-calibration using dynamic targets as references.
• Integrate optical flow for improved 2D temporal features.
• Replace blobs with modern tiny detectors for robustness.
• Fuse more cameras to improve depth accuracy.
• Quantify uncertainty for probabilistic 3D tracking outputs.","• Real-time multi-camera drone detection, tracking, localization.
• Address cross-camera re-identification during information fusion.
• Handle small, distant drones with minimal appearance cues.
• Provide 3D trajectories for multiple simultaneous drones.
• Demonstrate outdoor operation with overlapping camera views.","• Strong fit: multi-camera, flying 3D tracking focus.
• Lacks occupancy and Gaussian Splatting components.
• Self-calibration via drones aligns with thesis needs.
• Classification limited; augment with appearance classifiers.
• Requires all targets visible across overlapping cameras.
• Investigate synchronization, latency, and occlusion robustness.","• Geometry-based 3D estimation from pan-tilt camera models.
• Q = P + l R(α,β) D directional geometry equation.
• Distance scaling using FOV, pixel width, focal angles.
• Multi-camera least squares solving for unknown distances.
• Trajectory feature Xj: angle, curvature, pace.
• Cross-correlation maximizing inter-camera trajectory similarity.","• Domain: outdoor multi-drone flights over open field.
• Cameras: three; two ground webcams, one aerial.
• Overlap: central volume visible across ground cameras.
• Annotations: onboard GPS logs for trajectory comparison.
• 3D motion: two drones flying, overtaking maneuvers.
• Availability: YouTube videos; license unspecified.","• Hybrid detection: motion blobs plus YOLOv3-tiny verification.
• 2D tracking via geometry and optical flow integration.
• Calibration: self-align cameras using shared fixed points.
• 3D fusion: triangulation and least squares across cameras.
• Re-ID/classify via trajectory feature Xj cross-correlation.
• Implementation: MATLAB xcorr; real-time processing reported.","• Two drones tracked and localized simultaneously outdoors.
• Average positional error approximately eight percent.
• 3D trajectories align with GPS flight logs.
• Trajectory-based re-ID resolves duplicate broken tracks.
• System operates in real-time with three cameras.
• Accuracy limited by image quality and distance.",,,,https://ieeexplore.ieee.org/document/9358454/,https://zotero.org/alehanderoo/items/U2Z3G8LP,https://doi.org/10.1109/ICA-SYMP50206.2021.9358454,11/10/2025 2:13 (GMT+2)
Yes,Design a Hybrid Neural Network Tracking System Using Multiple Cameras,"2D features, classify, pose, track","• Repurpose late fusion as cross-view association prior
• Extend to overlapping views with triangulation
• Add self-calibration from dynamic motion cues
• Incorporate optical flow for temporal consistency
• Generalize to 3D volume, not ground-plane only
• Test on flying objects, multi-view RGB overlaps","• Improve tracking accuracy using multiple synchronized cameras
• Handle objects transitioning between camera zones
• Fuse per-camera detections for global position estimation
• Address limited FoV of single-camera trackers
• Provide trajectory prediction across camera handovers","• Good multi-camera framing, lacks 3D reconstruction
• Assumes known camera angles, not calibration-free
• Likely minimal view overlap; thesis needs overlap
• Object domain cars, not aerial targets
• No explainability methods provided
• Variance estimation details unclear","• World ground plane as common reference
• Per-camera rotation projection matrix P_i
• Weighted fusion with alpha-beta factors
• Variance-based weighting akin to Kalman gain
• Accuracy threshold A_th for measurement gating","• Road scene with two cars
• #frames/sequences not reported
• Three cameras along x-axis, overlap unstated
• Annotations: ground-truth 2D trajectories
• 3D motion: No, ground-plane only
• Availability/license: N/A","• Per-camera YOLO detection outputs L,A,C
• Independent networks, late fusion at prediction block
• Compute variances P_i, R_i per camera
• Alpha-beta weighted projection into ground plane
• MOT via class and position consistency checks
• Training: Adam, L2, batch 32, piecewise linear activation","• Trajectory estimation accuracy reported as 0.9653
• Error decreases and converges quickly
• Qualitative improvement over single-camera claimed
• Tested on two moving cars
• Runtime and memory: N/A",,,,https://ieeexplore.ieee.org/document/10827096/,https://zotero.org/alehanderoo/items/A367CEMN,https://doi.org/10.1109/PRAI62207.2024.10827096,11/10/2025 2:11 (GMT+2)
Yes,Multi-Object Multi-Camera Tracking Based on Deep Learning for Intelligent Transportation,"2D features, classify, track","• Reuse camera link modeling for multi-view association.
• Use DeepSORT baseline for initial 2D tracking.
• Adopt ID metrics (IDF1) for tracker evaluation.
• Risk: assumes ground-plane vehicles, non-overlapping views.
• Quick win: test topology-aware matching on overlapped cameras.
• Gap: lacks 3D occupancy, GS, flying-object focus.","• Survey deep-learning MOMCT for intelligent transportation.
• Unify detection, MOT, re-ID, cross-camera association.
• Compare datasets and evaluation metrics comprehensively.
• Visualize state-of-art systems and performance.
• Highlight recent advances missing in prior reviews.","• Strong on multi-camera association; weak on 3D reconstruction.
• Focuses on vehicles; mismatch for flying-object dynamics.
• Overlapping FOV scenarios discussed but not exploited geometrically.
• Consider self-calibration via trajectory correspondences across cameras.
• Need 4DGS-based dynamic volume modeling elsewhere.","• Modular pipeline: detection, SCT, re-ID, MC association.
• Similarity-based association using appearance and spatiotemporal cues.
• Graph models and clustering for trajectory linking.
• Metrics: MOTA, MOTP, IDF1, IDS emphasized.
• Losses: cross-entropy, triplet, contrastive for re-ID.","• Domain: road traffic surveillance and autonomous driving.
• Datasets: BDD100K, UA-DETRAC, KITTI, nuScenes.
• Cameras: multi-camera networks, overlapping/non-overlapping FOVs.
• Annotations: 2D boxes, IDs, trajectories.
• 3D motion present? Limited; mainly 2D tracking.
• Data availability: GitHub datasets link provided.","• Baseline: detect-then-track, then cross-camera association.
• Object detectors: YOLO, SSD, Faster/Mask R-CNN.
• Trackers: DeepSORT, MHT, RNN/LSTM association.
• Re-ID embeddings with siamese/triplet networks.
• Camera link models, spatiotemporal topology constraints.
• No calibration-free pose; optical flow rarely discussed.","• GCNM achieved IDF1 81.06 on BDD100K.
• UWIPL achieved IDF1 79.87 on BDD100K.
• BUPT 70.22; DyGLIP 64.9 on BDD100K.
• Online-MTMC 64.26; ELECTRICITY 53.8; NCCU 45.97.
• Single-stage detectors faster, two-stage generally more accurate.
• Robustness improved by attention, clustering, topology.",https://github.com/wwwj795/datasets,,,https://www.mdpi.com/1424-8220/23/8/3852,https://zotero.org/alehanderoo/items/M9YEH7IA,https://doi.org/10.3390/s23083852,11/10/2025 2:08 (GMT+2)
Yes,A robust deep networks based multi-object multi-camera tracking system for city scale traffic,"2D features, classify, track","• Reuse inter-class NMS for cleaner detections.
• Adopt aggregation loss for robust re-ID.
• Leverage Deep SORT for initial 2D tracklets.
• Limit comparisons to adjacent cameras when scalable.
• Beware non-overlapping settings; no geometry used.
• Benchmark modules within overlapping, multi-view setup.","• Robust MO-MCT for city-scale traffic surveillance.
• Handle non-overlapping cameras without calibration.
• Address occlusion, illumination, shadow, resolution challenges.
• Reduce training time with feature reuse.
• Improve re-ID and ID synchronization.","• Primarily 2D, non-overlapping; no 3D fusion.
• No extrinsic estimation; unsuitable for 3D occupancy.
• Useful re-ID loss and adjacency linkage ideas.
• No flying-object focus; vehicle domain only.
• Could seed identities before 3DGS reconstruction.","• Tracking-by-detection paradigm with Deep SORT.
• Inter-class NMS for overlapping vehicle classes.
• Aggregation loss: cross-entropy plus triplet.
• Hard mining: hard positives and negatives.
• Kalman filter constant-velocity motion model.","• AI City Challenge 2021 Track 3.
• 46 cameras, six scenarios, 215 minutes.
• Mostly non-overlapping views; adjacent camera matching.
• 313,931 boxes; 880 vehicle identities.
• Annotations: bounding boxes, identity labels.
• Available via registration; license unspecified.","• Mask R-CNN detections with inter-class NMS.
• Crop detections; build galleries and queries.
• Deep SORT/MORT tracklet generation and association.
• ResNet-152 backbone for re-ID feature extraction.
• Aggregation loss: triplet+cross-entropy, hard mining.
• RMSProp, lr 0.001, batch 128, GTX1080Ti.","• Validation IDF1 0.8289, Prec 0.9026, Rec 0.8527.
• ResNet152+Deep SORT outperform other variants.
• MORT variants underperform Deep SORT consistently.
• Test IDF1 0.5126 (Table 3).
• Strong vs eight methods on validation IDF1.",https://github.com/imranzaman5202/MO-MCT,,,https://link.springer.com/10.1007/s11042-023-16243-7,https://zotero.org/alehanderoo/items/TJCK96TY,https://doi.org/10.1007/s11042-023-16243-7,11/10/2025 2:06 (GMT+2)
Yes,DeepFusionMOT,"2D features, 3D features, 3D-fusion, track","• Adapt four-level association to multi-camera RGB pipeline
• Replace LiDAR 3D with 4DGS occupancy volumes
• Use IoU+distance cost on 3DGS volumes
• Early 2D tracks; upgrade when 3D volume stabilizes
• Add optical flow for motion cues
• Assess robustness with aerial, small, fast targets","• Balance 3D MOT accuracy and speed
• Robust association under occlusion and missed detections
• Fuse camera and LiDAR without heavy networks
• Track distant objects before LiDAR visibility","• Strong association ideas; LiDAR dependency mismatches thesis
• No multi-camera overlapping reasoning demonstrated
• No classification or explainability components
• Early 2D-to-3D switch suits 4DGS design
• Unclear calibration robustness without precise extrinsics
• Flying-object tracking performance remains untested","• Tracking-by-detection with four-level deep association
• Uses 3D IoU and distance cost
• Projection-based camera–LiDAR fusion
• Kalman-style state update as in AB3DMOT
• Separate 2D and 3D trajectory management","• Autonomous driving road scenes
• KITTI tracking; nuScenes tracking
• #cameras/layout/overlap: N/A
• 2D/3D bounding box annotations
• 3D motion present: vehicles, pedestrians
• Public datasets; standard licenses","• Inputs: 2D detector, 3D LiDAR detector
• Project 3D boxes to image; IoU-based fusion
• Four-level deep association strategy
• Calibration-based camera–LiDAR transform; not self-calibrated
• Track management with confirmed/tentative/reappeared/dead states
• Updates via AB3DMOT method; motion-only features","• Highest AMOTA, MOTA, Recall on nuScenes
• Highest HOTA and AssA on KITTI
• Significantly fewer ID switches than EagerMOT
• Ablations: appearance features small gains, large slowdown
• Stable across different detector choices
• Runs fast on CPU; real-time feasible",https://github.com/wangxiyang2022/DeepFusionMOT,,,https://ieeexplore.ieee.org/document/9810346/,https://zotero.org/alehanderoo/items/V9YSLY85,https://doi.org/10.1109/LRA.2022.3187264,11/10/2025 2:05 (GMT+2)
Yes,3D Object Detection and Tracking Methods using Deep Learning for Computer Vision Applications,"2D features, 3D features, 3D-fusion, classify, track","• Adopt AMOTA/AMOTP for MOT evaluation.
• Explore GNN association with RGB-only features.
• Investigate factor-graph backend for data association.
• Leverage multi-frame attention and optical flow.
• Avoid LiDAR dependencies; adapt monocular 3D methods.
• Use edge-based tracking as lightweight baseline.","• Survey 3D detection and tracking methods.
• Compare sensors, datasets, and application domains.
• Discuss evaluation metrics for 3D MOT.
• Identify challenges: occlusion, weather, compute constraints.
• Highlight future technologies and research directions.
• Gap: robust, efficient, benchmarkable 3D methods.","• Broad survey; limited multi-camera overlap discussion.
• No calibration-free pose estimation coverage.
• Flying objects largely unaddressed.
• No Gaussian Splatting or 4DGS relevance.
• Temporal cues sections useful for tracking.
• Consider RGB-only adaptations of cited methods.","• Taxonomy: RGB, LiDAR, fusion-based 3D perception.
• Data association via factor graph optimization.
• Graph neural networks for feature interaction.
• Quantum CNNs proposed for acceleration.
• Metrics: sMOTA, AMOTA, AMOTP for 3D MOT.
• Early/late fusion paradigms and frustum approaches.","• Survey paper; no original data collection.
• Domains: autonomous driving, robotics, AR, surveillance.
• #sequences/frames: N/A.
• #cameras/layout/overlap: N/A.
• Annotations: 3D boxes, segmentation; dataset-dependent.
• 3D motion present in benchmarks; availability varies.","• Categorizes RGB, point cloud, and fusion methods.
• Covers frustum, MV3D, BirdNet+, DPC-MN approaches.
• Describes factor graph joint association for MOT.
• GNN-based association for stable trajectories.
• Edge-based tracking variants and gradient methods.
• QCNN proposed for accelerating deep networks.","• No new experiments; primarily literature synthesis.
• Highlights multi-frame attention improving 3D detection.
• Notes FlowMOT robustness using point-wise flows.
• Introduces sMOTA, AMOTA, AMOTP metric discussions.
• Edge-based tracking described as efficient alternative.
• Summarizes LiDAR R-CNN, voxel-point hybrids.",,,,https://ieeexplore.ieee.org/document/9573964/,https://zotero.org/alehanderoo/items/SM3BJ9MZ,https://doi.org/10.1109/RTEICT52294.2021.9573964,11/10/2025 2:03 (GMT+2)
Yes,Generating Synthetic Training Data for Deep Learning-Based UAV Trajectory Prediction,"2D features, track","• Use MST to synthesize multi-view UAV trajectories.
• Condition 2D predictor to bridge detection gaps.
• Extend to multi-camera projections with shared 3D paths.
• Augment with optical flow or appearance cues.
• Beware 2D-only; integrate 3D fusion downstream.
• Validate domain gap on RGB-only datasets.","• Limited UAV trajectory datasets for learning-based prediction.
• Generate synthetic UAV trajectories in image space.
• Leverage quadrotor dynamics for realistic motion patterns.
• Train RNN-MDN solely on synthetic data.
• Benchmark against classical predictors on ANTI-UAV.","• Good for flying-object motion priors; single-camera only.
• No self-calibration; assumes known intrinsics/extrinsics.
• No 3D occupancy or Gaussian Splatting.
• MDN uncertainty helpful for probabilistic tracking.
• Extend to overlapping RGB cameras for thesis.
• Release of synthetic generator would aid reproducibility.","• Minimum snap trajectories via differential flatness.
• Piecewise polynomial path with derivative continuity constraints.
• Quadratic programming formulation with waypoint boundary conditions.
• RNN-MDN predicts Gaussian future position distributions.
• Optimizes negative log-likelihood over predicted Gaussians.","• Synthetic UAV trajectories, image-plane positions.
• 1000 MSTs; 10–20 fps sampled.
• Single pinhole camera; no multi-view overlap.
• Annotations: 2D centers from projection.
• 3D motion underlying; observed as 2D.
• ANTI-UAV: 100 EO/IR videos; public.","• Sample waypoints within camera frustum.
• Generate MST with speed, smoothness constraints.
• Project 3D path to image using intrinsics/extrinsics.
• Train LSTM-MDN on 2D positions.
• Loss: NLL of Gaussian predictions.
• Train 2000 epochs, Adam, lr decay.","• Outperforms Kalman CV and linear interpolation.
• EO 8-step FDE: 60.3 vs 81.1/87.0.
• IR 8-step FDE: 20.8 vs 22.3/24.2.
• Error increases with longer horizons.
• Diversity analysis reveals multiple motion prototypes.
• No runtime or memory reported.",,,,http://arxiv.org/abs/2107.00422,https://zotero.org/alehanderoo/items/5T4KF458,https://doi.org/10.5220/0010621400003061,11/10/2025 2:01 (GMT+2)
Yes,Mono3DVLT: Monocular-Video-Based 3D Visual Language Tracking,"2D features, 3D features, 3D-fusion, classify, track","• Reuse visual-language-depth fusion for 3D feature extraction
• Adapt memory TTM for temporal 3D tracking smoothing
• Replace monocular depth with multi-view geometry cues
• Remove language branch or use prompts for class hints
• Quick test: add optical flow features for dynamics
• Evaluate on flying-object sequences; domain adaptation needed","• Extend VLT from 2D to 3D tracking
• Enable monocular 3D tracking using language cues
• Avoid reliance on LiDAR, radar, depth sensors
• Provide dataset Mono3DVLT-V2X with language
• Release baseline Mono3DVLT-MT for task","• Monocular single-object; mismatches multi-camera, multi-object goals
• Useful temporal memory design for tracking stability
• Depth predictor may underperform for aerial scenes
• No occupancy or Gaussian splatting; requires additional modules
• No self-calibration or pose estimation mechanisms
• Open: scalability to multi-object, multi-view, 3D volume","• Multi-modal fusion: visual, depth, language tokens
• Language-guided visual encoder with MSDA, MHCA
• Language-guided depth encoder with MHCA, MHA
• Pixel-wise similarity scoring, Gaussian weighting map
• Memory-improved Token Turing Machine for temporal context
• Losses include focal, GIoU, L1, 3D IoU, Multi-Bin, Laplacian","• Domain: road scenes from V2X-Seq
• 79,158 sequences with text, 2D/3D boxes
• Splits: 56,106 train, 11,203 val, 11,849 test
• Cameras: monocular per sequence; no overlap
• Annotations: language, 2D bbox, 3D bbox
• 3D motion present; availability on GitHub; license N/A","• Pipeline: RoBERTa, Swin, depth predictor, encoder, decoder
• Pose strategy: none; monocular, no calibration
• 2D backbone: Swin Transformer; no optical flow
• 3D fusion: visual-language-depth attention, pixel-wise scores
• Heads: class, 2D box, 3D center/size/orientation/depth
• Training: AdamW 1e-4, Hungarian matching, composite losses","• Outperforms baselines on Mono3DVLT-V2X
• AOR 85.12%, ACE 0.521, PR@1.0 81.56%
• SR@0.9 58.86%, +9.93 over best baseline
• Ablations: Swin and memory TTM improve metrics
• Runtime and memory costs unreported",https://github.com/hongkai-wei/Mono3DVLT,,,,https://zotero.org/alehanderoo/items/CW8UHVYR,,11/10/2025 2:00 (GMT+2)
Yes,A Bayesian Filter for Multi-View 3D Multi-Object Tracking With Occlusion Handling,"2D features, 3D features, 3D-fusion, classify, track","• Reuse LoS-based occlusion probability module
• Adopt GLMB for multi-view 3D tracking
• Integrate JMS for motion-mode classification
• Ensure accurate extrinsics; calibration-free remains open
• Test with drones; strong vertical dynamics
• Combine with better monocular detectors for robustness","• Online multi-view 3D MOT with occlusions
• Avoid multi-camera retraining; monocular detectors suffice
• Operate in 3D world coordinates, not ground-only
• Handle vertical motion, jumping and falling
• Scalable complexity linear in total detections","• Strong fit: multi-camera 3D tracking with occlusions
• Mismatch: no calibration-free pose estimation
• No 3D occupancy or Gaussian Splatting
• Classification limited to standing/fallen modes
• Useful association backbone before 4DGS volume
• Verify performance on fast, small flying objects","• RFS GLMB multi-sensor Bayes filtering
• Occlusion-aware detection via LoS shadow modeling
• MS-GLMB recursion with GLMB approximation
• Ellipsoidal extent states; quadric projection
• Evaluation with OSPA(2) and 3D GIoU distances","• WILDTRACKS: 7 HD cameras, overlapping views
• 7x400 frames at 2 fps
• Annotations: ground-plane positions, 2D boxes
• CMC: 4 cameras, 5 sequences, 4 fps
• Annotations: 3D centroid, 3D extent, camera poses
• 3D vertical motion present; jumping/falling","• Monocular 2D detections: YOLOv3, Faster R-CNN
• Occlusion model using camera LoS and ellipsoid shadows
• Multi-view GLMB filtering with Gibbs sampling
• Quadric projection links 3D extents to 2D boxes
• JMS mode switch: standing versus fallen
• No multi-view training; calibrated camera matrices required","• Occlusion model improves MOTA and OSPA(2) consistently
• Comparable to Deep-Occlusion+KSP+ptrack on WILDTRACKS
• Tracks vertical motions; jumping and falling
• Handles camera addition/removal without retraining
• Runtime scales linearly with detections, quadratic in objects
• Fails when objects occluded in all views",,,,https://ieeexplore.ieee.org/document/9242263/,https://zotero.org/alehanderoo/items/ZKXGU3QD,https://doi.org/10.1109/TPAMI.2020.3034435,11/10/2025 1:57 (GMT+2)
Yes,Track initialization and re-identification for 3D multi-view multi-object tracking,"2D features, 3D-fusion, track","• Use MV-GLMB-AB for multi-view 3D tracking.
• Integrate feature-based re-ID for flying objects.
• Replace ground-plane clustering for aerial scenes.
• Evaluate occlusion ordering for airborne depth.
• Add optical flow to feature vectors.
• Develop calibration-free extrinsic estimation module.","• 3D MOT from multi-view monocular 2D detections.
• Integrates init, termination, re-ID, occlusion in filter.
• No detector retraining on camera reconfiguration.
• Linear complexity with detections across cameras.
• Addresses intractable exact Bayes filtering approximations.","• Strong multi-view tracking; pedestrian-focused assumptions.
• Requires calibrated cameras; not calibration-free.
• No 3D occupancy or Gaussian Splatting.
• Overlap exploited; suits overlapping RGB cameras.
• May struggle with erratic aerial motion.
• Extend to volumetric features and classification.","• Bayesian RFS with labeled GLMB approximation.
• Multi-view association maps across cameras.
• Occlusion via image-plane intersection-over-area score.
• Feature likelihood: stable/unstable modes, EMA updates.
• Nearly-constant-velocity 3D kinematics, ellipsoid shape.
• MV-GLMB recursion with importance sampling, truncation.","• Pedestrians, upright and fallen poses.
• WILDTRACK: 7 cameras, overlapping views.
• CMC: 4 cameras, five sequences.
• WT ground-plane labels; CMC 3D ellipsoids.
• 3D motion present across frames.
• Availability: WT public; CMC per prior work.","• Inputs: 2D boxes and appearance features.
• Known camera matrices; no self-calibration.
• Occlusion modeled by box overlap IoA.
• Adaptive birth via ground-plane clustering.
• Re-ID using cross-camera feature matching, TT relabeling.
• GLMB-based multi-view association and filtering.","• Outperforms MV-GLMB/MS-GLMB on MOTA, IDF1.
• Robust to camera addition, removal, repositioning.
• Re-identifies tracks after outages using features.
• IoA plus features yields best ablations.
• Faster runtime than prior MV-GLMB; online.",,,,https://linkinghub.elsevier.com/retrieve/pii/S1566253524002744,https://zotero.org/alehanderoo/items/6JWDF5PP,https://doi.org/10.1016/j.inffus.2024.102496,11/10/2025 1:45 (GMT+2)
Yes,MCBLT,"2D features, 3D features, 3D-fusion, pose, track","• Reuse early BEV fusion for multi-view feature aggregation
• Adapt hierarchical 3D GNN tracker for long occlusions
• Replace calibration with self-calibration SLAM for extrinsics
• Extend to volumetric 4DGS features instead of BEV
• Use 2D-3D association for robust appearance features
• Evaluate on flying-object datasets with overlapping cameras","• Robust MTMC 3D detection and tracking in BEV
• Address generalizability across scenes, camera counts, placements
• Improve long-term associations across thousands of frames
• Early multi-view aggregation vs late association limitations
• Handle occlusion, calibration distortions in multi-camera systems","• Strong multi-view tracking but assumes known calibration
• Ground-plane BEV limits flying object altitude modeling
• No occupancy grids or Gaussian Splatting integration
• Lacks classification beyond person; single-class focus
• Global GNN idea transferable to 3D flight tracking
• Need explainability modules; none provided","• BEVFormer spatiotemporal transformer with SCA and TSA
• 3D-2D projection using calibrated extrinsics, intrinsics
• Deformable DETR head for 3D box predictions
• Focal loss classification, L1 box regression
• Hierarchical GNN tracking with global merging block
• Appearance via multi-view ReID cosine distances","• Domains: warehouses, retail, hospitals; outdoor plaza (WildTrack)
• AICity'24: 90 scenes; 100M boxes; 2,491 people
• 953 cameras; static; varying views; partial overlaps
• WildTrack: 7 cameras; 400 frames; 2 FPS
• Annotations: AICity 3D boxes; WildTrack ground-plane grid
• Availability: public benchmarks; license details N/A","• Early multi-view BEV aggregation with dynamic camera counts
• Requires calibrated poses; static cameras; re-centering applied
• 2D detector DINO (FAN-small); ReID SOLIDER Swin-T
• BEVFormer SCA/TSA fusion; deformable DETR decoder
• 2D-3D association selects best crops; Hungarian matching
• Hierarchical GNN (SUSHI-3D) with global block associations","• AICity'24 SOTA: 81.22 HOTA, big gains
• WildTrack SOTA: 95.6 IDF1 using shared detections
• Global block boosts HOTA by +4.42 over heuristics
• 2D-3D association raises IDF1 from 63.2 to 93.4
• Detector ablations: mAP up to 95.36 validation
• Runtime ~1.5 FPS end-to-end on A100",,,,http://arxiv.org/abs/2412.00692,https://zotero.org/alehanderoo/items/C99JW6CY,https://doi.org/10.48550/arXiv.2412.00692,11/10/2025 1:43 (GMT+2)
Yes,Dynamic Graph CNN for Learning on Point Clouds,"3D features, classify","• Use EdgeConv on 3DGS splat centers as encoder.
• Replace voxel MLPs with dynamic graph blocks.
• Pretrain on ShapeNet; finetune for flying classes.
• Test robustness to sparse occupancy and occlusions.
• Risk: needs point clouds; no multi-view fusion.
• Integrate for classification; add separate 3D tracking.","• Learn features directly on irregular point clouds.
• Overcome missing topology in point cloud data.
• Capture local geometry via edge-based convolution.
• Replace fixed graphs with dynamic neighbors per layer.
• Improve classification and segmentation performance benchmarks.","• Strong 3D feature learning; fits feature volume encoding.
• Mismatch: no multi-camera overlap or calibration-free pose.
• No temporal modeling; unsuitable for tracking alone.
• Could process 4DGS object point extractions.
• Visualization aids semantics; limited formal explainability.","• EdgeConv uses asymmetric h(xi, xj−xi) with shared MLP.
• Dynamic kNN graphs recomputed after each layer.
• Symmetric max aggregation ensures permutation invariance.
• Partial translation invariance via relative coordinates.
• Cross-entropy losses for classification, segmentation.","• ModelNet40 CAD models; 12,311 shapes.
• ShapeNetPart; 16,881 shapes, 50 parts.
• S3DIS indoor scans; 6 areas, 272 rooms.
• No multi-camera; N/A overlap/layout.
• Annotations: categories, parts, semantic classes.
• Static shapes; no 3D motion; public datasets.","• Build kNN graph; compute EdgeConv edge features.
• Dynamically update graph after each EdgeConv layer.
• Classification: global pooling, FC layers, dropout.
• Segmentation: concatenate local+global features; per-point MLP.
• Pose strategy: N/A; no calibration steps.
• Training: SGD, cosine annealing, batchnorm, LeakyReLU.","• ModelNet40 OA 92.9%; 93.5% with 2048 points.
• Mean class accuracy 90.2%; surpasses baselines.
• ShapeNetPart mIoU 85.2%; competitive categories.
• S3DIS mean IoU 56.1%; OA 84.1%.
• Faster than PointNet++; 27.2ms forward; 21MB model.
• Robust to 50% point dropout in testing.",https://github.com/WangYueFt/dgcnn,,,http://arxiv.org/abs/1801.07829,https://zotero.org/alehanderoo/items/46GIZ4J9,https://doi.org/10.48550/arXiv.1801.07829,11/10/2025 1:42 (GMT+2)
Yes,Multi-view Convolutional Neural Networks for 3D Shape Recognition,"2D features, 3D-fusion, classify, explain","• Adopt view-pooling to fuse multi-camera 2D features
• Pretrain ImageNet; fine-tune per-camera domain
• Use gradient saliency for per-camera explainability
• Apply metric learning for compact embeddings
• Quick-win: aggregate per-camera features before 4DGS
• Risk: synthetic views differ from real scenes","• Evaluate view-based 2D vs native 3D shape descriptors
• Learn compact descriptor aggregating multiple rendered views
• Improve classification and retrieval accuracy on ModelNet40
• Reduce computation versus pairwise multi-view comparisons","• No flying objects; static CAD shapes only
• Multi-view fusion concept matches overlapping cameras
• No self-calibrated pose; fixed synthetic setups
• No occupancy, segmentation, or tracking addressed
• Saliency aligns with Grad-CAM-style explainability needs
• Useful front-end for 2D feature aggregation","• View-pooling layer performs element-wise max across views
• Shared-weights branches before pooling for invariance
• Large-margin low-rank Mahalanobis metric learning
• Softmax training; linear SVM on fc7 features
• Average-minimum distance metric for retrieval (Eq. 1)
• Gradient-based saliency maps across views for explainability","• Domain: CAD meshes, ModelNet40 categories
• 12,311 shapes, 40 classes evaluation
• 12 or 80 virtual cameras around centroid
• Annotations: category labels only, no instances
• No 3D motion; static synthetic renderings
• Public dataset; code/models available online","• Render multi-view images using Phong shading
• Extract VGG-M CNN features; fine-tune on views
• View-pool at conv5; aggregate compact descriptor
• Linear SVM classification; metric learning for retrieval
• Average-min distance across view descriptors for retrieval
• Pretrained on ImageNet; SGD; max view-pooling","• 90.1% accuracy vs ShapeNets 77.3%
• Retrieval mAP up to 80.2% with metric learning
• Multi-view pooling outperforms per-view voting baselines
• Pooling at conv5 best; earlier layers degrade
• Single-view CNN surpasses prior 3D descriptors
• Rendering per mesh under ten milliseconds",http://vis-www.cs.umass.edu/mvcnn,,,http://arxiv.org/abs/1505.00880,https://zotero.org/alehanderoo/items/UD42YKDN,https://doi.org/10.48550/arXiv.1505.00880,11/10/2025 1:41 (GMT+2)
Yes,Trusted Multi-View Classification With Dynamic Evidential Fusion,"2D features, classify, explain","• Use evidential heads per camera stream for reliability.
• Fuse per-camera beliefs via DST for trusted decisions.
• Gate downstream 4DGS using uncertainty thresholds.
• Add pseudo-view aggregator for cross-view complementarity.
• Beware DS independence assumption, correlated views.
• Quick win: retrofit Dirichlet head onto 2D backbones.","• Trusted multi-view classification with sample-wise reliability.
• Avoid overconfident softmax in multi-view settings.
• Handle noisy, corrupted, OOD view inputs.
• Dynamic per-view trust via uncertainty estimation.","• No pose, 3D fusion, or tracking support.
• Useful for multi-camera per-view trust weighting.
• Uncertainty aids OOD handling, sensor failures, occlusions.
• Independence assumption may break with correlated views.
• Extend DST to temporal smoothing for tracking.
• Consider Grad-CAM alongside evidences for interpretability.","• Variational Dirichlet for class probability distributions.
• Subjective logic maps Dirichlet to belief, uncertainty.
• Reduced Dempster-Shafer combination for view fusion.
• KL prior to uniform Dirichlet for calibration.
• Propositions guarantee accuracy/uncertainty integration properties.","• Vector datasets: Handwritten2, CUB(10), Caltech101, PIE, Scene15, HMDB.
• RGB-D: SUN RGB-D(10,335 pairs), NYUDv2(1,449 pairs).
• Image-text: UMPC-FOOD101, 86,796 samples, web-sourced.
• Views: multi-feature, RGB+Depth, image+text modalities.
• Annotations: class labels; no localization.
• Multi-camera overlap, 3D motion, licenses: N/A.","• Per-view encoder predicts Dirichlet concentration via Softplus.
• Variational loss: expected log-likelihood minus KL to uniform.
• Subjective logic yields belief masses and uncertainty.
• Fuse beliefs using reduced Dempster-Shafer rule.
• Pseudo-view ETMC via concatenated features.
• Backbones: ResNet-18/152, BERT; Adam optimization.","• Outperforms baselines across six vector datasets.
• Robust under noisy views; performance degrades least.
• SUN RGB-D fusion: 61.3% best accuracy.
• NYUDv2 fusion: 72.5% best accuracy.
• FOOD101 accuracy: 91.47%, highest reported.
• Compute similar to late fusion; far lower than TRecg.",https://github.com/hanmenghan/TMC,,,https://ieeexplore.ieee.org/document/9767662/,https://zotero.org/alehanderoo/items/NFMS2XS3,https://doi.org/10.1109/TPAMI.2022.3171983,11/10/2025 1:39 (GMT+2)
Yes,A review on multi-view learning,"2D features, classify","• Adopt auto-weighted late fusion across overlapping cameras.
• Use multi-view contrastive alignment for camera feature consistency.
• Handle dropped cameras via indicator-matrix training.
• Incorporate graph regularization across camera views.
• Prototype semi-supervised training with limited labels.
• Ablate early vs late fusion for 3D occupancy.","• Survey multi-view learning across paradigms and tasks.
• Unify classification and clustering under learning paradigms.
• Highlight consensus and complementarity principles.
• Discuss challenges: inconsistency, fusion, scalability, labels.
• Fill gap in paradigm-based taxonomy of methods.","• No 3D occupancy, tracking, or 4DGS discussed.
• Useful fusion/consistency principles for multi-camera RGB.
• Lacks calibration-free pose estimation guidance.
• Highlights incomplete-view handling for camera outages.
• Suggests semi-supervision beneficial for sparse labels.
• Needs extension to 3D feature fusion and tracking.","• Consensus and complementarity principles formalize view relations.
• Early vs late fusion conceptual schemes.
• Co-training, MKL, subspace, graph, CCA foundations.
• Regularizers: Laplacian, low-rank, sparsity, tensor constraints.
• Objectives: mutual information, conditional entropy minimization.
• Maximum entropy discrimination for semi-supervision.","• Survey paper; no primary datasets.
• #sequences/frames: N/A.
• #cameras/layout/overlap: N/A.
• Annotation type: N/A.
• 3D-motion present?: N/A.
• Availability: Open-access CC BY 4.0.","• Taxonomy: classification, semi-supervised, clustering, semi-supervised clustering.
• Fusion strategies: early fusion, late fusion.
• Incomplete-view handling: indicator matrices, imputation, contrastive alignment.
• Representations: autoencoders, graph learning, subspace, MKL, DGPs.
• Semi-supervision: label propagation, Laplacian regularization, anchors.
• Optimization: auto-weighted views, low-rank tensor constraints.","• Multi-view consistently outperforms single-view in cited studies.
• Consensus/complementarity central to robust view integration.
• Fusion timing influences performance and interpretability.
• Incomplete-view methods mitigate missing data impacts.
• Scalable ensemble clustering achieves near-linear complexity.
• Identified open issues: fusion, scalability, generalization.",,,,https://link.springer.com/10.1007/s11704-024-40004-w,https://zotero.org/alehanderoo/items/GC45CNHM,https://doi.org/10.1007/s11704-024-40004-w,11/10/2025 1:34 (GMT+2)
Yes,Multi-view Deep Network for Cross-View Classification,"2D features, classify","• Reuse g_c to learn view-invariant 2D embeddings
• Supervise with class labels across camera views
• Pretrain on multi-view RGB datasets
• Integrate embeddings into 3D fusion module
• Note: no geometry, no 3D or tracking
• Quick-win: compare Fisher vs triplet loss","• Cross-view recognition across heterogeneous views
• Learn view-invariant discriminative representation
• Address nonlinear view discrepancy limitations
• Supervised alternative to linear/unsupervised methods","• Good for cross-view 2D feature invariance
• No multi-camera calibration or 3D reasoning
• Inapplicable to flying object tracking directly
• Could aid multi-camera classification robustness
• Requires shared identity labels across views
• No explainability mechanisms provided","• View-specific subnets f_i remove view variations
• Common subnet g_c extracts shared embedding
• Fisher loss (Rayleigh quotient) objective
• Minimize S_W, maximize S_B across views
• Backprop via matrix calculus and chain rule","• Domains: faces across pose and feature types
• MultiPIE: 337 subjects, 13 poses
• FRGC: 6,388 train; 4,007 target; 2,004 query
• LFW: 6,943 train; 1,000 target; 1,221 query
• #cameras/overlap: N/A; single-image per view
• Annotations: identity labels; no 3D motion","• Per-view PCA dimensionality reduction
• View-specific MLPs (ReLU), 200-300 neurons
• Common MLP 300-1200-200 with linear head
• Fisher loss using S_W and S_B traces
• Optimized by gradient descent, L-BFGS
• Inputs: intensity and LBP; no optical flow","• MultiPIE avg rank-1 0.887, best among baselines
• Up to 13% improvement at 90-degree pose
• Outperforms MvDA, GMA, CCA, KCCA, PLS
• FRGC/LFW: higher ROC than linear/deep baselines
• Performance sensitive to common layer size
• Runtime/memory: N/A",,,,http://ieeexplore.ieee.org/document/7780893/,https://zotero.org/alehanderoo/items/LDWH2M2C,https://doi.org/10.1109/CVPR.2016.524,11/10/2025 1:32 (GMT+2)
Yes,Quick-UAV OCSORT: a fast and robust UAV-based tracking-by detection algorithm (2025),"2D features, track","• Adopt pre-matching to reduce expensive feature extraction.
• Use CMC for jittered UAV footage stabilization.
• Integrate B-IoU+ for small-object association gating.
• Leverage LightMBN for appearance embeddings.
• Port ideas to 3D gating before volumetric fusion.
• Evaluate on flying-object classes, not ground pedestrians.","• Robust UAV MOT under jitter and small targets.
• Reduce ReID overhead while maintaining accuracy.
• Improve matching for small, fast-moving objects.
• Mitigate camera motion drift during association.
• Address OC-SORT ID switches and occlusions.","• Single-camera 2D; no multi-view overlap exploited.
• No 3D occupancy, fusion, or volumetric features.
• Useful association tricks transferable to 3D tracking.
• CMC concept may extend to multi-camera egomotion.
• Lacks classification and explainability modules.
• Evaluate robustness on rapid aerial targets.","• Observation-centric SORT association paradigm.
• LightMBN multi-branch OSNet-based ReID.
• Buffered-IoU+ with longest-side adaptive buffer.
• Camera motion compensation via RANSAC affine.
• Kalman filtering for motion prediction.","• VisDrone2019-MOT aerial videos.
• ReID trained on PRAI1581 and VRAI.
• 72,497 images, 9,459 IDs for ReID.
• Single moving UAV camera, no multi-view overlap.
• 2D boxes with IDs; 3D motion N/A.
• Public datasets; licensing details N/A.","• YOLOv8s detector for frame-wise detections.
• Pre-matching using IoU, threshold γ=0.8.
• LightMBN ReID features for ambiguous matches.
• Cascade matching with B-IoU+ gating.
• OCR stage matches last observation with B-IoU+.
• CMC using RANSAC affine; Kalman filter updates.","• +0.88% MOTA over OC-SORT baseline.
• +5.51% IDF1 and +3.01% HOTA gains.
• 19.58 FPS with all modules enabled.
• B-IoU+ beats IoU and B-IoU.
• LightMBN improves Rank-1 by 4.0%.
• Best tradeoff among TBD baselines tested.",,,,https://link.springer.com/10.1007/s11760-025-04149-w,https://zotero.org/alehanderoo/items/H9S3MB4N,https://doi.org/10.1007/s11760-025-04149-w,11/10/2025 1:28 (GMT+2)
Yes,Enhancing UAV Detection in Surveillance Camera Videos through Spatiotemporal Information and Optical Flow,"2D features, classify","• Reuse multi-frame input for temporal cues.
• Integrate optical flow channels per camera.
• Use as per-camera detector before 3D fusion.
• Replace LK with RAFT/PWC for robustness.
• Evaluate on flying-object, multi-scene datasets.
• Benchmark speed on thesis hardware.","• Detect UAVs in surveillance camera videos.
• Small, low-contrast, weak-feature targets.
• Integrate temporal cues and optical flow.
• Improve accuracy while retaining real-time speed.
• Gap: surveillance drones, small-object video detection.","• Relevant for 2D features of flying objects.
• No multi-camera overlap or self-calibration.
• No 3D reconstruction, fusion, or tracking.
• Optical flow module likely beneficial upstream.
• Consider lightweight flow to preserve FPS.
• Needs integration with 3DGS, tracking later.","• Human vision uses temporal variations.
• Lucas–Kanade optical flow assumptions.
• Brightness constancy, small motion, spatial coherence.
• Spatiotemporal feature stacking emulates motion perception.
• Losses: Varifocal (cls,obj), CIoU (bbox).","• Surveillance scenes: parks, highways, forests, urban, sky.
• 4625 images, 1920×1080 resolution.
• Single camera; no multi-view overlap.
• Drone bounding-box annotations.
• Flying UAV motion present.
• Data available on request; not public.","• YOLOv5s backbone with CSPDarknet, FPN, Focus, SPP.
• Extend input to multi-frame sequences.
• Add LK optical flow tensors, HSV-encoded.
• Concatenate frames and flow; 1x1 channel reduction.
• Single-class detection head; no tracking module.
• Training: Adam lr 0.001, 500 epochs, bs=8.","• AP 86.87%, +11.49% over YOLOv5s.
• Multi-frame alone: +6.89% AP gain.
• Speed remains above 30 FPS.
• Frames+flow reduce speed by 34.61 FPS.
• Outperforms YOLOv5x accuracy; balanced AP-FPS.
• Improved detection of tiny, low-contrast drones.",,,,https://www.mdpi.com/1424-8220/23/13/6037,https://zotero.org/alehanderoo/items/5P45XC83,https://doi.org/10.3390/s23136037,11/10/2025 1:25 (GMT+2)
Yes,Does Deep Super-Resolution Enhance UAV Detection?,"2D features, classify","• Use SR preprocessor per camera for tiny objects.
• Jointly train SR with 2D detector features.
• Validate on multi-view overlapping cameras, flying targets.
• Beware artifacts harming optical flow and temporal cues.
• Prefer lightweight SR for real-time multi-camera throughput.
• Compare SR versus scaling on tiny-object benchmarks.","• Improve long-range UAV detection in optical videos.
• Address tiny, low-resolution drones fading into sky.
• Integrate deep super-resolution with detector end-to-end.
• Quantify recall gains over LR and naive upsampling.
• Evaluate runtime versus accuracy trade-offs.","• Strongly 2D; no multi-view, pose, or 3D tracking.
• Useful for tiny-object enhancement in RGB streams.
• End-to-end preprocessing joint training is valuable design.
• Needs temporal SR or flow consistency for tracking.
• Evaluate SR impact on 3DGS feature extraction quality.
• Open: SR generalization across camera domains, distances.","• SR via DCSCN residual CNN with skip connections.
• Reconstruction learns residuals over bicubic upsampling.
• PReLU activations and Network-in-Network modules used.
• Joint backprop tunes SR for detection objectives.
• SR trained with MSE and L2 regularization.","• Domain: UAV vs bird optical video sequences.
• Training: ~16K frames; validation: 2,814 frames.
• Test: four clips, distances ~120–280 meters.
• Single-camera views; no multi-view overlap reported.
• 2D bounding boxes; classes: drone, bird, rest.
• 3D motion present: flying targets; availability mixed/public.","• Pipeline: DCSCN x2 SR then Faster R-CNN detector.
• Backbone: MobileNet; RPN proposals; ROI pooling 7x7x512.
• End-to-end fine-tuning from detector back to SR.
• SR training: Adam, MSE, L2, Berkeley dataset.
• Detector training: SGD, 70k iters, LR schedule.
• NMS IoU 0.7; detection threshold 0.9; Titan XP.","• Recall gains up to 32.4% on hardest clip.
• Precision near 100% across settings, unchanged.
• cDCSCN 81.25% faster, ~1.9% mean recall drop.
• Naive upscaling improves some, worse than SR overall.
• DCSCN 0.32 FPS; cDCSCN 0.58 FPS measured.",,,,https://ieeexplore.ieee.org/document/8909865/,https://zotero.org/alehanderoo/items/46C8RS4N,https://doi.org/10.1109/AVSS.2019.8909865,11/10/2025 1:22 (GMT+2)
Yes,DroneSense,"2D features, 3D features, 3D-fusion, classify, pose","• Reuse cyclic angle loss for 3D orientation.
• Adapt U-Net parts segmentation for drone masks.
• Replace depth with multiview RGB-derived pseudo-depth.
• Integrate decision-tree head for class-specific experts.
• Validate 2D-only ablation using optical flow features.","• Full drone characterization from optical sensors.
• Identify type, segment parts, estimate orientation angles.
• Address scarcity of high-quality labeled drone datasets.
• Bridge gap beyond detection to detailed characterization.
• Validate sim-to-real using SPAD depth-intensity data.","• Strong fit: flying drones; weak: multi-camera overlap absent.
• No tracking; integrate with 3D MOT later.
• Useful fusion ideas; replace SPAD with multiview RGB.
• Consider explainability overlays for orientation decisions.
• Evaluate multi-drone scenes, occlusions, background clutter.","• Decision-tree classifier followed by ensemble subnets.
• Drone Feature Encoder fuses intensity and depth histograms.
• U-Net decoder for component-wise segmentation.
• Cyclic angle regression loss for periodic rotations.
• Sigmoid for type; ReLU for three orientation axes.","• Domain: consumer drones in flight.
• 72k simulated SPAD-like frames; 3.6k validation.
• Single SPAD camera; intensity+depth; no multi-view overlap.
• Labels: type, pixel segmentation, yaw/pitch/roll angles.
• 3D motion poses simulated; limited real sample available.
• Unreal generator public; dataset reproducible via code.","• Pipeline: detect type, then segment and orient.
• Pose via depth+intensity; no extrinsic self-calibration.
• 2D conv on intensity; 3D conv on histograms.
• Features concatenated; fusion precedes task-specific heads.
• Heads: sigmoid classifier; three ReLU regressors; U-Net.
• Training: BCE, cross-entropy, cyclic loss, Adam 1e-3.","• >90% accuracy across identification, segmentation, orientation.
• Real-data orientation achieved ~95% average accuracy.
• Depth input slightly outperforms intensity for orientation.
• Lower resolution degrades segmentation, orientation precision.
• Runtime/memory: N/A.",https://github.com/HWQuantum/DroneSense,,,https://ieeexplore.ieee.org/document/9743918,https://zotero.org/alehanderoo/items/7JJX2C6X,https://doi.org/10.1109/ACCESS.2022.3162866,11/10/2025 1:21 (GMT+2)
Yes,Deep learning-based strategies for the detection and tracking of drones using several cameras,"2D features, classify, track","• Reuse lightweight tiny-object detector front-end.
• Adopt montage single-pass multi-camera processing.
• Integrate TCT scheduling for attention control.
• Replace 2D Kalman with 3D estimator.
• Add optical flow motion cues.
• Collect overlapping multi-camera drone datasets.","• Autonomous multi-camera drone detection and tracking.
• Reduce GPU memory and runtime for surveillance.
• Detect tiny drones at long ranges.
• Minimize false alarms vs conventional methods.
• Single-pass detection on overlaid camera frames.","• Strong fit for flying object detection.
• Lacks overlapping multi-view and 3D reasoning.
• Useful resource-aware multi-camera scheduling ideas.
• Separate detect/classify improves small-object performance.
• Needs calibration-free pose estimation module.
• No public data hinders reproducibility.","• Lightweight YOLOv3 architecture for small objects.
• Kalman filter for motion state estimation.
• Hungarian assignment using bounding-box centroids.
• TCT score combines motion and visual cues.
• Size-dependent beta weighting for c_overall.
• Moving-average windowed classification decision.","• Drone, bird, airplane surveillance videos.
• 20 videos, 800 frames evaluated.
• Two RGB cameras: wide static, turret narrow.
• Unknown overlap; turret targets selected TCT.
• Bounding boxes, class labels; no 3D.
• Dataset unavailable; confidentiality claimed.","• Single lightweight YOLO detector for both frames.
• Calibration assumed; no self-calibration provided.
• No optical flow; YOLO features only.
• Montage overlay enables simultaneous multi-frame detection.
• YOLO classifier 64x64, four classes, ~10k images.
• Kalman+Hungarian tracking; windowed moving-average decision.","• YOLO true positive 0.91, zero false alarms.
• Haar true positive 0.95, 0.42 false alarms.
• GMM true positive 0.98, 0.31 false alarms.
• Tracks small, distant drones reliably.
• Overlay method preserves FPS and memory.
• YOLO inference approximately 0.07 seconds.",,,,https://link.springer.com/10.1186/s41074-019-0059-x,https://zotero.org/alehanderoo/items/6PYGJKBQ,https://doi.org/10.1186/s41074-019-0059-x,11/10/2025 1:18 (GMT+2)
Yes,Field Test Validations of Vision-based Multi-camera Multi-drone Tracking and 3D Localizing with Concurrent Camera Pose Estimation,"2D features, 3D-fusion, pose, track","• Reuse PTZ pose estimation with static landmarks.
• Adopt geometric 3D fusion from 2D bearings.
• Use trajectory-based cross-view association for flying objects.
• Replace KCF/DCF with modern detectors, optical flow.
• Risk: needs static references, accurate camera positions.
• Quick-win: replicate three-camera overlap; validate against RTK GPS.","• Vision-based multi-camera drone tracking and 3D localizing.
• Field validation against onboard GPS ground truth.
• Concurrent PTZ camera pose estimation without prior calibration.
• Address arbitrary non-aligned multi-camera geometry constraints.
• Multi-drone re-identification across overlapping cameras.","• Strong fit for multi-camera 3D tracking of drones.
• Lacks classification, occupancy, Gaussian splatting.
• Useful self-calibrated PTZ pose from static scene.
• Re-ID via trajectory heuristics; simple but effective.
• Open: robustness under occlusion, roll, larger swarms.
• Requires known camera positions; clarify acquisition method.","• Geometric triangulation using arbitrary non-aligned cameras.
• Direction cosines with pan/tilt cosine corrections.
• Distance scaling from FOV, focal, pixel ratios.
• Multi-drone nonlinear system solved least squares.
• PTZ pose via yaw/pitch rotation matrices, zoom scaling.
• Pose solved using nonlinear solver (fsolve).","• Domain: outdoor multi-drone flights.
• Scenarios: crossing and head-on passes.
• Cameras: 3; two ground, one aerial.
• Overlapping views of central flight volume.
• Ground truth: onboard GPS positions.
• Availability: YouTube videos; dataset details N/A.","• Motion-based blob detection for initial drone detection.
• KCF/DCF trackers for per-camera tracking.
• Cross-camera re-ID via trajectories and relative positions.
• 3D localization from fused 2D bearings geometry.
• Concurrent PTZ pose estimation using static references, fsolve.
• Offline processing; no deep training; optional optical flow.","• Average 3D error ±1 m at ~50 m.
• Good temporal-spatial agreement with GPS trajectories.
• Performance consistent across crossing, head-on scenarios.
• Errors attributed to pose estimation and GPS noise.
• Works with arbitrary non-aligned, overlapping cameras.
• Runtime real-time claimed; experiments processed offline.",,,,https://ieeexplore.ieee.org/document/9435654/,https://zotero.org/alehanderoo/items/U3PYQ5R3,https://doi.org/10.1109/ICCRE51898.2021.9435654,11/10/2025 1:16 (GMT+2)
Yes,Reconstruction of 3D flight trajectories from ad-hoc camera networks,"2D features, 3D-fusion, pose, track","• Reuse dynamic-target self-calibration for extrinsics and sync.
• Adopt spatio-temporal BA with RS and motion priors.
• Integrate 2D detector with optical-flow for robustness.
• Ensure ≥2-view overlap; avoid near-straight trajectories.
• Extend to multi-target association before 4DGS volumes.
• Test online reconstruction latency and stability.","• Outside-in 3D tracking of UAVs from ad-hoc cameras.
• Handles unknown poses, unsynced streams, rolling shutter.
• No reliance on static-scene features or calibration targets.
• Gap: practical multi-camera self-calibration using dynamic target.","• Strong fit for flying, multi-view, calibration-free tracking.
• Single-target only; no classification or occupancy.
• No static/dynamic separation beyond sky-background assumption.
• Incorporate RS modeling into 4DGS tracking module.
• Consider occlusions, cluttered backgrounds, non-sky scenes.
• Evaluate multi-object tracking and identity maintenance.","• Projective geometry with epipolar constraints for two-view estimation.
• Joint estimation of time shift and fundamental matrix.
• Spatio-temporal bundle adjustment minimizing reprojection error.
• Rolling-shutter time model per row integrated in BA.
• Spline trajectory with least-force, kinetic-energy priors.","• Domain: outdoor UAV flights, sky-background scenes.
• Datasets: 4 ours, 1 external; 120–600s sequences.
• Cameras: 4–7 consumer devices; overlapping views.
• Unsynced, rolling-shutter; intrinsics pre-calibrated.
• Annotations: 2D detections; RTK GNSS 3D ground truth.
• 3D motion present; data and code publicly available.","• Detect UAV via background subtraction, GMM.
• Fit per-camera 2D splines for temporal interpolation.
• Estimate pairwise pose and time-shift with minimal solver RANSAC.
• Triangulate, build 3D spline; register cameras via P3P.
• Bundle adjust poses, trajectory, sync, RS scan speed.
• Motion priors: least-force, kinetic-energy; no learning.","• Mean trajectory error <20 cm on datasets 1–3.
• Challenging dataset 4: mean error <40 cm.
• Halved RMSE versus prior work on dataset 5.
• Sub-frame desync recovery up to 50-frame initial offset.
• RS modeling improves hard cases; small overall gains.
• Camera pose error mean 17 cm on dataset 3.",https://github.com/CenekAlbl/mvus,,,http://arxiv.org/abs/2003.04784,https://zotero.org/alehanderoo/items/DTVZ8BU6,https://doi.org/10.48550/arXiv.2003.04784,11/10/2025 1:09 (GMT+2)
Yes,Flight Dynamics-Based Recovery of a UAV Trajectory Using Ground Cameras,"2D features, pose, track","• Use dynamics-regularized BA for extrinsics self-calibration.
• Adopt min-over-detections loss for association robustness.
• Initialize via RANSAC triangulation across overlapping cameras.
• Replace GMM with detector+optical flow for tiny objects.
• Extend to multi-object trajectories and online updates.
• Validate with unsynchronized cameras; handle clock drift.","• Estimate UAV 6-DoF trajectory from ground cameras.
• Overcome tiny target limitations in single-view tracking.
• Jointly optimize camera poses and 3D trajectory.
• Regularize trajectory using flight dynamics prior.
• Avoid explicit cross-view appearance matching.","• Strong match: flying object, multi-camera overlapping volume.
• Limited: no occupancy, 3DGS, or classification.
• Requires known intrinsics, synchronization, single target.
• Inspiring calibration strategy using tracked dynamics.
• Consider multi-target data association extensions.","• SfM BA with min-over-detections reprojection loss.
• Objective: E + λR with dynamics prior.
• Flight dynamics mappings F,G between X and Γ.
• Smooth latent U,Φ,Θ via Gaussian kernel H.
• Preserve sparsity using indirect regularization, Eq.13.","• Synthetic: 100 sequences, 10 cameras, 510 timesteps.
• LAB indoor: 6 cameras, 35s@30fps, OptiTrack ground truth.
• FARM outdoor: 6 cameras, 4min@15fps, GPS ground truth.
• Fixed cameras with overlapping central volume coverage.
• Detections: background subtraction candidates, multiple per frame.
• Availability unspecified; simulator GitHub provided.","• Intrinsics known; synchronized videos; single UAV assumption.
• Extrinsics initialized via handheld SfM calibration.
• RANSAC multi-view triangulation for per-timestep hypotheses.
• BA optimizing poses, trajectory, and detection assignments.
• Dynamics-based prior on smoothed U,Φ,Θ variables.
• Implemented in Ceres; robust loss; KF filters detections.","• Dynamics prior improves RMSE over baselines consistently.
• LAB RMSE: 0.0757m vs BA 0.1165m.
• Robust to camera pose perturbations; recovers control signals.
• FARM: 1.636m RMSE vs 1.998m baseline.
• Multiple-detections cost improves association and accuracy.",https://github.com/OMARI1988/Quadrotor_MATLAB_simulation,,,https://ieeexplore.ieee.org/document/8099749/,https://zotero.org/alehanderoo/items/KL538ZNZ,https://doi.org/10.1109/CVPR.2017.266,11/10/2025 1:07 (GMT+2)
Yes,FusionTrack,"2D features, track","• Reuse TMP to stabilize per-object temporal features.
• Adapt ReID transformer for multi-view feature aggregation.
• Employ NFM for overlap-aware cross-view associations.
• Combine cross-view matches with self-calibration via dynamics.
• Use SVT outputs to seed 4DGS object tracks.
• Risk: No 3D cues; geometry remains unsolved.","• Address arbitrary-view multi-drone multi-object tracking.
• Fill gap: free-viewpoint MVMOT lacking benchmarks.
• Propose FusionTrack end-to-end tracking+ReID framework.
• Construct MDMOT with overlapping/non-overlapping views.
• Target robust cross-view association under occlusions, appearance changes.","• Strong multi-view identity association; lacks 3D geometry.
• Useful memory and update design for temporal fusion.
• Progressive activation stabilizes cross-view learning.
• No optical flow or motion model leveraged.
• Applicability to flying-object segmentation remains unclear.
• Real-time performance and scalability unreported.","• Transformer queries for detection, tracking, and ReID.
• Tracklet Memory Pool maintains temporal identity consistency.
• Object Update Module aggregates cross-frame and cross-view features.
• Uncertainty-weighted multitask loss balances tracking and ReID.
• Losses: focal, L1, GIoU, ID cross-entropy, triplet.
• View-guided hierarchical clustering with neighbor consistency constraints.","• Drone urban traffic videos, multi-view.
• 20 sequences, ~122k frames total.
• 3-5 drones per clip, partial overlaps.
• Annotations: per-frame boxes, IDs across views.
• Moving cameras and targets; 3D motion exists.
• Availability/license: N/A.","• DETR-like detection and tracking decoders.
• No camera pose estimation; calibration not addressed.
• Backbone: ResNet50; DAB-Deformable-DETR pretrain.
• ReID transformer with frame embeddings.
• OUM temporal and cross-view query refinement.
• Uncertainty-weighted multitask loss; focal, L1, GIoU, triplet.","• MDMOT: CVMA 80.8, CVIDF1 75.2; best overall.
• Outperforms CrossMOT on four benchmarks.
• SVT: MOTA 88.13, IDF1 92.04, transformer-best.
• Ablations: TMP major boost; OUM, NFM incremental.
• Runtime or memory figures: N/A.",,,,http://arxiv.org/abs/2505.18727,https://zotero.org/alehanderoo/items/H9VIZ9BK,https://doi.org/10.48550/arXiv.2505.18727,11/10/2025 1:05 (GMT+2)
Yes,A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data,"2D features, 3D-fusion, pose, track","• Reuse post-track re-ID for 3D track continuity.
• Use epipolar triangulation as baseline 3D-fusion.
• Replace calibration with self-calibration from trajectories.
• Validate multi-camera synchronization, overlapping volume coverage.
• Add optical flow for temporal cues.
• Quick-win: test ByteTrack+triangulation on aerial datasets.","• Improve underwater MOT for small fish.
• Adapt single-view trackers to multi-view stereo.
• Handle occlusion, appearance changes, refraction noise.
• Generate 3D tracks from dual videos.
• Evaluate precision/recall, HOTA, IDF1.","• Multi-view overlap aligns with thesis goals.
• No calibration-free pose; add self-calibration module.
• No static/dynamic separation; 3DGS absent.
• Single-class detection; no classification head evaluation.
• Suitable baseline for 3D MOT without depth sensors.
• Open: robustness to flying objects, background clutter.","• Epipolar geometry with fundamental matrix F.
• Stereo triangulation using known intrinsics/extrinsics.
• YOLOv8 losses: CIoU, BCE, CE.
• ByteTrack association with Kalman prediction.
• Assumes synchronized stereo, overlapping FOV.","• Underwater fish ecology videos.
• 182 videos; 84,464 frames total.
• 91 stereo pairs; two cameras per pair.
• MOT-format 2D boxes as annotations.
• 3D motion present; swimming dynamics.
• Stereo calibration provided; license N/A.","• YOLOv8 detection; ByteTrack tracking.
• Post-track re-ID heuristic merges fragmented IDs.
• Stereo matching via epipolar lines, F estimation.
• Triangulate matched pairs to 3D coordinates.
• Evaluate HOTA, MOTA, IDF1, precision, recall.
• Uses provided stereo calibration; not self-calibrated.","• Precision 0.868; recall 0.480.
• HOTA 0.100; MOTA 0.506; IDF1 0.677.
• DetA extremely low: 0.023.
• Re-ID reduced IDs by 43.3% average.
• Stereo matching: 22–100% accuracy; longer videos stronger.
• 3D tracks created, but incomplete coverage.",,,,http://arxiv.org/abs/2505.17201,https://zotero.org/alehanderoo/items/VK6HL5S8,https://doi.org/10.48550/arXiv.2505.17201,11/10/2025 1:00 (GMT+2)
Yes,MITracker,"2D features, 3D features, 3D-fusion, pose, track","• Reuse 3D projection and BEV-guided fusion module.
• Add self-calibration to replace required extrinsics.
• Extend Z resolution for true 3D occupancy.
• Replace BBox head with 3D occupancy/class heads.
• Test with flying-object multi-view indoor sequences.
• Optionally integrate optical flow for temporal cues.","• Robust MVOT under occlusion and target loss.
• Lack of general multi-view tracking datasets.
• Integrate cross-view features into spatially consistent representation.
• Enable recovery in occluded views via fusion.
• Bridge single-view limits using overlapping cameras.","• Strong fit for overlapping multi-camera fusion.
• Mismatch: single-target, 2D boxes, no occupancy.
• Relies on calibration; thesis needs self-calibration.
• Z=3 limits altitude precision for flying objects.
• No Gaussian Splatting; add 4DGS for dynamics.
• No explainability; add Grad-CAM on BEV features.","• ViT backbone with temporal tokens for continuity.
• Project 2D features to 3D volume.
• BEV-guided aggregation reduces view ambiguity.
• Spatial-enhanced attention using 3D-aware token.
• Losses: focal, GIoU, L1, BEV focal.
• Assumes calibrated intrinsics/extrinsics and view overlap.","• Indoor scenes, everyday objects domain.
• 260 videos, 234K annotated frames.
• 3-4 synchronized Azure Kinects, overlapping FOVs.
• Annotations: 2D BBoxes, BEV coordinates, invisible labels.
• 3D motion present, e.g., tennis ball.
• License: N/A.","• Two-stage training: single-view then multi-view fine-tune.
• Calibration-based projection to 3D feature volume.
• ViT-B DINOv2 encoder, no optical flow.
• Aggregate along Z, produce BEV map.
• BBox head (CenterNet) per view; spatial attention.
• Losses: focal, GIoU, L1, BEV; A100 GPUs.","• SOTA on MVTrack multi-view: PNorm 91.87%.
• Single-view AUC 68.57% on MVTrack.
• GMTD zero-shot PNorm 87.05%.
• Recovery rate 79.2% vs 56.7% baseline.
• Ablations: BEV loss and spatial attention crucial.
• Runtime 14 FPS; modest memory increase.",https://miilaboratory.github.io/MITracker,,,http://arxiv.org/abs/2502.20111,https://zotero.org/alehanderoo/items/NVU5GFET,https://doi.org/10.48550/arXiv.2502.20111,11/10/2025 0:57 (GMT+2)
Yes,"Multi-view Tracking, Re-ID, and Social Network Analysis of a Flock of Visually Similar Birds in an Outdoor Aviary","2D features, 3D features, 3D-fusion, classify, track","• Adopt RT pointcloud reconstruction for multi-view fusion.
• Use trifocal constraints to reduce ghost correspondences.
• Integrate re-ID embeddings for 3D association continuity.
• Replace background subtraction with motion-aware CNN.
• Experiment: initialize 4DGS from reconstructed pointclouds.
• Risk: requires calibration; extend to self-calibrated poses.","• Track visually similar birds in 3D aviary.
• Handle occlusions, lighting, posture, scale variability.
• Evaluate RT vs stereo multi-view trackers.
• Provide multi-view dataset and WILD challenge.
• Assess re-ID for maintaining identities across occlusions.","• Strong fit: multi-camera, 3D flying birds.
• Not calibration-free; requires tagged extrinsics.
• No occupancy or Gaussian Splatting modeling.
• 2D features lack optical flow exploitation.
• Re-ID helps classification-like identity cues.
• No explainability analyses provided.","• RT vs TR tradeoffs in multi-view tracking.
• Epipolar and trifocal constraints for cross-view matching.
• DLT triangulation for 3D point reconstruction.
• DBSCAN clustering for pointcloud instance grouping.
• LPT with Hungarian matching for temporal association.
• Re-ID trained with triplet and cross-entropy losses.","• Domain: cowbirds in outdoor aviary.
• Four 15-minute multi-view segments.
• Eight synchronized cameras, 40 Hz, overlapping views.
• Annotations: 3D head/tail start-end, identities.
• 3D motion: hops, flights, perching transitions.
• Availability: promised release; license N/A.","• Mask R-CNN plus background subtraction for detections.
• Cross-view pixel matching with epipolar and trifocal checks.
• DLT triangulation to reconstruct dense 3D pointclouds.
• DBSCAN clustering; LPT tracking; re-tracking via extrapolation.
• Re-ID: ResNet50, BNNeck, triplet+ID losses.
• Cameras calibrated using checkerboard and AprilTags extrinsics.","• Pointcloud RT outperforms stereo across all thresholds.
• AC0.1 short segments: 0.44 vs 0.17 stereo.
• Performance degrades with longer sequences.
• 'Oracle' matching boosts long-sequence accuracy substantially.
• Re-ID: 68% overall; 97% at 0.8 threshold subset.
• Failure modes: occlusions, shadows, cluster splits/merges.",,,,http://arxiv.org/abs/2212.00266,https://zotero.org/alehanderoo/items/EIGKP6LV,https://doi.org/10.48550/arXiv.2212.00266,11/10/2025 0:56 (GMT+2)
Yes,Generic Multiview Visual Tracking,"2D features, track","• Reuse TPN for cross-view occlusion recovery
• Share multiview model via CCF
• Add optical flow to strengthen temporal cues
• Integrate explicit 3D triangulation for occupancy
• Evaluate on small flying objects
• Maintain calibration-free assumptions carefully","• Calibration-free generic multiview tracking
• Handle occlusion and view change
• Allow camera movement without extrinsics
• No specific object model required
• Provide new multiview tracking dataset","• Useful for calibration-free, overlapping multi-camera setups
• Lacks 3D occupancy or Gaussian Splatting
• No classification or explainability modules
• Single-object focus; extend to multi-object tracking
• Might struggle with tiny aerial targets
• Consider TPN extension to 3D coordinates","• DCF backbone with shared correlation filter
• Collaborative correlation filter objective function
• RNN-based TPN maps cross-view motion vectors
• Feature loss: L2 plus Sobel gradient term
• Confidence-weighted fusion for trajectory correction","• Generic multiview tracking across diverse scenes
• 10 sequences, 17,571 frames total
• 2–3 synchronized uncalibrated cameras
• Over 75% overlap; 1080p, 30fps
• Annotations: per-frame boxes, occlusion states
• Availability: code/dataset promised; license N/A","• ResNet-50 spatial-aware feature extractor
• Calibration-free; geometry encoded implicitly via TPN
• Correlation filter tracking with CCF updates
• 2D features only; optical flow N/A
• 3D fusion or triangulation: N/A
• Training: Adam, Rprop; losses described","• Robustness improved on GMTD: 0.7477 vs 0.2985
• Accuracy slightly below ECO: 0.6984 vs 0.7541
• PETS2009 robustness: 0.568 vs 0.434
• TPN-S outperforms naive baselines
• Runtime and memory: N/A",,,,http://arxiv.org/abs/1904.02553,https://zotero.org/alehanderoo/items/KLBHXJ4D,https://doi.org/10.48550/arXiv.1904.02553,11/10/2025 0:54 (GMT+2)
Yes,Visual Tracking With Multiview Trajectory Prediction,"2D features, track","• Reuse TPN for cross-view imputation of occluded views.
• Adopt CCF for shared multi-camera appearance model.
• Add optical flow features for stronger temporal cues.
• Ensure synchronization and confidence gating between views.
• Evaluate on flying objects with overlapping RGB cameras.","• Robust tracking under occlusion and view change.
• Generic multiview tracking without calibration assumptions.
• Avoid target-specific detectors or static cameras.
• Integrate cross-view cues to recover failures.","• Strong fit for multi-camera overlap; calibration-free helpful.
• Lacks 3D occupancy, fusion, and classification.
• Trajectory mapping may struggle with aerial depth changes.
• Hidden-state geometry idea transferable to self-calibration.
• Needs robust initialization; add re-detection safeguards.","• Cross-camera TPN maps motion vectors between views.
• Collaborative correlation filter aggregates reliable views.
• Two-stage feature training with FFT simulation.
• L2 loss with Sobel regularization for denoising.
• Sample weighting α correlates with view confidence.","• Generic multiview scenes, indoor/outdoor mixed.
• GMTD: 10 sequences, 17,571 frames.
• 2-3 synchronized uncalibrated cameras, overlapping >75%.
• Per-frame axis-aligned bbox, occlusion states.
• 3D motion implicit; only 2D trajectories annotated.
• Availability promised; license unspecified.","• Shared ResNet-50 feature extractor with FFT layers.
• Online collaborative correlation filter across views.
• Calibration-free TPN predicts occluded view trajectories.
• No 3D fusion; per-view 2D tracking only.
• TPN: encoder+RNN+PoseNet; hidden states per pair.
• Training: L2+Sobel, Adam; TPN Rprop, 20 epochs.","• Robustness 0.7477 vs ECO 0.2985 on GMTD.
• Accuracy 0.6984 vs ECO 0.7541 on GMTD.
• Highest success rates on CAMPUS in most sequences.
• TPN-S outperforms naive and TPN-O variants.
• Failure when initialization wrong; occlusions still challenging.
• Runtime and memory not reported.",,,,https://ieeexplore.ieee.org/document/9166744/,https://zotero.org/alehanderoo/items/XLD5F8FD,https://doi.org/10.1109/TIP.2020.3014952,11/10/2025 0:52 (GMT+2)
Yes,All-Day Multi-Camera Multi-Target Tracking,"2D features, track","• Leverage GMT association for cross-view ID linking
• Adopt NTC-style context cues for association robustness
• Adapt lighting-guided weighting using RGB-only cues
• Use fused 2D tracks to seed 3D reconstruction
• Beware reliance on thermal; replace with temporal features
• Test day/night robustness in overlapping RGB setup","• Robust MCMT tracking under day-night lighting
• Address low-light appearance degradation in MCMT
• Introduce infrared modality to complement RGB
• Construct RGBT MCMT dataset with low-light sequences
• Propose adaptive multi-modal fusion and context-aware tracking","• Strong for MCMT; not addressing 3D occupancy
• Multi-camera overlap aligns with thesis constraints
• Targets are ground objects, not flying
• No self-calibrated extrinsics or 3D fusion
• Lighting-aware gating concept transferable to RGB
• Open: geometry constraints versus appearance-only association","• Mamba-based selective state-space fusion (ADMF)
• Lighting Guidance Module with cross-entropy lighting loss
• CenterNet detection: heatmap, size, offset losses
• Transformer GMT association with cross-entropy assignment loss
• Nearby Target Collection contextual feature concatenation","• Domain: outdoor MCMT with UAV views
• Sequences: 88 across 19 scenes
• Frames: 118K×2 total frames
• Cameras: two drones, overlapping FoVs
• Annotations: 2D boxes with cross-view IDs
• 3D motion: moving cameras and targets","• DLA-34 backbones for RGB/IR feature extraction
• LGM predicts lighting; cross-entropy supervision
• ADMF mamba fusion across modalities
• CenterNet detector on fused features
• NTC concatenates nearby targets to Re-ID
• GMT transformer association; Adam, 20k iterations","• Outperforms baselines on M3Track single-camera MOTA (+2.73)
• Large gains on cross-view CVMA (+16.79) and CVIDF1 (+13.96)
• Nighttime improvements: +2.88 MOTA, +8.6 CVIDF1
• ADMF, LGM, LI ablations show consistent benefits
• NTC best with one neighbor; area threshold 0.6
• Runtime/memory: N/A",https://github.com/QTRACKY/ADMCMT,,,,https://zotero.org/alehanderoo/items/KEB7TDYE,,11/10/2025 0:47 (GMT+2)
Yes,DINO-Tracker,"2D features, track","• Use Delta-DINO for per-camera 2D feature tracking.
• Provide occlusion-aware tracks to 4DGS segmentation.
• Distill refined features as temporal descriptors.
• Seed multi-view matching with DINO best-buddies.
• Risks: single-view only; no geometric consistency.
• Quick test on drones, flying object videos.","• Self-supervised long-term dense point tracking in video.
• Reduce reliance on synthetic supervised trajectory datasets.
• Handle long occlusions and severe appearance ambiguities.
• Adapt DINO features for temporally consistent tracking.
• Combine test-time optimization with external image priors.","• Strong temporal features; helpful pre-step for 3D pipeline.
• Mismatch: single-view; no multi-camera overlap handling.
• Useful occlusion handling for small flying objects.
• Extend with multi-view consistency and epipolar constraints.
• Try calibration-free pose via tracked dynamics later.
• Evaluate under aerial viewpoints, fast parallax motion.","• Delta-DINO residuals refine frozen DINOv2 features.
• Cost volume with cosine similarity for matching.
• Contrastive best-buddies loss on feature correspondences.
• RAFT-supervised flow loss with Huber regression.
• Cycle-consistency and prior-preservation regularization terms.","• Real videos: TAP-Vid-DAVIS, TAP-Vid-Kinetics, BADJA.
• 30 DAVIS videos; 100 Kinetics subset; 9 BADJA.
• Single-camera videos; no multi-view overlap.
• Annotations: trajectories, visibility, keypoints.
• 3D-motion present? N/A.
• Public benchmarks; licenses per dataset.","• Extract DINOv2 features; predict residuals via Delta-DINO.
• Build cost volumes; CNN-refiner; softmax coordinate regression.
• Test-time training per video with self-supervision.
• RAFT optical flow for short-term pseudo-labels.
• Feature best-buddies and cycle-consistency contrastive losses.
• Occlusion prediction via trajectory agreement and feature similarity.","• SOTA among self-supervised; near supervised on TAP-Vid.
• Large gains under high occlusion rates.
• Outperforms Omnimotion across evaluated datasets.
• BADJA best δseg and δ3px results.
• Training about 1.6 hours per 100 frames.
• Inference faster, lower memory than TAPIR, Co-Tracker.",https://dino-tracker.github.io,,,http://arxiv.org/abs/2403.14548,https://zotero.org/alehanderoo/items/TCSCZV8U,https://doi.org/10.48550/arXiv.2403.14548,11/10/2025 0:44 (GMT+2)
Yes,FEATUP: A MODEL-AGNOSTIC FRAMEWORK FOR FEATURES AT ANY RESOLUTION,"2D features, explain","• Use FeatUp to sharpen per-camera 2D features.
• Improve 2D masks before 3DGS/4DGS fusion.
• Apply implicit variant for tiny flying objects.
• Ensure cross-camera consistency; risk: edge-induced misalignments.
• Experiment: CAM-guided segmentation refinement for drones.
• Explore multi-view jitter using self-calibrated warps.","• Upsample deep features to recover spatial detail.
• Preserve semantics while increasing feature resolution.
• Model-agnostic, drop-in for dense prediction tasks.
• Multi-view consistency from jittered input supervision.","• Strong fit for 2D feature quality, not 3D.
• No multi-camera overlap handling; add cross-view constraints.
• Could aid small-object segmentation in clutter.
• Lacks temporal consistency; integrate flow-based regularization.
• Useful for explainability modules (Grad-CAM).
• Verify generalization outdoors, aerial viewpoints.","• Multi-view consistency loss analogous to NeRF.
• Gaussian likelihood with adaptive uncertainty modeling.
• Learned downsampler: blur or attention pooling.
• JBU-based guided upsampler generalization with MLP.
• Implicit MLP with Fourier positional and color features.
• Total variation prior on feature magnitude.","• Domains: ImageNet, COCO-Stuff, ADE20k.
• CAM eval: 2000 ImageNet validation images.
• Segmentation: COCO-Stuff 27-class linear probe.
• Depth: MiDaS pseudo-labels; scale/shift-invariant supervision.
• Cameras: single-view; overlap N/A; 3D motion: N/A.
• Code available: aka.ms/featup, tinyurl.com/28h3yppa.","• Jitter inputs; collect multiple low-res feature views.
• Reconstruct views from latent high-res feature map.
• Pose/self-calibration: N/A.
• Backbones: ViT/DINO/ResNet/CLIP; no optical flow.
• Upsamplers: JBU stack or implicit MLP.
• Losses: Gaussian likelihood, uncertainty, TV (implicit).","• Better CAM A.D.↓ and A.I.↑ than baselines.
• Higher segmentation mIoU with linear probes.
• Lower depth RMSE; higher δ>1.25 accuracy.
• SegFormer mIoU improved with modest compute.
• CUDA JBU faster, lower memory than Unfold.
• Robust across ViT, ResNet, CLIP backbones.",https://tinyurl.com/28h3yppa,,,,https://zotero.org/alehanderoo/items/A49ICZ3Z,,11/10/2025 0:41 (GMT+2)
Yes,DINOv3,"2D features, 3D features, classify, pose, track","• Use DINOv3 for 2D semantic feature extraction.
• Leverage patch-similarity for flow-less tracking cues.
• Pair with VGGT for calibration-free pose estimation.
• Feed features to 4DGS object reconstruction.
• Start with ViT-L distilled for efficiency.
• Validate on multi-view drone datasets with overlap.","• Scale SSL to versatile vision foundation models.
• Fix dense feature degradation in long training.
• Leverage massive data without annotations.
• Provide frozen backbones for many tasks.
• Release distilled models for resource constraints.","• Strong fit for 2D features and tracking.
• No native occupancy, fusion, or Gaussian splatting.
• VGGT integration helps multi-view pose.
• Aerial results suggest domain transfer to flying.
• Compute heavy; distilled variants likely sufficient.
• Need explainability modules; not provided.","• Joint DINO and iBOT self-supervised objectives.
• Koleo regularizer for uniform feature distribution.
• Gram anchoring matches patch similarity Gram matrices.
• Rotary positional embeddings with box jittering.
• Register tokens stabilize attention communications.
• Constant schedules with EMA teacher training.","• Pretraining on LVD-1689M curated web images.
• Raw pool approximately 17B Instagram images.
• Single-view RGB images; no multi-camera overlap.
• Self-supervised; no human annotations.
• Video benchmarks include DAVIS, YTVOS, MOSE.
• Availability: models released; license N/A.","• ViT-7B/16 backbone with registers and RoPE.
• Losses: DINO, iBOT, Koleo, Gram anchoring.
• High-resolution adaptation with mixed crops.
• Distillation to ViT-S/B/L/H+, ConvNeXt.
• Frozen-backbone decoders for detection, segmentation, depth.
• Constant LR, batch 4096, multi-crop training.","• ADE20k linear mIoU 55.9, best SSL.
• NYUv2 RMSE 0.309, KITTI 2.346, top.
• DAVIS J&F 83.3 at large resolution.
• COCO mAP 66.1 with frozen backbone.
• Gram anchoring adds +2 mIoU on ADE20k.
• Strong 3D correspondences; NAVI recall 64.4%.",,,,http://arxiv.org/abs/2508.10104,https://zotero.org/alehanderoo/items/PTE7NBR3,https://doi.org/10.48550/arXiv.2508.10104,11/10/2025 0:39 (GMT+2)
Yes,LM-MCVT,"2D features, 3D-fusion, classify","• Reuse GEEF for multi-camera feature fusion.
• Adopt hybrid CNN-ViT local/global 2D encoders.
• Start with four overlapping cameras for efficiency.
• Replace pooling fusion in 3DGS pipeline.
• Beware no temporal modeling or self-calibration.
• Quick-win: benchmark GEEF vs average pooling on ours.","• Efficient multi-view 3D object recognition for robots.
• Reduce compute versus dense multi-view backbones.
• Capture both local and global features effectively.
• Improve multi-view fusion beyond average pooling.","• Focuses classification, not occupancy or tracking.
• Multi-view but not multi-camera calibration-free.
• No flying-object datasets; static tabletop objects.
• Useful fusion insights for overlapping camera setup.
• Consider extending entropy weights with temporal uncertainty.","• Hybrid CNN-ViT for local/global feature synergy.
• Entropy-weighted multi-view fusion (GEEF) formulation.
• Class-token plus patch embeddings jointly considered.
• Assumes complementary RGBD, limited views sufficient.
• Loss function unspecified.","• ModelNet40/10 synthetic CAD objects; classification labels.
• OmniObject3D real scans; 6000 objects, 190 categories.
• Four-view primary setting; also 1–12 views tested.
• Views arranged circular or hemidodecahedron layouts.
• Static objects; no 3D motion or time.
• Public datasets; license N/A.","• Render multi-view RGBD; initial conv+norm preprocessing.
• Pre-residual CNN encoders for local details.
• Local/global transformers for cross-view context.
• Middle-residual CNN encoders refine transformer embeddings.
• GEEF entropy-weighted fusion; MLP classifier head.
• Adam optimizer, lr 2e-4, cosine schedule; loss N/A.","• ModelNet10 RGBD: 98.5% with four views.
• ModelNet40 RGBD: 95.6% with four views.
• OmniObject3D RGBD: 85.1%, best among baselines.
• 10.5M params; 7.0 ms RGBD inference.
• GEEF outperforms AEF/ACF/ECF on fusion accuracy.
• Robust across varied four-view configurations.",,,,http://arxiv.org/abs/2504.19256,https://zotero.org/alehanderoo/items/FNYCQREA,https://doi.org/10.48550/arXiv.2504.19256,11/10/2025 0:37 (GMT+2)
Yes,Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image,"2D features, 3D features, 3D-fusion, classify","• Reuse 2D-to-3D lifting for multi-view voxel fusion
• Adapt 3D heatmap centers for flying objects
• Use PCA-SDF for fast occupancy reconstruction
• Inject optical flow into lifted voxel features
• Add self-calibration using multi-view dynamics
• Prototype multi-camera fused 3D heatmap centers","• Monocular 3D detection and reconstruction from one image
• Address depth ambiguity of 2D feature methods
• Learn voxel features aligned to scene space
• Jointly predict 3D boxes and surfaces
• Improve speed over local implicit methods","• Monocular setting; no multi-camera overlap support
• No tracking or temporal modeling provided
• Indoor static objects; not flying-object focused
• Voxel lifting aligns with planned 4DGS separation
• PCA-SDF suits efficient occupancy reconstruction needs
• Requires known calibration; not calibration-free","• 2D-to-3D feature lifting via projection matrix
• Positional encoding to reduce ray feature smearing
• 3D U-Net aggregates local and global context
• CenterNet3D heatmap for 3D object centers
• Coarse occupancy plus local PCA-SDF representation
• Losses: focal, L1 box, voxel CE, SDF L2","• ShapeNet pairs/triplets synthetic renderings datasets
• ScanNet-MDR: 18k real single images
• Single monocular camera, known calibration, no overlap
• Annotations: 3D boxes, CAD, occupancy, SDF samples
• No 3D motion; static objects and scenes
• Code page available; dataset release unspecified","• ResNet34 multi-scale 2D features, upsampled, 1x1 convs
• Back-project to voxels using camera matrix P
• Apply positional encoding; 3D U-Net aggregation
• CenterNet3D: 3D heatmap; regress offsets, size, yaw
• Coarse occupancy branch; fine PCA-SDF via MLP codes
• Joint training with focal, L1, CE, SDF L2","• mAP 51.5% vs 48.6% (@0.5 IoU)
• Higher reconstruction IoU than CoReNet, Points2Objects
• ~5% global IoU gain on ShapeNet-triplets
• PCA-SDF ~10x faster than DeepLS inference
• Positional encoding improves both detection and reconstruction
• ScanNet-MDR: mAP 22.8%, IoU 38.2%",http://cvlab.cse.msu.edu/project-mdr.html,,,http://arxiv.org/abs/2111.03098,https://zotero.org/alehanderoo/items/Q9CSZ2SE,https://doi.org/10.48550/arXiv.2111.03098,11/10/2025 0:35 (GMT+2)
Yes,BARF: Bundle-Adjusting Neural Radiance Fields,"3D features, 3D-fusion, pose","• Use BARF for self-calibrated multi-view extrinsics.
• Initialize 4DGS/3DGS with BARF poses.
• Mask dynamics; restrict BARF to static background.
• Extend schedule to multi-camera joint BA.
• Validate with overlapping RGB-only aerial setups.","• Train NeRF with imperfect or unknown camera poses.
• Jointly optimize registration and reconstruction from RGB.
• Mitigate positional encoding hurting pose convergence.
• Enable view synthesis and camera localization.
• Provide coarse-to-fine registration strategy.","• Strong fit for self-calibrated pose from RGB.
• Mismatch: assumes static scenes, no dynamics.
• Single-camera sequences; adapt for synchronized multi-cams.
• Helpful coarse-to-fine encoding idea for 4DGS.
• Requires dense overlap; flying targets may violate.","• Photometric synthesis objective for joint optimization.
• Connection to Lucas-Kanade gradient-based alignment.
• Differentiable volume rendering constraints NeRF training.
• Frequency-masked positional encoding as low-pass filter.
• Jacobian-based steepest descent for pose updates.","• Synthetic NeRF objects: 8 scenes, 100 images each.
• LLFF real scenes: 8 forward-facing handheld sequences.
• Single moving camera; overlapping views via motion parallax.
• Intrinsics known; extrinsics unknown or perturbed.
• Static scenes; no dynamic 3D motion.
• Public datasets; project page linked.","• Optimize poses and NeRF jointly with L2 photometric loss.
• Lie algebra se(3)/sl(3) pose parametrization.
• Coarse-to-fine positional encoding scheduling (alpha ramp).
• Differentiable volume rendering for 3D fusion/compositing.
• Adam optimizer; ray sampling; 128–200K iterations.
• No 2D feature or flow backbones.","• Near-perfect pose recovery on synthetic (<0.2° rotation).
• View synthesis comparable to reference NeRF.
• Full positional encoding fails registration badly.
• No-encoding registers but yields blurry reconstructions.
• Runtime, memory efficiency not reported.",https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF,,,,https://zotero.org/alehanderoo/items/RTWVZC76,,11/10/2025 0:34 (GMT+2)
Yes,NEURAL GROUNDPLANS: PERSISTENT NEURAL SCENE REPRESENTATIONS FROM A SINGLE IMAGE,"2D features, 3D features, 3D-fusion, pose","• Use dynamic groundplan for movable occupancy initialization
• Extend BEV to multi-height layers for flying
• Integrate optical flow for temporal association
• Train on overlapping RGB multi-view videos
• Add calibration-free pose from static background","• Persistent 3D from single image
• Disentangle static and dynamic elements
• Self-supervised learning from multi-view videos
• Enable novel view synthesis and editing
• Overcome memory costs of 3D voxels","• Static/dynamic separation suits pipeline needs
• Ground-aligned compression weak for altitude variance
• No tracking; add temporal association module
• Requires calibration; self-calibration desirable
• Not Gaussian Splatting; integration nontrivial
• Multi-view overlap matches thesis setup","• Ground-aligned 2D feature grids (BEV)
• Contracted coordinates for unbounded scenes
• Differentiable volume rendering with MLP
• Static-dynamic density/color compositing
• Losses: L2, LPIPS, surface, sparsity
• Shift-equivariant 2D CNN processing","• Domains: CLEVR, CoSY driving
• CoSY: 9000 scenes, 10 frames
• 15 cameras per scene, wide baselines
• Calibrated intrinsics/extrinsics, self-supervised
• 3D motion: moving cars present
• Planned public release; synthetic data","• ResNet34 encoder extracts 2D features
• Unproject features to contracted 3D volume
• Pillar aggregation builds groundplan grid
• 2D CNN disentangles static and dynamic
• Volume rendering with coarse/fine, log sampling
• Calibrated multi-view training; LPIPS finetuning","• Beats PixelNeRF/uORF on PSNR, SSIM, LPIPS
• CLEVR ARI and NV-ARI: 0.84
• Better occlusion completion and detail
• Quality improves with more context views
• Single-image inference yields plausible completions
• Trains on single V100; efficient rendering",,,,,https://zotero.org/alehanderoo/items/NI8555XA,,11/10/2025 0:32 (GMT+2)
Yes,OccTransformers,"2D features, 3D features","• Reuse conditioned attention for occupancy decoding head
• Adopt ViT patch encoder for RGB features
• Insert occupancy prior before 4DGS fusion
• Add multi-view conditioning for overlapping cameras
• Combine with separate 3D MOT for tracking
• Evaluate on flying objects datasets","• Predict voxel occupancy from images or point clouds
• Unified transformer for 3D reconstruction tasks
• Avoid specialized encoders and finetuning
• Leverage attention for occupancy learning
• Achieve SOTA Chamfer distance on ShapeNet","• Not multi-view; no overlapping camera fusion
• No pose self-calibration or extrinsics estimation
• Static shapes only; no dynamics or tracking
• Useful occupancy decoder baseline for pipeline
• Could seed per-object 3D occupancy volumes
• Sampling many query points may be costly","• Occupancy networks formulation fθ(p,x) classification
• Transformer encoder-decoder with self-attention layers
• Multi-head conditioned attention mechanism
• Conditional normalization via learned beta and gamma
• Cross-entropy over sampled points as loss","• ShapeNet subset, 13 categories
• 43,783 CAD objects total
• Single random rendered image per object
• No multi-camera overlap; single-view only
• Annotations: meshes, SDF, occupancy labels
• Public dataset; academic research use","• Patchify observations with positional encodings (ViT-style)
• Eight-layer transformer encoder and decoder
• Conditioned attention between latent and query points
• Predict occupancy per 3D query point
• Mesh extraction via MISE adaptive sampling
• Adam 1e-4, batch 32, ~1200-2000 epochs","• SOTA Chamfer on single-image reconstruction
• Competitive IoU and normal consistency overall
• Strong performance on noisy point clouds
• Conditioned attention outperforms standard attention
• Eight layers outperform six in ablations
• Trained on RTX 3090, 64GB RAM",,,,https://ieeexplore.ieee.org/document/9733609/,https://zotero.org/alehanderoo/items/5Z6GIBBL,https://doi.org/10.1109/ICCP53602.2021.9733609,11/10/2025 0:31 (GMT+2)
Yes,NeSF,"3D features, 3D-fusion, classify, pose","• Reuse density-to-semantics on 3DGS/4DGS volumes.
• Use 2D masks to supervise 3D semantic fields.
• Integrate after static/dynamic separation stage.
• Add class heads for flying-object categories.
• Beware pose accuracy; consider self-calibration first.
• Test multi-camera overlap with sparse 2D labels.","• Produce 3D semantic fields from posed RGB images.
• Eliminate dependence on 3D sensors and labels.
• Generalize segmentation to novel, unseen scenes.
• Supervise with 2D semantic maps only.
• Address limitations of single-scene NeRF semantics.","• No dynamics, tracking, or flying-object focus.
• Requires known calibration; not calibration-free.
• Supports multi-view fusion and 3D class probabilities.
• Adaptable to 4DGS for dynamic semantics.
• Resolution limits thin, fast objects; watch floaters.
• Add explainability over 3D UNet features.","• Implicit neural fields for geometry via NeRF density.
• Volumetric rendering for 2D semantic supervision.
• 3D UNet for spatial reasoning on density grids.
• Trilinear interpolation and MLP decoding to logits.
• Cross-entropy with smoothness regularization loss.
• Assume known camera intrinsics and extrinsics.","• Synthetic indoor toy scenes: KLEVR, ToyBox5, ToyBox13.
• #scenes: 100/20, 500/25, 500/25 splits.
• 210 train cameras, 90 test cameras per scene.
• 256x256 frames, 4–12 objects per scene.
• Annotations: 2D semantic maps, depth, camera poses.
• Static scenes; planned public release via Kubric.","• Per-scene NeRF pretraining to recover density fields.
• Sample density into 3D grids; 3D UNet processing.
• MLP decodes tri-linearly interpolated features to classes.
• Render semantic maps via volumetric rendering.
• Known poses; no calibration-free or optical flow.
• Loss: CE + smoothness; z-rotation augmentation.","• Competitive 2D/3D mIoU on synthetic datasets.
• KLEVR: 92.6 2D, 97.5 3D on tests.
• ToyBox13: lags DeepLab by 6.6% 2D mIoU.
• Underperforms SparseConvNet lacking 3D supervision.
• Ablations: rotations and grid resolution crucial.
• Accuracy improves with NeRF reconstruction quality.",,,,http://arxiv.org/abs/2111.13260,https://zotero.org/alehanderoo/items/36JSMWCZ,https://doi.org/10.48550/arXiv.2111.13260,11/10/2025 0:28 (GMT+2)
Yes,PointNet,"3D features, classify, explain","• Use PointNet as 3D point feature encoder.
• Classify 4DGS-reconstructed object point samples.
• Integrate T-Net to handle calibration noise.
• Add temporal modules for tracking; not provided.
• Fuse multi-camera RGB to points before PointNet.","• Directly learn on unordered point clouds, not voxels.
• Achieve classification, segmentation, scene parsing with one architecture.
• Ensure permutation and transformation invariance of point sets.
• Improve efficiency, robustness vs volumetric and multiview methods.","• No multi-camera overlap handling; single point cloud input.
• No pose self-calibration; unrelated to extrinsics.
• No tracking; unsuitable for 3D MOT directly.
• Useful for 3D features and classification heads.
• Critical point visualization aids explainability needs.","• Symmetric max pooling approximates continuous set functions.
• Shared MLP per-point; permutation invariance by design.
• T-Net spatial transformers align inputs and features.
• Orthogonality regularization for feature transform stability.
• Critical points and bottleneck dimension explain robustness.","• ModelNet40: 12,311 shapes, 40 classes.
• ShapeNet Part: 16,881 shapes, 50 parts.
• Stanford semantic parsing: 271 rooms, six areas.
• Single point clouds; no multi-camera overlap.
• Annotations: class, part, semantic labels; licenses N/A.","• Shared MLP per point with batchnorm, ReLU.
• Global max pooling aggregates into 1024-d descriptor.
• T-Net alignment for input and feature spaces.
• Segmentation concatenates global and local features.
• Losses: cross-entropy; transform orthogonality regularization.
• Augmentations: rotation, jitter; efficient O(N) complexity.","• State-of-art 3D point-input classification on ModelNet40.
• ShapeNet part mIoU 83.7%, improved over baselines.
• Stanford scene mIoU 47.7%, accuracy 78.6%.
• Robust under missing points, outliers, perturbations.
• Orders fewer FLOPs, parameters than MVCNN, 3D CNN.",,,,http://arxiv.org/abs/1612.00593,https://zotero.org/alehanderoo/items/WQHTV59N,https://doi.org/10.48550/arXiv.1612.00593,11/10/2025 0:26 (GMT+2)
Yes,4D Spatio-Temporal ConvNets,"3D features, 3D-fusion, classify, pose","• Use MinkowskiEngine for 4D spatiotemporal feature volumes.
• Adopt hybrid kernels along temporal axis.
• Apply TS-CRF for temporal label consistency.
• Fuse 3DGS outputs into sparse 4D tensors.
• Risk: requires depth; adapt to RGB-derived occupancy.
• Quick test: 4D conv over 3DGS dynamic voxels.","• Directly process 3D video with 4D CNNs.
• Overcome inefficiency of dense 3D convolutions.
• Provide generalized sparse convolutions for high dimensions.
• Enforce spatio-temporal consistency via stationary CRF.
• Release open-source sparse tensor autodiff library.","• Strong 3D/4D features; no tracking modules.
• Not calibration-free; assumes known extrinsics.
• No multi-camera fusion from RGB-only.
• Useful for static/dynamic smoothing via CRF.
• No explainability tools provided.","• Sparse tensors in COO coordinate format.
• Generalized sparse convolution over arbitrary kernels.
• Hybrid kernel: cubic spatial, cross temporal.
• Residual MinkowskiNet architecture adaptation.
• 7D trilateral stationary-CRF with mean-field updates.","• ScanNet, S3DIS, RueMonge, Synthia 4D.
• Synthia: 20k train, 815 val, 1886 test.
• Four stereo RGB-D cameras, overlapping views.
• Semantic segmentation labels per voxel/point.
• 3D motion present across time sequences.
• Public datasets; MinkowskiEngine open-source.","• Quantize point clouds to sparse tensors.
• 4D sparse convolutions across space and time.
• Pose: use extrinsics to world-align nodes.
• No 2D backbone or optical flow modules.
• Temporal fusion via 7D TS-CRF layers.
• Cross-entropy loss, Momentum SGD, Poly schedule.","• 67.9% mIoU on ScanNet at submission.
• Up to 72.1% mIoU post-deadline.
• 4D+TS-CRF improves Synthia mIoU to 78.67.
• 4D networks more robust to input noise.
• Batch 4D processing yields runtime benefits.",https://github.com/StanfordVL/MinkowskiEngine,,,https://ieeexplore.ieee.org/document/8953494/,https://zotero.org/alehanderoo/items/L99KPU7F,https://doi.org/10.1109/CVPR.2019.00319,11/10/2025 0:24 (GMT+2)
Yes,Fast and Explicit Neural View Synthesis,"2D features, 3D features, 3D-fusion, pose","• Reuse inverse projection volumetric encoder for 3D features.
• Use RGBA volume as occupancy prior per object.
• Adopt amortized renderer for multi-camera view fusion.
• Extend to 4D volume for dynamic objects.
• Prepend self-calibrated pose estimation using static background.
• Compare explicit voxels vs 4D Gaussian Splatting.","• Fast, explicit novel view synthesis from sparse views.
• Avoid per-scene optimization of implicit radiance fields.
• Enable amortized rendering across target views.
• Learn 3D geometry via self-supervision.","• No classification or tracking components present.
• Assumes known poses; not calibration-free.
• Explicit voxel aligns with 3D occupancy prediction.
• Lacks static/dynamic separation and motion handling.
• Multi-view overlap supported; good for fusion.
• Consider sparse voxels or Gaussians for scalability.","• Explicit RGBA voxel volume representation.
• Inverse projection preserves geometry and texture.
• Relative pose transforms volumes between views.
• Alpha compositing amortized volume rendering.
• Rendering loss: L2 and SSIM.","• ShapeNet objects: chairs, cars, 13 categories.
• 50 views per object at 128x128.
• Single or two source views at inference.
• #cameras: posed multi-view; overlapping field-of-view.
• Annotations: known camera poses, constant intrinsics.
• 3D motion: none; static scenes; non-commercial license.","• 2D U-Net encoder with ResNet-18 backbone.
• Inverse projection into latent 3D volume using intrinsics.
• 3D U-Net predicts RGBA voxel volume.
• Apply relative pose to align volume.
• Amortized alpha compositing volume rendering.
• Train with L2+SSIM; Adam; 150 epochs.","• Matches or beats pixelNeRF on SSIM/LPIPS.
• 245 FPS amortized per-object rendering speed.
• 100x faster per-frame rendering than pixelNeRF.
• Unsupervised 3D mIoU 0.58 on airplanes.
• Generalizes to unseen categories with sharp images.
• Real car transfer: fewer artifacts than pixelNeRF.",,,,http://arxiv.org/abs/2107.05775,https://zotero.org/alehanderoo/items/6GBZX55E,https://doi.org/10.48550/arXiv.2107.05775,11/10/2025 0:19 (GMT+2)
Yes,Neural Fields as Learnable Kernels for 3D Reconstruction,"3D features, 3D-fusion","• Use as occupancy prior for multiview fused points
• Evaluate on per-object dynamic point clouds
• Estimate normals from RGB; test robustness
• Not suitable for classification/3D tracking tasks
• Quick-win: add denoising weights to noisy reconstructions
• Explore sparse/Nyström kernels for scalability","• Reconstruct implicit 3D surfaces from sparse oriented points
• Bridge learned priors with interpolation respecting inputs
• Generalize across categories, scenes, and point densities
• Avoid slow test-time gradient descent optimization
• Robust to noise via weighted kernel regression","• Mismatch: no multi-camera RGB, pose, or motion
• Useful: strong implicit 3D occupancy reconstruction prior
• Denoising weights helpful for multiview noise
• Closed-form solve attractive for per-frame stability
• Consider temporal kernel extension for 4D dynamics
• Investigate normals estimation for flying objects","• Learned RKHS kernel conditioned on input points
• Kernel ridge regression with positive definite Gram matrix
• Surrogate loss using offset samples along normals
• BCE occupancy + L1 surface regularization
• Weighted ridge regression for noise robustness","• ShapeNet objects; 13 categories; 1k points typical
• ScanNet scenes; 10k points per scene
• Cameras/layout/overlap: N/A
• Annotations: occupancy labels, surface points, normals
• 3D motion present: No; static reconstructions
• Availability: Code+pretrained; license N/A","• Pipeline: Feature grid, learned kernel, closed-form solve
• Pose strategy: N/A (no cameras; oriented points)
• 2D backbones/flow: N/A
• 3D fusion: data-dependent kernel aggregating global context
• Heads: classify/track N/A; implicit field only
• Training: BCE occupancy, surface L1, regularization λ","• SoTA IoU/Chamfer on ShapeNet reconstructions
• Strong OOD generalization to unseen categories
• ScanNet scenes: half Chamfer versus baselines
• Robust across varying point densities
• Test-time convex solve; stable minima
• Bottleneck: dense linear solve limits scalability",https://nv-tlabs.github.io/nkf,,,http://arxiv.org/abs/2111.13674,https://zotero.org/alehanderoo/items/HT98NG93,https://doi.org/10.48550/arXiv.2111.13674,11/10/2025 0:17 (GMT+2)
Yes,6DGS,"2D features, 3D features, pose","• Use for calibration-free extrinsics from static background.
• Precompute 3DGS of shared volume.
• Integrate DINOv2 features into 2D pipeline.
• Extend to multi-camera by joint ray selection.
• Consider 4DGS for dynamic objects separation.
• Add optical flow features for temporal robustness.","• 6DoF camera pose from single RGB image.
• Leverage prebuilt 3DGS scene model.
• Avoid iterative NeRF alignment and initialization.
• Closed-form, real-time pose without pose prior.
• Improve accuracy over NVS baselines.","• Strong fit for self-calibrated scene pose.
• No direct support for flying object tracking.
• Requires static scene and prebuilt 3DGS.
• Ellicell attention design transferable to 4DGS.
• Potential bias in translation near object.
• Evaluate multi-camera overlapping consistency.","• Invert 3DGS rendering via radiant Ellicell rays.
• Ray–pixel attention using DINOv2 features.
• Weighted least squares ray intersection for translation.
• Rotation from optical axis inferred by rays bundle.
• L2 loss on attention scores supervision.
• 3DGS rendering equations phi, gamma, tau referenced.","• Domains: indoor/outdoor static scenes.
• Mip-NeRF 360: seven scenes evaluated.
• Tanks&Temples: five objects evaluated.
• Single moving camera; multiview training overlap.
• Annotations: camera poses, images; static scenes.
• Public datasets; standard academic licenses.","• Build 3DGS via SfM and photometric optimization.
• Generate equal-area Ellicell rays per ellipsoid.
• Extract DINOv2 image features; MLP ray embeddings.
• Attention binds rays to pixels; top-N selection.
• Solve camera center by weighted least squares.
• Training: Adafactor, 1.5k iters, 2000 ellipsoids/iter.","• Outperforms iNeRF, Parallel iNeRF, NeMo+VoGE.
• MAE and MTE improvements without pose prior.
• 12% rotation, 22% translation gains reported.
• Near real-time: 15–16 fps on RTX 3090.
• Robust to 3DGS noise and sparse views.
• Optimal N_top around 100 rays.",https://mbortolon97.github.io/6dgs/,,,http://arxiv.org/abs/2407.15484,https://zotero.org/alehanderoo/items/3T2XCG2F,https://doi.org/10.48550/arXiv.2407.15484,11/10/2025 0:16 (GMT+2)
Yes,FeatureGS,"3D features, 3D-fusion","• Adopt geometric loss within 4DGS dynamic modules.
• Use for static background 3DGS to aid separation.
• Tune h_photo for geometry versus PSNR tradeoff.
• Test robustness on flying-object, multi-camera scenes.
• Ablate kNN size and feature choice per scene.","• 3DGS Gaussians misalign with true surfaces.
• Floater artifacts inflate Gaussian count and storage.
• Improve geometric accuracy and reduce artifacts.
• Add eigenvalue-based geometric loss to 3DGS.","• Strong for static multi-view; no dynamics evaluated.
• Useful to reduce floaters before 4D tracking.
• No multi-camera synchronization or calibration-free pose.
• No 2D semantics, flow, classification, tracking heads.
• Integrates well with 4DGS object volume reconstruction.","• Photometric loss: L1 plus D-SSIM.
• Geometric features: planarity, omnivariance, eigenentropy.
• kNN covariance eigenvalues encode neighborhood structure.
• Normalized eigenvalues ensure scale invariance.
• Combined photometric-geometric loss weighting h_photo.","• DTU static tabletop object scenes.
• 49 or 64 RGB images per scene.
• Single moving camera, multi-view overlap.
• Reference point clouds via structured-light scanner.
• No 3D motion; static scenes only.
• Public dataset; academic license.","• Initialize 3DGS from SfM point clouds.
• Use provided camera poses; no self-calibration.
• Add eigenvalue-feature geometric loss during optimization.
• Neighborhood features via kNN=50 on Gaussian centers.
• Train 15k iters; h_photo=0.05; LR per parameter.
• Early-stop at matched PSNR for fair comparison.","• Chamfer improves ~20-30% versus 3DGS.
• Floaters reduced ~90% across scenes.
• Gaussians reduced ~90-95%, big memory savings.
• PSNR drops ~3.3 dB at fixed iterations.
• Same PSNR achievable with fewer Gaussians.
• RTX3090 used; runtime not emphasized.",,,,http://arxiv.org/abs/2501.17655,https://zotero.org/alehanderoo/items/6HWYCEW7,https://doi.org/10.48550/arXiv.2501.17655,11/10/2025 0:14 (GMT+2)
Yes,Feature 3DGS,"2D features, 3D features, 3D-fusion, classify","• Use Feature 3DGS for 3D semantic features.
• Distill CLIP/LSeg for language-driven classification.
• Integrate with 4DGS for dynamic object modeling.
• Add 2D instance masks for object-level splats.
• Replace SfM with self-calibrated multi-view poses.
• Benchmark on flying objects with overlapping cameras.","• Distill 2D foundation features into 3DGS fields.
• Overcome NeRF slow training and inference speeds.
• Address feature resolution and channel mismatch issues.
• Enable promptable 3D segmentation and editing.
• Improve segmentation accuracy and rendering speed.","• Strong fit for 3D features; lacks tracking.
• Static-scene assumption mismatches dynamic flying targets.
• Multi-view overlap aligns with thesis cameras.
• Speed-ups compatible with real-time pipelines.
• No optical flow; consider temporal feature integration.
• No explainability; add 3D Grad-CAM equivalents.","• Explicit 3D Gaussians with semantic feature vectors.
• Parallel N-dimensional Gaussian rasterizer, tile-based.
• Alpha blending with transmittance front-to-back.
• Teacher-student distillation from SAM/LSeg features.
• Joint loss: L_rgb plus gamma L_f.
• Softmax prompt scoring via cosine similarity.","• Datasets: Replica, LLFF scenes.
• Multi-view posed RGB images via SfM.
• Annotation: distilled features; no manual masks required.
• Static scenes; no 3D motion present.
• Evaluation on novel-view semantic segmentation.
• Availability/license details: N/A.","• Initialize Gaussians from SfM sparse points.
• Poses from SfM; not calibration-free.
• Parallel rasterization for RGB and feature maps.
• 1x1 conv decoder for channel expansion.
• Distill SAM/LSeg features with per-pixel supervision.
• Joint L1+DSSIM RGB loss and feature L1.","• mIoU 0.782 vs 0.636 NeRF-DFF.
• Accuracy 0.943 vs 0.864 NeRF-DFF.
• FPS 14.55 with speed-up module.
• Up to 2.7x faster training and rendering.
• PSNR 37.0 vs 36.1 base 3DGS.
• SAM inference up to 1.7x faster.",,,,https://ieeexplore.ieee.org/document/10656650/,https://zotero.org/alehanderoo/items/3EAHIVWV,https://doi.org/10.1109/CVPR52733.2024.02048,11/10/2025 0:09 (GMT+2)
Yes,3D Gaussian Splatting for Real-Time Radiance Field Rendering,"3D features, 3D-fusion, pose","• Use 3DGS for static background reconstruction.
• Swap 4DGS for dynamic object volumes.
• Integrate renderer for fast multi-view feature fusion.
• Retain SH scheduling; apply per-object appearance codes.
• Calibrate via SfM; later add self-calibration module.
• Evaluate optical-flow features before 4DGS densification.","• Real-time 1080p radiance fields without quality loss.
• Avoid slow volumetric ray-marching sampling overhead.
• Represent unbounded scenes with efficient explicit primitives.
• Reduce training time versus SOTA NeRFs.","• Strong for static background; not dynamic tracking.
• Lacks self-calibration; uses SfM poses.
• No 2D features or optical flow used.
• No classification, tracking, or explainability outputs.
• Densification and anisotropy transfer well to 4DGS.
• Investigate multi-camera online updates and occlusion handling.","• Anisotropic 3D Gaussians as volumetric primitives.
• Alpha-blending equivalent to volumetric rendering equations.
• SH coefficients model view-dependent color.
• Covariance parameterized by scale and quaternion.
• L1 and D-SSIM loss; opacity sigmoid constraints.
• Tile-based sorting approximates visibility-ordered blending.","• Static multi-view captures; indoor/outdoor real scenes.
• 13 real scenes; Blender synthetic dataset.
• Cameras from SfM; overlapping multi-photo captures.
• No annotations; self-supervised via photometric loss.
• No 3D motion; static-only training/test.
• Code/data available; license unspecified.","• Initialize Gaussians from SfM sparse points.
• Optimize positions, opacity, anisotropic covariance, SH.
• Calibration: requires SfM poses; not calibration-free.
• Densification via clone/split using gradient thresholds.
• Differentiable tile-based rasterizer with radix sort.
• Training: L1 and D-SSIM; multires warmup; SH scheduling.","• Matches Mip-NeRF360 quality with 35–45 min training.
• 1080p rendering: 134–197 FPS on A6000.
• Outperforms fast baselines in quality after longer training.
• Ablations: anisotropy, unlimited gradients crucial.
• Strong PSNR/SSIM across datasets; comparable or better.
• Memory higher than NeRF; hundreds MB per scene.",https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/,,,http://arxiv.org/abs/2308.04079,https://zotero.org/alehanderoo/items/SK344C2N,https://doi.org/10.48550/arXiv.2308.04079,11/10/2025 0:03 (GMT+2)
Yes,GaussianPro,"2D features, 3D features, 3D-fusion","• Use propagation for static background 3DGS stage.
• Integrate planar loss for background surface regularization.
• Mask moving objects during propagation using flow.
• Test on overlapping cameras with texture-less backgrounds.
• Assess robustness under self-calibration pose noise.
• Explore 4DGS extension for dynamic flying objects.","• 3DGS fails in texture-less regions.
• Densification ignores geometry priors.
• SfM initialization insufficient for large scenes.
• Need guided densification using multi-view consistency.","• Strong for static multi-view; not dynamics.
• Improves background geometry for static/dynamic separation.
• Relies on accurate poses; not calibration-free.
• Patch matching likely fails on moving, flying objects.
• Planar bias may hurt curved, deformable targets.
• Needs adaptation for online, multi-object 3D tracking.","• Progressive plane propagation in 2D and 3D.
• Photometric consistency via NCC and homographies.
• Planar constraint: L_normal and L_scale.
• Alpha-blended depth/normal rendering from Gaussians.
• Total loss: L1, D-SSIM, plus planar.","• Waymo urban street scenes; nine scenes.
• MipNeRF360 indoor/outdoor small-scale scenes.
• Posed multi-view RGB images; overlapping views.
• No annotations; self-supervised reconstruction.
• Assumes static scenes; dynamics cause artifacts.
• Public datasets; code built on 3DGS.","• Render depth/normal maps via alpha-blending.
• Propagate plane candidates using checkerboard neighbors.
• Homography-based patch matching with NCC.
• Geometric consistency filtering across views.
• Back-project selected pixels to initialize Gaussians.
• Planar loss and scale regularization; periodic propagation.","• +1.15 dB PSNR on Waymo over 3DGS.
• SSIM +0.011; LPIPS −0.035 on Waymo.
• Comparable FPS to 3DGS; 108 vs 103.
• Robust under fewer training views.
• Ablations: propagation and planar both contribute.
• Fewer noisy Gaussians; more compact representation.",,,,http://arxiv.org/abs/2402.14650,https://zotero.org/alehanderoo/items/ETAPPQMW,https://doi.org/10.48550/arXiv.2402.14650,09/10/2025 21:43 (GMT+2)
Yes,WildGS-SLAM,"2D features, 3D-fusion, pose","• Reuse uncertainty-weighted dynamic suppression module.
• Replace DBA with multi-view BA for overlapping cameras.
• Integrate 3DGS for static map initialization.
• Add 4DGS for dynamic object volumetric modeling.
• Use Metric3D V2 for depth priors.
• Quick-win: DINOv2 features + online uncertainty masks.","• Robust monocular SLAM in dynamic environments.
• Remove moving distractors without semantics or depth.
• Improve tracking, mapping, and rendering under motion.
• Address generalization across unpredictable motion patterns.","• Strong for static/dynamic separation; monocular-only pose.
• Lacks multi-camera fusion; adapt BA accordingly.
• No object classification or 3D object tracking.
• Useful prefilter before 4DGS reconstruction.
• Potentially helpful for flying distractor suppression.
• Verify robustness with aerial, fast-moving targets.","• 3D Gaussian Splatting as dense scene representation.
• Uncertainty prediction from DINOv2 features via MLP.
• Uncertainty-weighted DBA for camera pose optimization.
• Metric depth regularization using Metric3D V2.
• Uncertainty-weighted rendering and isotropic regularization.
• Losses: modified SSIM, L1 depth, regularizers.","• Domains: indoor/outdoor dynamic scenes; phones, RealSense.
• Wild-SLAM MoCap: 10 RGB-D sequences.
• Wild-SLAM iPhone: 7 RGB sequences.
• Monocular camera; single view; no multi-camera overlap.
• Ground-truth trajectories via OptiTrack MoCap.
• Datasets released; license unspecified.","• Input monocular RGB; extract DINOv2 3D-aware features.
• Train shallow MLP online for per-pixel uncertainty.
• Optical-flow DBA from DROID-SLAM with uncertainty weights.
• Depth regularization using Metric3D V2 predictions.
• 3DGS mapping with uncertainty-weighted render loss.
• Loop closure and global BA for drift reduction.","• Wild-SLAM MoCap ATE 0.46 cm average.
• Bonn Dynamic ATE 2.31 cm, best overall.
• TUM dynamic ATE 1.63 cm, state-of-the-art.
• PSNR 20.59 vs Splat-SLAM 17.23 average.
• Fast mode 1.96 fps with minimal accuracy loss.
• Ablations validate uncertainty and depth regularization.",https://wildgs-slam.github.io,,,http://arxiv.org/abs/2504.03886,https://zotero.org/alehanderoo/items/PGTTYSEG,https://doi.org/10.48550/arXiv.2504.03886,09/10/2025 21:41 (GMT+2)
Yes,ShapeSplat,"3D features, classify","• Use Gaussian-MAE to pretrain GS feature encoder.
• Prefer E(C,S,R,O) with splats pooling.
• Static CAD domain may not transfer to flying.
• Collect multi-camera real GS for domain adaptation.
• Quick win: centroid-only features for segmentation.
• Explore 4DGS extension for motion-aware tokens.","• Lack large 3DGS dataset for learning.
• Enable SSL directly on Gaussian parameters.
• Bridge 3DGS to classification and segmentation tasks.
• Analyze parameter distributions affecting downstream performance.","• Strong fit for GS feature learning.
• Mismatch: no tracking, no pose self-calibration.
• No multi-camera overlap assumptions evaluated.
• Useful for classification module pretraining.
• Consider integrating 2D foundation features later.
• Lacks explainability tools like Grad-CAM.","• 3DGS primitives: C,O,S,R,SH parameterization.
• Masked autoencoder pretraining on Gaussian tokens.
• Grouping feature G and embedding feature E.
• Chamfer loss for centroids, L1 for others.
• Temperature-scaled splats pooling for aggregation.
• Alpha blending rendering equations referenced.","• CAD objects: ShapeNet, ModelNet, Objaverse subsets.
• 206K objects, 87 categories overall.
• 72 views per object, upper hemisphere layout.
• Static scenes; no 3D motion present.
• Annotations: class labels, part segmentation.
• Availability: dataset/code promised; license N/A.","• Render 400x400 images from 72 poses.
• Train 3DGS per object with pruning, regularization.
• Downsample splats; tokenize via grouping and pooling.
• Calibration uses known synthetic poses; not self-calibrated.
• Encoder-decoder ViT; classification/segmentation heads.
• Losses: Chamfer, L1; 300 epochs AdamW.","• Additional parameters improve classification over centroids.
• Centroids alone benefit segmentation mIoU by 0.4%.
• Slight accuracy gains over point-cloud baselines.
• Grouping and splats pooling yield up to 1.1% gains.
• Pretraining generalizes to point clouds reasonably.
• Best mask ratios around 0.4–0.6 for downstream.",,,,http://arxiv.org/abs/2408.10906,https://zotero.org/alehanderoo/items/KEET7HEQ,https://doi.org/10.48550/arXiv.2408.10906,09/10/2025 21:39 (GMT+2)
Yes,pixelNeRF,"2D features, 3D features, 3D-fusion, pose","• Reuse pixel-aligned feature conditioning module.
• Adopt view-space fusion for multi-camera overlap.
• Pretrain encoder on target domain RGB.
• Integrate features into 4DGS volume.
• Benchmark with calibration-free pose estimates.
• Beware static-scene assumption; no dynamics.","• Few-shot novel view synthesis from sparse inputs.
• Avoid per-scene NeRF optimization.
• Learn scene prior across scenes.
• Use only 2D multiview supervision.
• Generalize to unseen categories and scenes.","• Good for multi-view static fusion; not for dynamics.
• Lacks tracking, classification, explainability modules.
• Requires known poses; conflicts with self-calibration goal.
• View-space design applicable to 4DGS objects.
• Feature modulation could guide 3D occupancy priors.
• Investigate combining with optical flow for motion.","• NeRF volume rendering integral and L2 loss.
• Pixel-aligned conditioning via projected features.
• View-space representation, not canonical space.
• Positional encoding for 3D coordinates.
• Multi-view average pooling within MLP.","• ShapeNet chairs, cars; Kato 13 categories.
• 6591 chairs; 3514 cars; 24 views.
• DTU: 88 train scenes, 15 test scenes.
• Posed multi-view cameras; wide baselines; overlap.
• Supervision: 2D images; no masks; no 3D.
• Static scenes; no dynamic motion.","• ResNet34 encoder for pixel-aligned feature grids.
• Project 3D points; bilinear sample per-view features.
• NeRF MLP with residual feature modulation.
• Aggregate views via average pooling in network.
• Requires known intrinsics and relative poses.
• Hierarchical sampling; L2 RGB loss; feed-forward.","• Outperforms SRN, DVR on PSNR/SSIM/LPIPS.
• Strong single/few-view synthesis from one image.
• Generalizes to unseen categories, multi-object scenes.
• DTU: plausible views from three inputs.
• No test-time optimization; faster than NeRF training.
• Rendering still slow; runtime increases with views.",https://github.com/sxyu/pixel-nerf,,,http://arxiv.org/abs/2012.02190,https://zotero.org/alehanderoo/items/CXCWUVKW,https://doi.org/10.48550/arXiv.2012.02190,09/10/2025 21:38 (GMT+2)
Yes,4D-Rotor Gaussian Splatting,"3D features, 3D-fusion","• Use 4DGS for dynamic 3D feature volumes.
• Derive motion priors for 3D tracking.
• Apply entropy loss to suppress floaters.
• Add 4D consistency for smoother trajectories.
• Risk: requires accurate multi-view poses.
• Quick-win: test on multi-camera flying objects.","• Efficient dynamic NVS with fine details.
• Handle abrupt motion, appearance, disappearance.
• Achieve real-time rendering for 4D scenes.
• Overcome spatial-temporal entanglement limitations.","• Strong dynamic modeling; no classification or tracking heads.
• Supports multi-view overlap; suitable for central volume.
• Not calibration-free; depends on COLMAP/posed cameras.
• No explicit static/dynamic separation module.
• Lacks explainability tools (Grad-CAM, TREx).
• Promising 4D base for flying object tracking.","• Rotor-based 4D rotation representation.
• Temporal slicing 4D→3D Gaussian formulation.
• Alpha compositing as in 3DGS.
• Entropy loss to binarize opacities.
• 4D KNN motion consistency loss.
• Total loss: L1+SSIM+entropy+consistency.","• Plenoptic Video: 6 real scenes, multi-view videos.
• 17–20 GoPro views, overlapping central view.
• D-NeRF: 8 synthetic monocular sequences.
• Resolutions: 1352×1014 and 400×400.
• Annotations: RGB only, no labels.
• Strong 3D motion present; code available.","• Initialize 100k 4D Gaussians, COLMAP spatial prior.
• Rotor parameters model spatio-temporal transformations.
• Temporally slice to 3D, splat via rasterization.
• CUDA-accelerated forward/backward, densify/prune Gaussians.
• Losses: L1, SSIM, entropy, 4D consistency.
• Cameras assumed posed; no optical flow inputs.","• 277 FPS at 1352×1014 on RTX 3090.
• 583 FPS on RTX 4090.
• PSNR 31.62 on Plenoptic; best reported.
• PSNR 34.26 on D-NeRF; very fast.
• Entropy reduces floaters; consistency smooths motions.
• Speed field emerges from slicing; optical flow visualized.",https://github.com/weify627/4D-RotorGaussians,,,http://arxiv.org/abs/2402.03307,https://zotero.org/alehanderoo/items/LWRNDDEQ,https://doi.org/10.48550/arXiv.2402.03307,09/10/2025 21:35 (GMT+2)
Yes,Embracing Dynamics,"2D features, 3D features, 3D-fusion, pose, track","• Adopt 4DGS for dynamic/static separation and fusion.
• Use LEAP for dynamics-aware point selection.
• Replace depth with multi-view RGB triangulation.
• Extend to multi-camera overlapping volume fusion.
• Integrate per-object 4DGS for classifying movers.
• Beware compute load; compress Gaussians aggressively.","• Robust SLAM in dynamic scenes with 3DGS.
• Static 3DGS fails under motion, causing drift.
• NeRF SLAM too slow for real-time dynamic mapping.
• Propose 4DGS-based SLAM embracing dynamics directly.
• Reduce pose drift and artifacts without filtering movers.","• Strong for dynamic mapping; weak for object-level tracking.
• Single camera; no multi-view overlap support.
• Relies on depth; mismatched with RGB-only goal.
• 4DGS insights helpful for 3D occupancy dynamics.
• Consider per-object segmentation before 4DGS.
• Open: performance with flying small targets.","• 4D Gaussian representation with spatio-temporal covariance.
• Conditional 3D spatial, marginal temporal decomposition.
• Differentiable 4DGS rendering equation with 4DSH colors.
• Photometric and geometric residuals for pose optimization.
• Dynamics-aware isotropic regularization for Gaussians.
• LEAP provides visibility, dynamics, reliability priors.","• Datasets: BONN dynamic, TartanAir-Shibuya.
• #sequences/frames: N/A.
• #cameras/layout/overlap: single RGB-D, egocentric, N/A overlap.
• Annotation type: N/A.
• 3D motion present: balls, hands, pedestrians.
• Availability/license: datasets public; license N/A.","• Anchors via gradients; LEAP for long-term point tracking.
• Filter by visibility, dynamics, reliability quantiles.
• Pose: RGB-D optimization; photometric+geometric losses.
• Mapping: 4DGS with dynamics-aware isotropic regularization.
• Keyframing, densification, pruning; temporal covisibility.
• Not calibration-free; assumes RGB-D; 0.9–1.5 FPS.","• Best ATE on BONN: 5.1 cm average.
• Beats DG-SLAM by 7.3% ATE.
• TartanAir: 95% lower ATE than SplaTAM.
• Higher PSNR/SSIM; LPIPS significantly reduced.
• Ablation: InfoModule and 4DGS both critical.
• Real-time-ish speeds; heavy GPU required.",,,,http://arxiv.org/abs/2504.04844,https://zotero.org/alehanderoo/items/SH2JS2IG,https://doi.org/10.48550/arXiv.2504.04844,09/10/2025 21:20 (GMT+2)
Yes,E-4DGS,"2D features, 3D features, 3D-fusion, pose","• Adopt deformable 4DGS for dynamic volume representation.
• Port intensity-importance pruning to RGB 4DGS.
• Mimic adaptive slicing using flow-based windowing.
• Exploit multi-view consistency regularization.
• Risk: requires accurate poses; not calibration-free.
• Quick-win: replace events with optical-flow pseudo-events.","• Dynamic 4D reconstruction from multi-view event streams.
• Handle fast motion, HDR, and motion blur.
• Avoid E2VID+SfM temporal precision loss.
• First event-driven dynamic 3DGS for multi-view.
• Stabilize training under sparse event supervision.","• Strong fit for multi-view dynamic 3DGS fusion.
• Mismatch: event sensors; thesis uses RGB-only.
• No object classification or 3D tracking modules.
• Pose assumed known; add self-calibration component.
• Consider object-level segmentation atop 4DGS.
• Test on flying drones/birds with overlapping cameras.","• Deformable3DGS canonical/deformed Gaussian representation.
• Event generation model linking log-intensity and events.
• Event-based single integral photometric supervision.
• Adaptive contrast threshold as learnable parameter.
• Multi-view 3D consistency regularization.
• Intensity importance pruning for floater removal.","• Synthetic: eight dynamic scenes, Blender v2e events.
• Six moving event cameras, 360-degree around object.
• Resolution 346×260; 3000 FPS simulation.
• Real: DSEC sequences; single forward event camera.
• No semantic labels; NVS evaluation only.
• Models CC-BY; code/dataset promised.","• Event-based Gaussian initialization strategies.
• Event-adaptive slicing and accumulation with noise.
• Deformable3DGS backbone with multi-view consistency.
• Intensity importance pruning; disable opacity reset.
• Assumes accurate intrinsics/extrinsics; SLERP/LERP interpolation.
• 50k iters, Adam, RTX4090; L1+D-SSIM+TV losses.","• Outperforms baselines on PSNR/SSIM/LPIPS across scenes.
• Event-only beats E2VID+D3DGS consistently.
• Event+RGB achieves best overall metrics.
• ESS and IIP significantly boost performance.
• Robust across mild/strong motion blur levels.
• Qualitative gains on DSEC; fewer artifacts.",,,,http://arxiv.org/abs/2508.09912,https://zotero.org/alehanderoo/items/XFB96DWJ,https://doi.org/10.48550/arXiv.2508.09912,09/10/2025 21:16 (GMT+2)
Yes,4D Gaussian Splatting SLAM,"2D features, 3D features, 3D-fusion, pose","• Reuse static/dynamic separation for 4DGS objects.
• Adopt optical-flow rendering to supervise motion.
• Port control-point MLP deformations to objects.
• Replace depth with multi-view RGB self-calibration.
• Train with overlapping cameras; enforce cross-view consistency.
• Beware YOLO masks and RAFT biases.","• SLAM in dynamic scenes with 4D Gaussian splats.
• Avoid discarding dynamics during mapping and tracking.
• Bridge 2D images and 4D radiance fields.
• Improve pose and reconstruction in high dynamics.","• Strong dynamic mapping; lacks object-level tracking.
• Uses RGB-D; thesis requires RGB-only.
• Optical-flow supervision aligns with temporal module.
• Control-point deformations suit flying-object nonrigidity.
• Extend to multi-camera overlapping volume.
• Need classification and 3D MOT additions.","• Alpha compositing for color, depth, opacity rendering.
• Dynamic/static Gaussian separation with dy attribute.
• Control-point MLP predicting time-varying 6-DoF transforms.
• Linear Blend Skinning with RBF interpolation weights.
• Losses: photometric, depth, optical-flow, ARAP, isotropy.
• Keyframe window optimization and two-stage mapping.","• Indoor RGB-D sequences: TUM, BONN dynamic.
• #sequences not stated; multiple named sequences.
• Single handheld RGB-D camera; no multi-camera overlap.
• Annotations: ground-truth trajectories; no object labels.
• Dynamic motion present: humans, balloons, boxes.
• Availability: public datasets; code on GitHub.","• Initialize static/dynamic Gaussians via motion masks.
• Track camera using static Gaussians only.
• Optical flow rendered from dynamic Gaussians.
• Supervise with RAFT flows within motion regions.
• Control points + MLP + LBS for deformations.
• Two-stage mapping and global color refinement.","• ATE improves: 3.6cm BONN average.
• ATE improves: 1.8cm TUM average.
• PSNR/SSIM/LPIPS outperform GS-SLAM baselines.
• Ablations: flow loss and separation both critical.
• Robust pose under highly dynamic scenes.
• Runtime and memory not reported.",https://github.com/yanyan-li/4DGS-SLAM,,,http://arxiv.org/abs/2503.16710,https://zotero.org/alehanderoo/items/JPGA6AQG,https://doi.org/10.48550/arXiv.2503.16710,09/10/2025 21:14 (GMT+2)
Yes,GIFStream: 4D Gaussian-based Immersive Video with Feature Stream (2025),"3D features, 3D-fusion","• Leverage feature streams for fast flying object motion
• Adopt densification strategy for transient movers
• Integrate per-object 4DGS after 2D segmentation
• Replace COLMAP with self-calibrated multi-view poses
• Evaluate compression impact on tracking stability
• Prototype drones dataset with overlapping cameras","• Efficient 4DGS for immersive video compression
• Balance quality and storage for dynamic scenes
• Model fast motion with manageable memory
• Provide real-time rendering and decoding","• Strong for multi-view dynamic reconstruction, not tracking
• No classification or explainability components
• Not calibration-free; relies on COLMAP initialization
• Useful 4DGS base for per-object volumes
• Compression techniques may aid real-time inference
• Need integration with 3D MOT and classifiers","• Canonical 3D space with deformation field
• Time-dependent feature streams per anchor
• Motion-adaptive sparsity via M_de regularization
• KNN-aggregated motion with M_knn blending
• SE3 motion predicted via quaternion parameterization
• Quantization-aware training with entropy regularization","• Dynamic multi-view indoor and sports scenes
• Neur3D: 6 sequences, 18–21 cameras
• Panoptic Sports: 2 sequences, 31 cameras
• MPEG: 2 sequences, 65 frames, ~1080p
• Annotations: images only, no semantics
• Fast 3D motion present across scenes","• Scaffold-GS anchors with time-independent and feature streams
• Two heads: attributes and SE3 motion
• KNN neighbor aggregation for local motion smoothing
• COLMAP initialization, GOP-wise training
• 3D-to-2D sorting, video-style compression
• Losses: photo, entropy, temporal, smoothness, mask","• Best RD performance versus 4DGS and deformation baselines
• High quality at 30 Mbps 1080p
• Real-time rendering, 60–100 FPS on RTX 4090
• Smallest storage across datasets
• Ablations validate feature streams and densification
• Fast decoding with sparse streams and tiny networks",https://xdimlab.github.io/GIFStream,,,http://arxiv.org/abs/2505.07539,,https://doi.org/10.48550/arXiv.2505.07539,09/10/2025 21:09 (GMT+2)
Yes,PolarFormer,"2D features, 3D features, 3D-fusion, classify, pose","• Reuse cross-plane encoder for 2D-to-3D features.
• Adapt polar alignment for multi-camera overlap.
• Replace detection head with 3D occupancy estimator.
• Integrate temporal fusion with optical flow cues.
• Add self-calibration to estimate extrinsics online.
• Validate on flying objects, large altitude ranges.","• Camera-only 3D detection accuracy limitations.
• Cartesian BEV mismatches camera imaging geometry.
• Advocate polar coordinates for BEV detection.
• Address radial scale variation in polar space.","• Strong multi-camera fusion aligns with thesis.
• Assumes known calibration; conflicts calibration-free goal.
• Ground-centric; limited for flying object volumes.
• No Gaussian Splatting; consider 4DGS reconstruction.
• Lacks tracking; attach 3D MOT and priors.","• Polar BEV aligns perspective wedge geometry.
• Cross attention learns column to ray depth.
• Multi-scale polar representation handles scale variation.
• Losses: Focal classification, L1 regression.
• Deformable attention aggregates multi-scale BEV features.","• Autonomous driving, nuScenes benchmark.
• 1000 scenes, 2 Hz annotations.
• Six cameras, 360° overlapping coverage.
• Image resolution 1600x900.
• 3D boxes with velocities provided.
• Public dataset, noncommercial license.","• Cross-plane encoder maps image columns to polar rays.
• Polar alignment fuses rays across cameras.
• Multi-scale BEV encoder using deformable attention.
• Polar decoder predicts boxes, orientation, velocities.
• Known intrinsics and extrinsics; no self-calibration.
• Optional temporal BEV fusion with one sweep.","• Prototype R101: 41.5 mAP, 47.0 NDS.
• Improved V2-99: 45.5 mAP, 50.3 NDS.
• Temporal PolarFormer-T: 49.3 mAP, 57.2 NDS.
• Polar surpasses Cartesian in ablations consistently.
• Better near and medium range performance.",,,,https://ojs.aaai.org/index.php/AAAI/article/view/25185,https://zotero.org/alehanderoo/items/I7LYCWRM,https://doi.org/10.1609/aaai.v37i1.25185,09/10/2025 21:05 (GMT+2)
Yes,COTR,"2D features, 3D features, 3D-fusion, classify","• Reuse compact OCC encoder for 3D feature volume
• Use group decoder for rare flying-object categories
• Replace LiDAR depth with self-supervised multi-view
• Add self-calibration for camera extrinsics/ego-motion
• Evaluate on multi-view flying-object datasets
• Integrate 4D temporal cues for tracking","• Efficient 3D occupancy from multi-camera RGB
• Preserve 3D geometry versus BEV/TPV compression
• Reduce OCC sparsity computational cost
• Improve semantic discriminability under class imbalance
• Deliver compact OCC with similar performance","• Useful for RGB multi-camera 3D occupancy fusion
• Lacks flying-object focus and aerial datasets
• Assumes calibrated cameras; no self-calibration
• No Gaussian Splatting or 4D separation
• No 3D multi-object tracking outputs
• Extendable to temporal, airborne scenarios","• Explicit-implicit view transformation for OCC features
• Compact OCC via downsample-UNet-upsample
• Transformer decoder with 3D deformable attention
• Mask classification for occupancy semantics
• 3D-SCA deformable attention, Eq.(1)
• Sigmoid mask prediction, Eq.(2)","• Autonomous driving, Occ3D-nuScenes domain
• 700 train scenes, 150 val scenes
• Six surround cameras; overlapping FoV typical
• Dense voxel-wise semantic occupancy annotations
• Dynamic traffic present; optional temporal frames
• Public dataset; nuScenes license","• Multi-camera image featurizer, depth net (BEVStereo)
• Known intrinsics/extrinsics; no self-calibration
• ResNet-50, Swin-B backbones; varied image sizes
• EVT voxel-pooling to OCC; IVT 3D SCA
• U-Net downsample/upsample compact OCC features
• MaskFormer-style decoder; grouped queries; AdamW, 24 epochs","• +1.2–5.5 IoU over baselines
• +3.8–5.2 mIoU over baselines
• BEVDet4D+COTR mIoU 44.5% at 256×704
• Compact OCC ~5× cheaper than raw OCC
• Modules cumulatively add +5.38 mIoU
• Rare classes significantly improved recognition",https://github.com/NotACracker/COTR,,,https://ieeexplore.ieee.org/document/10656319/,https://zotero.org/alehanderoo/items/EN87JAMK,https://doi.org/10.1109/CVPR52733.2024.01884,09/10/2025 19:55 (GMT+2)
Yes,SurroundOcc,"2D features, 3D features, 3D-fusion, classify, pose","• Adopt 2D-3D attention for volume lifting.
• Leverage multi-scale 3D U-Net decoder.
• Adapt label densification for RGB-only pseudo-GT.
• Replace calibrated poses with self-calibrated estimates.
• Augment with optical flow for motion cues.
• Test robustness to pose noise, flying targets.","• Predict dense 3D occupancy from multi-camera RGB.
• Avoid sparse LiDAR supervision limitations.
• Preserve 3D info beyond BEV projection.
• Generate dense occupancy labels without manual annotation.
• Handle occlusions with volumetric representation.","• Strong for multi-camera occupancy; no tracking.
• Not tailored for flying objects or aerial volumes.
• Requires known extrinsics; mismatches calibration-free goal.
• Static/dynamic separation pipeline informative.
• Could seed occupancy flow for 4DGS integration.
• Evaluate performance with overlapping camera subset.","• 2D-3D deformable cross-view attention mechanism.
• Volumetric occupancy as probabilistic voxel grid.
• 3D U-Net with multi-scale deconvolution.
• Cross-entropy and scene-class affinity losses.
• Decayed multi-scale supervision weighting.","• Autonomous driving street scenes.
• nuScenes validation; SemanticKITTI monocular benchmark.
• Six surround cameras; overlapping FOVs.
• Annotations: 3D detection, 3D semantic segmentation.
• Dynamic objects present; moving vehicles/pedestrians.
• Code and generated labels available; license N/A.","• Extract multi-scale 2D features via ResNet101-DCN+FPN.
• Project 3D queries to views; deformable attention.
• Fuse to 3D volume; 3D conv encoder-decoder.
• Dense GT via stitching, Poisson reconstruction, NN semantics.
• Supervise multi-scale with cross-entropy, affinity loss, decay.
• Uses calibrated intrinsics/extrinsics; no self-calibration.","• State-of-art SC IoU 31.49 on nuScenes.
• SSC mIoU 20.3; surpasses BEVFormer, TPVFormer*.
• 3D attention > BEV attention in ablations.
• Dense labels outperform sparse supervision significantly.
• 3D reconstruction F-score 0.483; CD 1.95.
• Latency 0.34s; memory 5.9GB on RTX3090.",https://github.com/weiyithu/SurroundOcc,,,http://arxiv.org/abs/2303.09551,https://zotero.org/alehanderoo/items/JHI3AHSY,https://doi.org/10.48550/arXiv.2303.09551,09/10/2025 19:53 (GMT+2)
Yes,VoxFormer,"2D features, 3D features, 3D-fusion, classify","• Reuse depth-based voxel query proposal.
• Adopt voxel-to-image deformable cross-attention.
• Add temporal frames and optical flow.
• Extend to multi-camera overlapping setup.
• Separate static/dynamic before 3D completion.
• Use outputs to seed 4DGS volumes.","• Camera-only 3D semantic scene completion (SSC).
• Avoid false features from 2D-to-3D projection.
• Exploit reconstruction-first and spatial sparsity.
• Reduce gap to LiDAR SSC performance.","• Good for 3D occupancy; no tracking.
• Requires calibrated poses; no self-calibration.
• Single-view; lacks multi-camera overlap.
• Not focused on flying objects.
• No Gaussian splatting integration.
• Useful 3D feature learning insights.","• Reconstruction-before-hallucination principle.
• Sparsity-in-3D-space assumption.
• Voxel-to-image deformable cross-attention.
• MAE-like sparse-to-dense completion.
• Weighted cross-entropy with class weights.
• Scene-class affinity auxiliary loss.","• SemanticKITTI outdoor driving scenes.
• 22 odometry sequences; many frames.
• Monocular cam2; stereo depth for queries.
• Annotations: 20-class semantic voxel grids.
• 3D motion present: vehicles, pedestrians.
• Availability: public benchmark; license N/A.","• Stage-1: depth-based voxel query proposal.
• Pose: known intrinsics/extrinsics; not self-calibrated.
• ResNet-50+FPN 2D features; optional temporal.
• Deformable cross-attention, then self-attention.
• Head: per-voxel semantic segmentation.
• Losses: CE, class weights, scene affinity.","• Beats MonoScene on IoU and mIoU.
• Large short-range mIoU gains at 12.8m.
• Improved small objects: poles, trunks, signs.
• Near LiDAR performance at close range.
• Temporal input further boosts mIoU.
• ∼60M params; <16GB training memory.",https://github.com/NVlabs/VoxFormer,,,https://ieeexplore.ieee.org/document/10203337/,https://zotero.org/alehanderoo/items/WX8AWXTA,https://doi.org/10.1109/CVPR52729.2023.00877,09/10/2025 19:49 (GMT+2)
Yes,SDGOCC,"2D features, 3D features, 3D-fusion, classify","• Adapt semantic-guided view transform without LiDAR.
• Replace LiDAR depth with MVS or self-supervised depth.
• Use semantic masks from 2D segmenter for seed reduction.
• Integrate BEV fusion module for multi-camera RGB features.
• Evaluate on flying-object volumes; avoid ground-plane BEV biases.
• Add optical flow for temporal cues, moving targets.","• Camera-only depth inaccurate for occupancy.
• LiDAR-only sparse and occlusion-prone.
• LSS BEV space underutilized, inefficient.
• Fuse semantics and LiDAR depths for transformation.
• Distill multimodal knowledge into image features.","• Strong for multi-camera overlap; not camera-only.
• Ground-centric BEV unsuitable for aerial 3D volumes.
• No pose self-calibration; extrinsics assumed known.
• No tracking; only voxel semantics.
• Concepts transferable: semantic-guided lifting, AR/IR distillation.
• Consider 4DGS feature volume instead of BEV.","• Semantic-guided depth diffusion within class masks.
• Bilinear incremental discretization for virtual point seeds.
• Neighborhood attention fusion with gated weighting.
• Occupancy-driven active distillation, AR/IR weighting.
• Channel-to-height occupancy head for 3D logits.","• Domain: autonomous driving, urban road scenes.
• Dataset: Occ3D-nuScenes, SurroundOcc-nuScenes.
• Cameras: six surround-view, overlapping FOVs.
• Annotations: voxel semantic occupancy labels.
• 3D motion present?: N/A.
• Availability: code promised; datasets public.","• ResNet50+FPN multi-view encoder; DepthNet semantics+depth.
• LiDAR depth projected; semantic diffusion builds semi-dense depth.
• Bilinear discretization; BEV pooling produces image BEV.
• SPVCNN LiDAR branch; BEV features extracted.
• Neighborhood attention fusion with gated weights; occupancy head.
• AdamW 1e-4; uses provided extrinsics; no flow.","• Occ3D mIoU 51.66 (SDG-Fusion), real-time 133ms.
• SDG-KL faster 83ms, slight performance drop.
• SurroundOcc mIoU 52.2 with visible masks.
• SDG improves mIoU +13.82 over baseline.
• Ablations: optimal r=1, l=8; K=7 neighborhood.",https://github.com/DzpLab/SDGOCC,,,https://ieeexplore.ieee.org/document/11093945/,https://zotero.org/alehanderoo/items/K5TQXZ2E,https://doi.org/10.1109/CVPR52734.2025.00633,09/10/2025 19:45 (GMT+2)
Yes,OccFormer,"2D features, 3D features, 3D-fusion, classify","• Reuse dual-path encoder for 3D feature encoding
• Adopt class-guided sampling for rare flying classes
• Use preserve-pooling for sparse voxel masks
• Plug 3D mask-classification decoder before 4DGS
• Requires calibrated extrinsics; plan self-calibration module
• Benchmark on multi-view, overlapping camera rigs","• Vision-based 3D semantic occupancy from images
• Move beyond BEV to volumetric semantics
• Address sparse, discontinuous camera-generated 3D features
• Reduce heavy, inflexible 3D convolutions
• Mitigate class imbalance and supervision sparsity","• Strong fit for occupancy; no tracking components
• No dynamic/static separation or motion handling
• Assumes known camera geometry; calibration-free absent
• Likely extends to multi-view; overlap unreported
• Good backbone for 3D feature volume construction","• Dual-path transformer: local and global BEV pathways
• Local windowed attention per horizontal slice
• Global BEV pooling with shared attention and ASPP
• Mask2Former adapted for 3D mask classification
• Preserve-pooling via max-pool for masks
• Hungarian matching; mask-cls + depth losses","• Autonomous driving urban scenes
• SemanticKITTI: 22 sequences, 10/1/11 split
• Monocular left camera on SemanticKITTI
• nuScenes: 700/150/150 scenes; 1000 sequences
• Annotations: 21-class occupancy; sparse LiDAR semantics
• Cameras/layout/overlap specifics: N/A","• EfficientNetB7/ResNet101 image encoder for 2D features
• LSS depth distribution and voxel pooling to 3D
• Dual-path transformer encoder with weighted fusion
• 3D deformable attention pixel decoder, multi-scale
• Mask2Former-style transformer decoder for occupancy
• Class-guided sampling; preserve-pooling; AdamW training","• SOTA monocular SSC on SemanticKITTI test
• +1.24 mIoU over MonoScene; 11% relative
• +1.4 mIoU over TPVFormer on nuScenes
• Efficient vs 3D convs; fewer params, GFLOPs
• Ablations validate encoder, decoder, sampling benefits",https://github.com/zhangyp15/OccFormer,,,https://ieeexplore.ieee.org/document/10376645/,https://zotero.org/alehanderoo/items/BB47UAJG,https://doi.org/10.1109/ICCV51070.2023.00865,09/10/2025 19:39 (GMT+2)
Yes,FB-OCC,"2D features, 3D features, 3D-fusion, classify","• Reuse forward-backward view transform for 3D voxels.
• Adopt joint voxel-BEV feature fusion block.
• Pretrain depth+semantics with SAM-style labels.
• Use temporal aggregation for long-range occupancy.
• Beware calibration dependency; add self-calibration module.
• Prototype as 3D feature input to 4DGS.","• Camera-only 3D occupancy prediction for AV scenes.
• Unify forward and backward view transformations.
• Address sparsity from forward projection (LSS).
• Leverage large-scale pretraining to avoid overfitting.
• Win CVPR2023 occupancy challenge with SOTA mIoU.","• Strong multi-camera 3D fusion; no self-calibration.
• No tracking; only occupancy semantics.
• Not tailored for flying-object volumetric space.
• Voxel-BEV fusion useful for 4DGS features.
• Temporal cues applicable; optical flow untested.
• Check adaptation without LiDAR depth supervision.","• Forward-backward view transformation unification.
• LSS depth uncertainty for forward projection.
• BEVFormer-inspired depth-aware backward attention.
• Joint voxel-BEV representation compression/expansion.
• Losses: distance-aware Focal, Dice, Lovasz-softmax.
• Affinity geometry/semantic losses; mIoU evaluation.","• Domain: urban driving, nuScenes occupancy dataset.
• #sequences/frames: N/A.
• #cameras/layout/overlap: N/A.
• Annotations: 3D voxels, 18 classes, camera masks.
• 3D-motion present? N/A.
• Availability/license: public dataset, license N/A.","• Encode multi-view images with strong 2D backbones.
• Forward project features into 3D voxels (LSS).
• Compress voxels to BEV; backward attend to images.
• Fuse BEV and voxels; voxel encoder, occupancy head.
• Assumes known camera calibration; not self-calibrated.
• Pretrain joint depth+semantic using SAM-generated masks.","• Test mIoU 54.19% with ensemble, first place.
• Scaling backbone to 1.2B improves mIoU.
• Joint depth-semantic pretraining yields notable gains.
• Ignoring invisible voxels increases accuracy.
• TTA and temporal TTA boost distant voxels.
• Training used 32 A100 GPUs, heavy compute.",https://github.com/NVlabs/FB-BEV,,,http://arxiv.org/abs/2307.01492,https://zotero.org/alehanderoo/items/U9LIV4U2,https://doi.org/10.48550/arXiv.2307.01492,09/10/2025 19:37 (GMT+2)
Yes,UniScene,"2D features, 3D features, 3D-fusion, classify","• Adopt occupancy pretraining to warm-up RGB encoders.
• Use multi-frame fusion cautiously; handle dynamics.
• Fine-tune for 3D occupancy classification before tasks.
• Add semantic supervision to separate static/dynamic.
• Risk: requires LiDAR pairs; not RGB-only.
• Evaluate transfer to flying-object, aerial scenes.","• Unified pre-training for multi-camera 3D perception.
• Leverages 3D occupancy reconstruction, not depth only.
• Exploits spatial-temporal correlations across cameras.
• Reduces 3D annotation reliance using unlabeled pairs.
• Gap: monocular pre-training ignores multi-view structure.","• Strong multi-camera pretext; no tracking support.
• Not calibration-free; assumes known camera extrinsics.
• Focus on ground scenes, not flying objects.
• Occupancy pretraining could benefit 3DGS features.
• Consider 4D occupancy for motion-aware tracking.
• Investigate RGB-only surrogate labels for pretraining.","• Occupancy as holistic 3D scene representation.
• BEV features transformed to 3D voxel volume.
• Lightweight 3D conv decoder predicts voxel occupancy.
• Binary focal loss with α=2, γ=0.25.
• Multi-frame LiDAR fusion generates occupancy labels.
• Spatio-temporal fusion across views and frames.","• Domain: autonomous driving, urban roads.
• nuScenes: 28,130 train, 6,019 val samples.
• Six cameras, 360° coverage, overlapping views.
• Labels: occupancy from fused LiDAR keyframes.
• 3D motion present: dynamic traffic participants.
• Availability: code released; dataset public.","• Backbones: ResNet-101, Swin-B for 2D features.
• View transform: LSS or Transformer to BEV.
• 3D fusion: BEV to voxel grid via reshaping.
• Decoder: 3D conv occupancy head; discarded after pretrain.
• Training: 24 epochs, focal loss, 8×A40 GPUs.
• Calibration-free pose: N/A; optical flow: N/A.","• Improved mAP/NDS by ~2% over monocular pretraining.
• Test set: +1.9% mAP, +1.7% NDS.
• Semantic occupancy mIoU +3.14% over BEVStereo.
• Label efficiency: 75% labels match full.
• Best with 3-frame LiDAR fusion for labels.
• Comparable to BEVDistill without annotations.",https://github.com/chaytonmin/UniScene,,,http://arxiv.org/abs/2305.18829,https://zotero.org/alehanderoo/items/Z3GFZQCT,https://doi.org/10.48550/arXiv.2305.18829,09/10/2025 19:35 (GMT+2)
Yes,DETR3D,"2D features, 3D-fusion, classify","• Reuse 3D-to-2D query fusion for multi-view RGB
• Replace box head with 3D occupancy outputs
• Add optical flow features for motion cues
• Integrate calibration-free pose estimation front-end
• Use queries to seed 4DGS object splats
• Prototype flying-object classification on query features","• Multi-view 3D detection from RGB only
• Avoid depth reconstruction compounding errors
• Fuse cameras early; eliminate NMS post-processing
• Improve performance in camera overlap regions
• Cast detection as 3D set prediction","• Strong multi-camera fusion; helpful for overlapping views
• No tracking; add 3D MOT head needed
• Assumes known extrinsics; conflicts calibration-free goal
• No explicit 3D volume; adapt occupancy representation
• Ground-plane bias; adapt for aerial flying objects
• No explainability; consider Grad-CAM on backbones","• 3D-to-2D back-projection with known camera matrices
• Sparse learned object queries encode 3D reference points
• Multi-head self-attention for inter-object interactions
• Set-to-set Hungarian matching supervision
• Focal loss classification; L1 box regression
• Iterative refinement across layers","• Domain: autonomous driving, nuScenes benchmark
• 1,000 sequences; 20s each; 20 Hz sampling
• Six surround cameras; overlapping FOVs
• Annotations: 3D boxes, classes, velocities
• 3D motion present; dynamic road scenes
• Availability: public dataset; official evaluation toolkit","• Shared ResNet101+FPN multi-scale 2D features
• Uses provided intrinsics/extrinsics; not calibration-free
• Decode queries to 3D centers iteratively
• Back-project centers; bilinear sample from all cameras
• Attention refines queries; heads regress boxes, classify
• Train with focal+L1, Hungarian; AdamW, 12 epochs","• SOTA NDS 0.479 on nuScenes test
• Outperforms baselines without NMS post-processing
• Strong gains in overlap regions; NDS 0.384
• Pseudo-LiDAR baseline underperforms substantially
• Iterative layers improve NDS from 0.38 to 0.425
• Training ~18 hours; inference efficient without NMS",https://github.com/WangYueFt/detr3d,,,http://arxiv.org/abs/2110.06922,https://zotero.org/alehanderoo/items/5ANIV8L8,https://doi.org/10.48550/arXiv.2110.06922,09/10/2025 19:33 (GMT+2)
Yes,A Review of Multi‐Object Tracking in Recent Times,"2D features, track","• Reuse ByteTrack/BoT-SORT for association baseline
• Adopt CenterTrack/FairMOT for 2D seed tracks
• Evaluate on UAV datasets with flying targets
• Extend to multi-camera 3D association framework
• Integrate optical flow and robust ReID features
• Beware single-camera bias; limited 3D guidance","• Survey recent deep-learning MOT methods
• Categorize detection-, SOT-, segmentation-based approaches
• Review datasets and evaluation metrics
• Discuss challenges and future directions
• Highlight integrated core technologies gap","• Primarily 2D, single-camera; limited multi-view relevance
• No pose self-calibration guidance provided
• No 3D occupancy or Gaussian Splatting content
• Useful catalog of association and ReID modules
• Transformer memory ideas helpful for long occlusions
• Validate applicability to flying object dynamics","• MOT paradigms: TBD, JDT, SOT, segmentation
• Transformer-enhanced tracking frameworks overview
• Data association via motion and appearance cues
• Metrics defined: MOTA, MOTP, IDF1 equations
• Assumption: detector quality drives tracking performance","• Domain: general MOT, pedestrians, vehicles, actors
• Number of sequences: N/A
• Cameras/layout/overlap: mostly single-camera; N/A overlap
• Annotation type: boxes; masks for MOTS
• 3D-motion present?: limited; mainly 2D trajectories
• Availability/license: datasets public; article open-access","• Categorizes TBD, JDT, transformer, SOT, segmentation
• Pose strategy: N/A self-calibration or extrinsics
• 2D backbones: YOLOX, CenterNet, RetinaNet, DETR
• 3D fusion method: N/A
• Heads: ReID embeddings, Kalman, memory, association
• Training details: N/A losses, compute specifics","• ByteTrack, BoT-SORT excel on MOT17/20 benchmarks
• Transformers strong on DanceTrack; heavy compute costs
• FairMOT balances accuracy and real-time speed
• Segmentation improves occlusion handling; annotation costly
• SOT integration recovers misses; scalability challenges",,,,https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cvi2.70010,https://zotero.org/alehanderoo/items/HS9PAY5Q,https://doi.org/10.1049/cvi2.70010,09/10/2025 19:30 (GMT+2)
Yes,ODTrack,"2D features, track","• Reuse temporal token memory for per-camera 2D cues.
• Use tokens as prompts for 3D fusion gating.
• Add optical-flow tokens to strengthen motion cues.
• Extend to multi-object association before 3D tracking.
• Risk: single-camera, single-object, no 3D reasoning.
• Test on flying-object videos for robustness.","• Track objects using dense temporal associations.
• Overcome sparse image-pair temporal modeling limitations.
• Online token propagation for video-level tracking.
• Avoid complex online update strategies.","• Strong 2D temporal modeling; lacks multi-view support.
• No pose, 3D features, or occupancy modeling.
• Token memory could aid 4DGS temporal consistency.
• Consider multi-object, multi-camera token propagation.
• Needs 2D segmentation for dynamic/static separation.
• Investigate token drift during long occlusions.","• Token sequence compresses target appearance and localization.
• Auto-regressive temporal token propagation across frames.
• Two attention mechanisms: concatenated, separated tokens.
• 2D ViT backbone, video-as-sentence formulation.
• Losses: focal, L1, GIoU combined.","• Single-object tracking videos across seven benchmarks.
• GOT10K >10k sequences, protocol compliant training.
• LaSOT 1120 train, 280 test sequences.
• VOT2020 60 sequences with mask annotations.
• TrackingNet test 511 videos; LaSOT ext 150.
• Single-camera videos; 3D motion annotations N/A.","• Inputs: k references, n search frames, arbitrary length.
• 2D ViT-Base encoder with MAE initialization.
• Temporal token attention: concatenated and separated variants.
• Prediction heads: classification and bounding box regression.
• Optimizer AdamW, 300 epochs, staged learning rates.
• Inference: propagate tokens; 32 fps on 2080Ti.","• SOTA on GOT10K: 77.0 AO (B), 78.2 (L).
• LaSOT AUC 73.2 (B), 74.0 (L).
• TrackingNet AUC 85.1 (B), 86.1 (L).
• VOT2020 EAO 0.581 (B), 0.605 (L).
• Token propagation +1.2% AUC over no tokens.
• 92M params, 73G FLOPs, real-time 32 fps.",https://github.com/GXNU-ZhongLab/ODTrack,,,http://arxiv.org/abs/2401.01686,https://zotero.org/alehanderoo/items/L3FBF56W,https://doi.org/10.48550/arXiv.2401.01686,09/10/2025 19:29 (GMT+2)
Yes,HOTA,"classify, track","• Use HOTA to evaluate 3D MOT outputs
• Adopt Multi-Camera HOTA for overlapping RGB cameras
• Report DetA, AssA, AssRe/Pr, LocA for diagnostics
• Use OHOTA for online tracking evaluation
• Use CA-HOTA for class-aware flying object results
• Consider FA-HOTA if fragmentation impacts safety","• Unified MOT metric balancing detection, association, localization
• Addresses MOTA bias toward detection
• Addresses IDF1 non-monotonic detection behavior
• Provide decomposable submetrics for error analysis
• Align metric with human visual judgment","• Strong fit for evaluating 3D multi-view tracking quality
• Not a method for detection/fusion itself
• Supports 3D boxes and multi-camera IDs
• Useful for tuning association versus detection balance
• Default HOTA ignores fragmentation; FA-HOTA mitigates
• Online scenarios prefer OHOTA variant","• Double Jaccard formulation for scoring
• HOTA equals sqrt(DetA × AssA)
• Defines TPA, FNA, FPA association sets
• Integrates over localization thresholds for LocA
• Bijective detection-level matching via Hungarian
• Monotonicity and metric properties analyzed","• MOT17 pedestrian tracking benchmark evaluated
• 37 published trackers compared on MOT17
• 36 six-second clips for user study
• Single-camera sequences; multi-camera extension discussed
• 2D boxes; 3D generalization described conceptually
• Open benchmark; public detections used","• Detection-level bijective matching each frame
• Score matches to maximize HOTA
• Association via TPA/FPA/FNA per TP
• Integrate scores over IoU thresholds
• Submetrics: DetA, AssA, DetRe/Pr, AssRe/Pr, LocA
• Extensions: OHOTA, FA-HOTA, W/CA/Fed/CR-HOTA","• Balanced detection-association measurement achieved
• Higher correlation with human judgments than MOTA/IDF1
• HOTA correlates strongly with AssA, moderately with DetA
• Independent of frame rate variations
• Decomposable submetrics enable targeted analysis
• Ranks trackers differently from MOTA/IDF1 appropriately",https://github.com/JonathonLuiten/HOTA-metrics,,,http://arxiv.org/abs/2009.07736,https://zotero.org/alehanderoo/items/SGU78X79,https://doi.org/10.1007/s11263-020-01375-2,09/10/2025 19:26 (GMT+2)
Yes,Rethinking RAFT for Efficient Optical Flow,2D features,"• Use Ef-RAFT for per-camera temporal features.
• Derive motion masks to separate dynamic/static.
• Test on aerial flying targets, fast motions.
• Compare RAFT versus Ef-RAFT on videos.
• Integrate flows to aid 4DGS object segmentation.
• Beware 2D-only; enforce multi-view temporal consistency.","• Improve RAFT on large displacements, repetitive patterns.
• Reduce runtime and GPU memory overheads.
• Maintain accuracy with efficient modifications.
• Introduce ALO and AFL modules.
• Validate on Sintel and KITTI datasets.","• Strong 2D flow module; no 3D fusion.
• Helpful for dynamic/static separation pre-4DGS.
• No multi-camera overlap or self-calibration.
• Likely robust to repetitive skies and clouds.
• Need evaluation on flying objects and occlusions.
• Consider flow-guided multi-view consistency losses.","• RAFT iterative refinement with correlation volumes.
• ALO scales/translates lookup grids adaptively.
• AFL adds global features via attention.
• Relative positional encoding for delta estimates.
• Multi-step L1 loss with gamma=0.8.","• FlyingChairs, FlyingThings for pretraining.
• Sintel train split evaluated.
• KITTI-15 train split evaluated.
• Single-camera frame pairs; no multi-view overlap.
• Annotations: dense optical flow ground truth.
• Code available; datasets standard licenses.","• RAFT backbone with feature and context encoders.
• ALO modifies correlation lookup radius adaptively.
• AFL applies row/column multi-head attention.
• Pose estimation: none; calibration-free not addressed.
• Training: AdamW, one-cycle; gradient clipping ±1.
• Compute: single RTX 3090; PyTorch implementation.","• Sintel EPE improved ~10% over RAFT.
• KITTI F1-All improved nearly 6%.
• Runtime overhead approximately 33% vs RAFT.
• Memory usage increased about 13%.
• Ablations show ALO and AFL contributions.",https://github.com/n3slami/Ef-RAFT,,,http://arxiv.org/abs/2401.00833,https://zotero.org/alehanderoo/items/WVMLNQWJ,https://doi.org/10.48550/arXiv.2401.00833,09/10/2025 19:17 (GMT+2)
Yes,RAFT,2D features,"• Use RAFT for per-camera optical flow features.
• Leverage flow to separate static and dynamic.
• Warm-start across frames for stable motion cues.
• Feed correlation features to 3DGS module.
• Risk: single-view only; no multi-camera constraints.
• Quick-win: pretrain RAFT; evaluate on flying objects.","• Achieve SOTA dense optical flow accuracy.
• Handle fast motion, occlusion, blur, low texture.
• Avoid coarse-to-fine cascades' error propagation.
• Improve training efficiency and generalization.","• Good fit for temporal 2D motion features.
• Mismatch: no pose, 3D fusion, or tracking.
• Helps dynamic object segmentation before 4DGS.
• Likely robust for small, fast flying objects.
• Need multi-view flow consistency across cameras.
• Test on aerial scenes, sky backgrounds, occlusions.","• All-pairs 4D correlation over learned per-pixel features.
• Multi-scale correlation pyramid captures large displacements.
• Recurrent ConvGRU updates mimic optimization steps.
• Sequence loss with exponential weighting (gamma=0.8).
• Learned convex upsampling preserves motion boundaries.","• Datasets: Sintel, KITTI-2015, HD1K, DAVIS.
• #frames: pairs; videos for warm-start.
• #cameras: single; no overlap.
• Annotations: dense flow ground truth.
• 3D motion present implicitly; 2D labels.
• Availability: public datasets; open-source code.","• Pipeline: features → 4D correlation → ConvGRU updates.
• Pose strategy: N/A.
• 2D backbone: CNN residual encoder; no external flow.
• 3D fusion method: N/A.
• Heads: no classify/track; outputs dense flow.
• Training: L1 sequence loss, AdamW, gradient clipping.","• Sintel final EPE 2.855; 30% improvement.
• KITTI F1-all 5.10%; 16% reduction.
• Cross-dataset EPE 5.04 on KITTI.
• 10 FPS at 1088×436; 5.3M parameters.
• Small model 20 FPS; still SOTA.
• Ablations support GRU, pooling, all-pairs choices.",https://github.com/princeton-vl/RAFT,,,http://arxiv.org/abs/2003.12039,https://zotero.org/alehanderoo/items/JJBIE58K,https://doi.org/10.48550/arXiv.2003.12039,09/10/2025 19:14 (GMT+2)
Yes,CaMuViD,"2D features, 3D-fusion, classify, pose","• Reuse calibration-free multi-view fusion for RGB-only cameras.
• Replace BEV/3D projection with learned feature-space mappings.
• Add temporal modules, optical flow, for motion cues.
• Extend to 3D occupancy via 4DGS volume heads.
• Integrate multi-class heads for flying object categories.
• Validate on flying, overlapped multi-camera datasets.","• Occlusion-robust multi-view pedestrian detection without calibration.
• Avoid BEV projection distortions and overfitting.
• Detect directly in image space across cameras.
• Generalize across camera layouts and scenes.
• Reduce reliance on ground-truth annotations.","• Strong fit for calibration-free, overlapping multi-view fusion.
• Mismatch: no 3D occupancy or tracking outputs.
• Not tailored for flying, non-grounded objects.
• Learned feature projections inspire 3DGS feature alignment.
• Need temporal consistency, cross-view ID association modules.
• Investigate explainability, inverse consistency stability.","• Calibration-free feature projection and back-projection matrices.
• Common representation space derived from per-view features.
• Concatenation fusion plus 1x1 conv channel reduction.
• Inverse-consistency enforced: A_b approximates A_p inverse.
• Multi-view projection loss L_vp; standard detection losses.","• Domain: pedestrian multi-view surveillance scenes.
• Wildtrack: 7 cameras, 3.74-view average overlap.
• MultiviewX: 6 cameras, 4.41-view average overlap.
• Annotations: 2D boxes, ground-plane IDs, grid occupancy.
• 3D motion present across synchronized frame sequences.
• Public datasets, open access licenses.","• InternImage-t backbone; Cascaded R-CNN detection head.
• FCNs predict per-view A_p, A_b from features.
• Concatenate projected features; 1x1 conv for fusion.
• Calibration-free; no homographies or extrinsics required.
• Training: 20 epochs, lr 1e-4, 640×1338 inputs.
• Losses: bbox/classification + L_vp (weight 1e-4).","• MODA: 95.0 Wildtrack; 96.5 MultiviewX; SOTA.
• Best recall and F1 across both datasets.
• Cross-dataset MODA 86.4, surpassing prior methods.
• Concatenation+FRM outperforms summation fusion.
• Geometry-based projection replacement drops MODA by 9.1%.",https://github.com/amiretefaghi/CaMuViD,,,https://ieeexplore.ieee.org/document/11094313/,https://zotero.org/alehanderoo/items/KCKLXGN4,https://doi.org/10.1109/CVPR52734.2025.00122,09/10/2025 19:12 (GMT+2)
Yes,Enhancing Multi-view Pedestrian Detection Through Generalized 3D Feature Pulling,"2D features, 3D features, 3D-fusion, pose","• Reuse 3D feature pulling for volumetric fusion.
• Adopt maximal fusion for multi-view robustness.
• Extend Z dimension for aerial object volumes.
• Integrate optical flow for temporal feature lifting.
• Add self-calibration module; remove reliance on extrinsics.
• Use 4DGS to replace BEV collapse.","• Robust multi-view pedestrian detection across diverse scenes.
• Mitigate IPM feature loss and distortion.
• Unify view-specific features in 3D space.
• Improve generalization across camera setups.","• Strong match on multi-camera overlap and 3D fusion.
• Mismatch: assumes ground-plane occupancy, not flying.
• Requires calibrations; no self-calibration strategy.
• No classification or 3D tracking outputs.
• Design insight: parameter-free pulling reduces overfitting.
• Open: extend to dynamic 4D volumes with tracking.","• Param-free 3D feature pulling via projection.
• Uses known intrinsics and extrinsics matrices.
• Bilinear sampling for sub-pixel feature retrieval.
• Max-pooling fusion across views for robustness.
• MSE loss on ground-plane occupancy maps.
• 3D CoordConv encodes positional information.","• WildTrack: 7 cameras, 400 frames, real.
• MultiviewX: 6 cameras, 400 frames, synthetic.
• GMVD: 3–8 cameras, multiple scenes, synthetic.
• Annotations: BEV occupancy maps, pedestrian locations.
• Overlapping views; average 3.74 cameras per person.
• Availability/license: N/A.","• Dilated ResNet-18 extracts multi-view 2D features.
• Foreground selector channel-attention filters semantics.
• 3D feature pulling with frustum mask.
• Maximal fusion via voxel-wise max-pooling.
• Large-kernel refiner with depthwise separable convolutions.
• Adam, cosine schedule; voxel 10×10×20 cm; MSE.","• SOTA MODA on WildTrack: 94.1.
• SOTA MODA on MultiviewX: 95.7.
• Higher recall than baselines across datasets.
• Generalization: 82.6 MODA, 93.4 recall (7 cams).
• GMVD generalization: 73.3 MODA, outperforming GMVD baseline.
• Trainable on >15GB GPU; efficient parameterization.",,,,https://ieeexplore.ieee.org/document/10483981/,https://zotero.org/alehanderoo/items/MRCCLTHJ,https://doi.org/10.1109/WACV57701.2024.00123,09/10/2025 19:09 (GMT+2)
Yes,Game Engine Based Multi-View Video Dataset Synthesis for Pedestrian Detection and Tracking (2024),"2D features, 3D-fusion, pose, track","• Use as data engine for multi-view flying objects.
• Add sky scenes and aerial trajectories in Unity.
• Extend POM to volumetric 3D occupancy grids.
• Integrate 2D optical flow for temporal cues.
• Export extrinsics for self-cal baseline evaluation.
• Beware ground-plane assumptions; adapt to 3D flight.","• Lack of large multi-view training data.
• Privacy/ethics hinder real data collection.
• Provide modular synthetic dataset pipeline.
• Support overlapping multi-camera configurations.
• Output WILDTRACK-format images and annotations.","• Strong fit for multi-camera overlap and tracking labels.
• Mismatch: ground-plane POM, not volumetric occupancy.
• No Gaussian Splatting or 3DGS/4DGS integration.
• Useful for pose-controlled synthetic calibration tests.
• Needs flying object assets and motion models.
• Consider domain gap to aerial RGB imagery.","• Multi-view geometry and back-projection for POM.
• OpenCV-based intrinsic/extrinsic calibration usage.
• Right-hand/left-hand coordinate conversion.
• Unity HDRP, PBR, ray tracing rendering.
• No new losses; dataset synthesis only.","• Domain: synthetic pedestrians in urban scenes.
• Frames: configurable; example 12x200 per batch.
• Cameras: 4–12+, ellipse or manual placement.
• Overlap: multi-view FoV with shared AOI.
• Annotations: 2D boxes/masks, IDs, 3D positions, POM.
• 3D motion: wandering trajectories; code public, assets licensed.","• Unity3D synthesis with HDRP and randomization.
• Camera placement via ellipse/hand; aim target centered.
• Calibration metadata exported in datasetParameters.py.
• Synchronized export with publisher-subscriber cameras.
• MultiviewX Perception generates POM by back-projection.
• Ground truth IDs, 2D boxes/masks, 3D positions.","• 40–200 frames/minute depending on scene/camera count.
• Throughput decreases as camera number increases.
• Supports scalable multi-camera dataset generation.
• Claims robustness on novel environments.
• No accuracy metrics versus baselines reported.",https://github.com/TsingLoo/com.tsingloo.wildperception,,,https://ieeexplore.ieee.org/document/10740021/,https://zotero.org/alehanderoo/items/PQAUHNUI,https://doi.org/10.1109/MetaCom62920.2024.00049,09/10/2025 19:05 (GMT+2)
Yes,WILDTRACK,"2D features, 3D-fusion, pose, track","• Use dataset to develop multi-camera fusion module.
• Leverage BA procedure to validate self-calibration.
• Prototype ground-plane occupancy before full 3D volume.
• Adapt detection heads for flying-object classes.
• Test KSP association; extend to 3D trajectories.
• Risk: ground-plane constraint mismatches aerial targets.","• Lack of large-scale overlapping multi-camera pedestrian dataset.
• Improve detection under occlusions using multi-view geometry.
• Provide precise joint calibration and synchronization benchmarks.
• Benchmark multi-view detection and tracking methods.","• Strong multi-view overlap aligns with thesis sensors.
• No flying objects; motion limited to ground-plane.
• Calibration not calibration-free; could inspire initialization.
• Lacks 3D volume modeling or Gaussian Splatting.
• Useful for benchmarking fusion and tracking components.
• Open: dataset license, raw videos availability.","• Pinhole camera model with radial/tangential distortion.
• Joint bundle adjustment minimizing reprojection error (Eq.1).
• Probabilistic Occupancy Map with mean-field inference.
• CNN-CRF higher-order occlusion reasoning on ground plane.
• K-Shortest Paths tracking on occupancy graph.","• Public plaza pedestrians, dense crowds, unscripted.
• 7 static HD cameras, highly overlapping views.
• 1920x1080, 60fps; annotations at 2fps.
• 400 frames per view; 9518 multi-view annotations.
• 3D ground-plane locations and 2D boxes.
• Availability public; license N/A.","• Manual ground points, joint BA for intrinsics/extrinsics.
• Synchronized within ~50ms; strong multi-view overlap.
• Baselines: POM-CNN, DeepMCD, Deep-Occlusion, RCNN-projected.
• Backbones: GoogLeNet, ResNet-18, DenseNet-121.
• Top-view NMS; grid 60x180; r=0.5m.
• Tracking: KSP and ptrack post-processing.","• Multi-view beats monocular across metrics.
• Deep-Occlusion MODA 0.741, Precision 0.95, Recall 0.80.
• ResNet-DeepMCD MODA 0.678, MODP 0.642.
• Tracking IDF1 78.4 with ptrack.
• Multi-view training increases accuracy and confidence.
• Occlusion impact reduced via overlapping views.",,,,https://ieeexplore.ieee.org/document/8578626/,https://zotero.org/alehanderoo/items/66MBS7JP,https://doi.org/10.1109/CVPR.2018.00528,09/10/2025 19:03 (GMT+2)
Yes,Bringing Generalization to Deep Multi-View Pedestrian Detection,"2D features, 3D-fusion, pose","• Reuse permutation-invariant multi-view pooling module.
• Adopt DropView to handle missing/async cameras.
• Use KLCC loss for occupancy probability maps.
• Pretrain on GMVD for multi-view fusion robustness.
• Integrate with 4DGS as BEV fusion prior.
• Plan calibration-free pose; method assumes known extrinsics.","• Improve generalization in deep multi-view pedestrian detection.
• Handle varying number of cameras robustly.
• Handle changing camera positions and configurations.
• Generalize to unseen scenes without retraining.
• Address overfitting from homogeneous train-test splits.","• Strong multi-view fusion; lacks volumetric 3D occupancy.
• Assumes ground-plane; unsuitable for flying objects.
• No self-calibration; conflicts with calibration-free goal.
• No classification or 3D tracking heads.
• Design lesson: enforce permutation invariance across views.
• Need extension to 3D voxel features and dynamics.","• Assumes calibrated static cameras with overlapping FoV.
• Project 2D features to ground-plane via perspective transform.
• Permutation-invariant average pooling across views.
• DropView regularization randomly discards one view.
• KL-Divergence plus Cross-Correlation loss (KLCC).
• Occupancy map predicted by dilated convolutions.","• Domain: crowded pedestrian multi-view detection.
• GMVD: 53 sequences, 7 scenes, 4983/1012 frames.
• Cameras: 3–8 per scene, overlapping static layout.
• Annotations: BEV occupancy, plus GTAV track IDs.
• 3D motion: walking pedestrians; no flying objects.
• Availability: public GitHub; license unspecified.","• ResNet18 backbone with dilated convolutions.
• Calibrated poses; no self-calibration or flow.
• Perspective projection to BEV feature grid.
• Average-pool features across arbitrary views.
• Dilated-conv head outputs occupancy map.
• KLCC loss, DropView, ImageNet pretrain, NMS, 10 epochs.","• Competitive MODA on WildTrack and MultiViewX.
• Strong under camera dropout: MODA 77.0 vs 66.6.
• Robust to configuration changes; >20 MODA improvements.
• Scene generalization: 66.1–70.7 MODA MultiViewX→WildTrack.
• GMVD training transfers: 80.1 MODA on WildTrack.
• Slightly below MVDeTr/SHOT on in-scene tests.",https://github.com/jeetv/GMVD,,,http://arxiv.org/abs/2109.12227,https://zotero.org/alehanderoo/items/UJ3NC4ZU,https://doi.org/10.48550/arXiv.2109.12227,09/10/2025 19:01 (GMT+2)
Yes,MMPTRACK,"2D features, 3D features, 3D-fusion, classify, pose, track","• Reuse multi-view heatmap fusion for 3D occupancy cues
• Adapt Hungarian 3D association to flying targets
• Avoid ground-plane assumptions for aerial objects
• Leverage environment adaptation for multi-camera RGB
• Use dataset to test multi-view association modules
• Benchmark ReID robustness across overlapping views","• Lack of large multi-camera MOT datasets
• Need overlapped calibrated setups for occlusions
• Expensive manual multi-view annotations
• Provide auto-annotated dense 3D tracking labels
• Benchmark real-time multi-camera trackers
• Study domain adaptation for ReID","• Strong overlap, calibration align with multi-camera needs
• Ground-plane assumption mismatches flying objects
• No calibration-free self-calibration demonstrated
• No Gaussian Splatting or volumetric features
• Useful for association, fusion design insights
• Check data availability and calibration formats","• Top-down ground-plane occupancy mapping
• Tracking-by-detection with Hungarian assignment
• Multi-view heatmap fusion via homographies
• 3D pose fusion using VoxelPose
• Extrinsic optimization using ArUco graph
• Focal Loss for ground-point estimation","• Indoor retail, lobby, industry, cafe, office
• ~2.98M annotated frames total
• 23 calibrated RGBD cameras across five environments
• Highly overlapped views; fixed cameras
• 2D boxes+IDs; 3D top-down tracks
• Public release planned; license N/A","• RGBD auto-annotation with 3D tracker
• Calibrated extrinsics via ArUco bipartite optimization
• DMCT: CornerNet heatmaps; YOLOv5 top-down detector
• Heatmaps fused on ground-plane via homographies
• VoxelTrack: VoxelPose 3D skeleton tracking
• Hungarian association; ReID via FastReID","• DMCT-Ext-TD: 74.1 IDF1, 94.6 MOTA
• Deep top-down detector reduces IDs significantly
• External OpenImages pretraining gives limited gains
• Environment-specific training boosts IDF1 strongly
• VoxelTrack underperforms due to domain gap
• Auto-annotation achieves ~100% IDF1 on checks",,,,http://arxiv.org/abs/2111.15157,https://zotero.org/alehanderoo/items/UW7A8NYG,https://doi.org/10.48550/arXiv.2111.15157,09/10/2025 18:59 (GMT+2)
Yes,A unified multi-view multi-person tracking framework,"2D features, 3D-fusion, pose, track","• Reuse PDNC for cross-view association of aerial targets
• Adapt CMMT for robust temporal 3D triangulation
• Ensure overlapping calibrated cameras for full-volume coverage
• Augment with optical flow for temporal cues
• Swap detectors for flying-object categories
• Explore self-calibration to remove calibration dependency","• Unify footprint and pose 3D multi-view tracking
• Jointly exploit multi-view and multi-frame associations
• Overcome ground-plane homography limits for poses
• Improve robustness under occlusion and sparse cues","• Strong multi-view overlap assumption matches thesis setup
• Requires calibration; thesis seeks calibration-free poses
• No 3D occupancy or Gaussian Splatting components
• Ideas transfer to association and triangulation modules
• Sliding-window linkage beneficial for online robustness
• Needs adaptation for non-human, flying-object dynamics","• Normalized epipolar distance removes projection-scale bias
• PDNC: propagatable distance non-parametric clustering
• CMMT: multi-frame multi-view triangulation with clustering
• Sliding-window online processing with spatiotemporal constraints
• Linear assignment for cross-window tracklet linking","• Datasets: Campus, Shelf, WILDTRACK, MMPTRACK
• Cameras: 3, 5, 7, and 4–6 respectively
• Subjects: 3, 4, 313 footprints, 28 total
• Overlapping, synchronized, calibrated multi-camera views
• Annotations: 3D poses, footprints; CLEAR, PCP metrics
• Availability/license: N/A","• YOLOX detection; per-camera SORT builds 2D tracklets
• Compute normalized epipolar cross-view distances
• Associate tracklets using PDNC under constraints
• Triangulate via CMMT with interpolation and clustering
• Link 3D tracklets across windows by linear assignment
• Optional ReID appearance; calibrated intrinsics/extrinsics required","• SOTA PCP on Campus and Shelf datasets
• Competitive WILDTRACK MOTA, IDF1 versus baselines
• MMPTRACK: 95% MOTA, 84% IDF1 with PDNC+CMMT
• PDNC improves association robustness and MOTA significantly
• CMMT outperforms RANSAC for 3D PCP
• Real-time speeds; scales with cameras and people",,,,https://ieeexplore.ieee.org/document/10897678/,https://zotero.org/alehanderoo/items/CYYGKUBD,https://doi.org/10.1007/s41095-023-0334-8,09/10/2025 18:58 (GMT+2)
Yes,Learning from Synchronization,"2D features, track","• Reuse synchronization pretext for uncalibrated multi-view association.
• Adopt camera embeddings to bypass explicit extrinsics.
• Integrate edge association to reduce matching ambiguity.
• Combine with optical flow for moving flying objects.
• Use as cross-view module before 3DGS fusion.
• Ensure tight time sync; static cameras only.","• Robust cross-view person association without labels.
• Re-ID unreliable with similar appearances.
• Avoid using calibrated camera parameters.
• Learn geometry via synchronization supervision.
• Handle crowded, overlapping multi-view scenes.","• Strong fit for uncalibrated multi-camera overlap.
• Mismatch: no 3D volume or occupancy.
• No explicit extrinsics; potential pose cues.
• Not designed for flying, small objects.
• Could aid cross-view ID before 3D tracking.
• Lacks explainability; add Grad-CAM on encoder.","• Synchronized views share rigid layout distributions.
• Encode geometry using Fourier positional encodings.
• Learnable camera embeddings for view invariance.
• Hungarian matching bridges instance-image distances.
• Triplet loss on image pairs.
• Linear constraints: reprojection L1, edge triplet.","• WILDTRACK: 7 cams, crowded outdoors, public.
• MVOR: 3 cams, operating room, public.
• SOLDIERS: 6 cams, indoor atrium, public.
• Uses detector pseudo boxes for training.
• Overlapping synchronized views; identity labels for eval.
• 3D motion present; license N/A.","• Detect persons per view with off-the-shelf detector.
• Fourier positional encodings for box geometry.
• Learnable camera embeddings for view invariance.
• Synchronization triplet loss with Hungarian association.
• Linear constraints: L1 reprojection, edge association.
• Adam optimizer, lr 1e-4→1e-5, A40 GPU.","• SOTA unsupervised on three multi-view benchmarks.
• WILDTRACK IPAA-100 54.64, surpasses ViT-P3DE.
• Precision 92.31, recall 94.34 on WILDTRACK.
• MVOR ACC 89.38, comparable to supervised.
• SOLDIERS ACC 95.89, large gains.
• Runtime/memory N/A.",https://github.com/CAMMA-public/Self-MVA,,,https://ieeexplore.ieee.org/document/11094625/,https://zotero.org/alehanderoo/items/2INXNLH6,https://doi.org/10.1109/CVPR52734.2025.02274,09/10/2025 18:53 (GMT+2)
Yes,Depth Anything at Any Condition,"2D features, 3D features","• Use as per-view depth prior for 3D fusion.
• Apply SDR to sharpen boundaries for 2D segmentation.
• Fine-tune on your camera perturbations with consistency.
• Beware monocular scale ambiguity across multi-cameras.
• Calibrate cross-view scales before 4DGS fusion.
• Quick test: integrate AC depth for dynamic/static separation.","• Robust MDE under adverse weather and lighting.
• Address data scarcity for corrupted conditions.
• Avoid poor pseudo-labels from corrupted images.
• Maintain generalization on standard benchmarks.
• Enhance boundaries via spatial distance constraints.","• No multi-view, overlap, or pose estimation.
• Useful robustness for outdoor flying conditions.
• Not providing classification or tracking modules.
• Depth could aid 3DGS initialization and masking.
• Consider temporal consistency extensions for videos.
• Verify performance on aerial, sky-dominant scenes.","• Perturbation-based consistency regularization between predictions.
• Affine-invariant disparity loss for scale robustness.
• Knowledge distillation with frozen teacher guidance.
• Spatial Distance Relation combining position and disparity.
• Spatial Distance Constraint loss aligns inter-patch geometry.
• Total loss weighted: Lc, Lkd, Ls.","• Domains: indoor and outdoor general scenes.
• 540K unlabeled images for fine-tuning.
• Single monocular camera per image; no overlap.
• Annotations: none; self-supervised consistency.
• 3D motion presence: N/A.
• Code and project page available, license unspecified.","• Fine-tune DepthAnythingV2 with perturbation consistency.
• Frozen teacher guides normal-branch predictions.
• Backbone ViT-S, DPT decoder; no optical flow.
• Spatial Distance Relation computed on patches.
• Losses: Lc, Lkd, Ls; equal weights.
• Training: AdamW, LR 5e-6, 20 epochs, 4x3090.","• Best DA-2K blur accuracy: 0.880 vs 0.862.
• Improved RobotCar-night δ1: +0.037 over V2.
• Strong on NuScenes-night AbsRel: 0.198.
• Comparable performance on general benchmarks.
• Perturbations and SDR both improve robustness.
• Runtime and memory: N/A.",https://github.com/HVision-NKU/DepthAnythingAC,,,http://arxiv.org/abs/2507.01634,https://zotero.org/alehanderoo/items/2RI9XH6B,https://doi.org/10.48550/arXiv.2507.01634,09/10/2025 18:50 (GMT+2)
Yes,Metric3Dv2,"2D features, 3D features, 3D-fusion","• Use CSTM to normalize varying intrinsics.
• Adopt model for per-frame metric depth priors.
• Derive normals to sharpen 2D segmentation edges.
• Self-train normals via depth-normal consistency.
• Fuse multi-view depths for static scene 3DGS initialization.
• Validate on drone, fisheye, outdoor scenes.","• Zero-shot monocular metric depth estimation.
• Zero-shot surface normal estimation from single image.
• Resolve metric ambiguity across camera intrinsics.
• Scale training to diverse camera models.
• Enable downstream SLAM, reconstruction, metrology.","• Strong 3D feature backbone for single-view geometry.
• No multi-camera overlap exploitation.
• No dynamic object separation or tracking.
• Helps pose-free scale via intrinsics normalization.
• Useful priors for 4DGS, classification, tracking.
• Need temporal consistency for flying objects.","• Canonical camera space transformation (CSTM) proposed.
• Focal length critical for metric recovery.
• Joint depth-normal recurrent optimization (ConvGRU).
• Random proposal normalization loss (RPNL).
• Depth-normal consistency self-supervision loss.
• Scale-invariant log, PWN, VNL losses used.","• 16+ public RGB-D datasets, indoor/outdoor.
• Over 16M training images total.
• Single-view training; some multi-camera datasets.
• Camera intrinsics provided; varied camera models.
• Labels: metric depth, normals, pseudo normals.
• 3D motion present in driving datasets.","• Single-image input; predict metric depth and normals.
• CSTM on images or labels using intrinsics.
• Backbones: ConvNeXt-L, DINOv2 ViTs with DPT.
• Recurrent ConvGRU refines depth/normal jointly.
• No optical flow; no extrinsics estimation.
• Losses: silog, PWN, VNL, RPNL; 48 A100, 800k.","• SoTA on 16 depth/normal benchmarks.
• NYUv2 FT AbsRel 0.047; KITTI FT 0.039.
• Zero-shot δ1 up to 0.980 on NYU.
• Reduces mono-SLAM drift significantly.
• Robust across thousands of camera models.
• ViT-L runs ~9.5 fps; memory ~7GB.",,,,http://arxiv.org/abs/2404.15506,https://zotero.org/alehanderoo/items/3JU2M5JR,https://doi.org/10.1109/TPAMI.2024.3444912,09/10/2025 18:47 (GMT+2)
Yes,One-Shot Multiple Object Tracking in UAV Videos Using Task-Specific Fine-Grained Features,"2D features, classify, track","• Use FDN for decoupled 2D features per camera
• Use PTE for small flying-object features
• Employ embeddings for cross-view association seeding
• Integrate as 2D front-end before 3D fusion
• Risk: single-view assumptions; no pose or 3D cues
• Quick-win: benchmark on drone flying-object subsets","• Robust MOT in UAV videos
• One-shot MOT suffers detection-ReID conflict
• Small targets hinder accurate localization
• Improve speed without sacrificing accuracy","• Strong 2D MOT; no multi-camera overlap support
• Not addressing pose, 3D occupancy, or splatting
• Useful for per-view appearance features
• Decoupling idea transferable to multi-task setups
• Need class extension to drones, birds, aircraft
• Verify robustness on fast, small aerial targets","• One-shot MOT with shared backbone
• Feature decoupling via self/cross-attention
• Transformer encoder with multi-scale dilation
• Losses: BCE cls, GIoU reg, ID CE
• Uncertainty weighting balances tasks
• Kalman-Hungarian association framework","• UAV aerial videos (VisDrone2021, UAVDT)
• VisDrone: 56 train, 7 val, 33 test
• UAVDT: 100 videos, ~80k images
• Single moving camera; no multi-view overlap
• Annotations: 2D boxes with IDs
• 2D tracks; no 3D annotations","• YOLOv5L backbone; shared features extracted
• FDN splits detection/ReID features
• PTE learns scale-aware fine-grained features
• ReID and detection heads predict outputs
• Association: cosine, Kalman, Hungarian, IOU second pass
• Training: 30 epochs, SGD, augmentations; BCE, GIoU, ID","• SOTA MOTA: 34.3 (VisDrone), 48.6 (UAVDT)
• SOTA IDF1: 45.0 and 66.2 respectively
• Outperforms FairMOT on both benchmarks
• Ablations: FDN/PTE significantly improve metrics
• Runtime 17.6 FPS; 69.3M params; 73.6G MACs
• Reduced IDS and Frag vs baselines",,,,https://www.mdpi.com/2072-4292/14/16/3853,https://zotero.org/alehanderoo/items/5R77ZITF,https://doi.org/10.3390/rs14163853,09/10/2025 18:43 (GMT+2)
Yes,Multi-Object Tracking by Flying Cameras Based on a Forward-Backward Interaction,"2D features, pose, track","• Reuse backward tuning for adaptive thresholds/DBSCAN.
• Use camera compensation for ego-motion removal.
• Leverage outlier-flow clustering for dynamic separation.
• Risks: single-view, 2D only; lacks 3D fusion.
• Quick-win: per-view stabilization before 3DGS reconstruction.
• Quick-win: integrate optical flow cues into 2D segmentation.","• Real-time MOT from drones with moving cameras.
• Remove manual parameter tuning via backward feedback.
• Handle ego-motion and dynamic environments robustly.
• Track multiple objects without prior knowledge.","• Mismatch: no multi-camera, no 3D occupancy.
• Useful: dynamic/static separation via motion compensation.
• Backward interaction inspires self-calibration loops.
• Finite-state lifecycle aids robust 3D tracker gating.
• Needs extension to overlapping multi-view synchronization.
• Evaluate for flying-object motion at altitude.","• Forward-backward interaction between detection and tracking.
• Homography-based camera compensation using LMedS.
• Lucas-Kanade flow, Shi-Tomasi features extraction.
• DBSCAN clustering of outlier flows for objects.
• Similarity combines position, shape, appearance histograms.
• Finite-state object lifecycle with thresholds T1,T2.","• Domain: UAV aerial videos, moving cameras.
• 53 videos, about 61k frames total.
• Single camera, head/top view; no overlap.
• Annotations: 2D boxes with IDs.
• 3D-motion present? N/A; image-plane motion only.
• Public datasets plus MIVIA; annotations available.","• Pipeline: camera compensation, dual detection, fusion, tracking.
• Calibration-free homography estimated from features.
• Shi-Tomasi corners, Lucas-Kanade optical flow.
• DBSCAN clusters outliers; fuse with foreground mask.
• Local association with position/shape/appearance similarity.
• Backward chain tunes M,K,eps,MinPts; FSM lifecycle.","• Overall F=0.58, MOTA=0.48, MOTP=0.60.
• Berlin F-scores up to 0.99; outperforms baselines.
• UAV123 performance lower; motion blur challenging.
• Real-time 16 FPS at 640x360 on Intel Joule.
• Robustness from automatic parameter adaptation.
• Best on controlled moving cameras; worst on MIVIA.",,,,https://ieeexplore.ieee.org/document/8432406/,https://zotero.org/alehanderoo/items/ZXQZR2HY,https://doi.org/10.1109/ACCESS.2018.2864672,09/10/2025 18:40 (GMT+2)
Yes,TeamTrack,"2D features, track","• Pretrain detectors on small, occluded athletes
• Use top/side views to test cross-view association
• Evaluate trackers under appearance-similarity stress
• Not suitable for flying-object classes
• Consider field-line self-calibration experiments
• Verify view synchronization before multi-camera fusion","• Full-pitch MOT in team sports
• Address similar appearance, heavy occlusions
• Provide multi-view side/top datasets
• Benchmark detection, forecasting, tracking
• Fill gap beyond broadcast partial views","• Strong for 2D MOT; limited 3D relevance
• Multi-view full-pitch aids overlap experiments
• No 3D labels or camera extrinsics
• Valuable for re-ID under similar uniforms
• Flying-object focus mismatch; ground athletes only
• Need synchronization, calibration details clarified","• Tracking-by-detection paradigm overview
• Appearance similarity via cosine distance
• Motion metrics: adjacent-frame IoU, position switches
• Detection metric: COCO mAP50:95
• Tracking metrics: HOTA, MOTA, IDF1","• Sports: soccer, basketball, handball
• 279,900 frames; 4,374,900 boxes total
• Cameras: drones top, fisheye side; full-pitch
• Multi-view availability; overlap across views
• Annotations: boxes, track IDs, team labels
• Availability: Kaggle/Drive; license N/A","• Fine-tuned YOLOv8 detectors per sport/view
• AdamW, augmentations, TTA; varied image sizes
• Trajectory models: constant velocity and LSTM
• Trackers: ByteTrack and BoT-SORT baselines
• No calibration, 3D fusion, or optical flow
• Annotation with CVAT/Labelbox; interpolation used","• Fine-tuning greatly improved detection mAP
• Top-view detection mAP lower for small targets
• LSTM forecasting outperformed constant velocity
• Tracking performance varied strongly by viewpoint
• Average ByteTrack comparable to MOT17; tougher than DanceTrack
• Runtime and memory metrics N/A",https://atomscott.github.io/TeamTrack/,,,http://arxiv.org/abs/2404.13868,https://zotero.org/alehanderoo/items/4N8Y69D2,https://doi.org/10.48550/arXiv.2404.13868,09/10/2025 18:38 (GMT+2)
Yes,A Robust Online Multi-Camera People Tracking System With Geometric Consistency and State-aware Re-ID Correction,"2D features, 3D features, 3D-fusion, pose, track","• Reuse multi-view geometric affinity for association.
• Integrate state-aware Re-ID feature bank.
• Replace homography with full 3D triangulation for flying.
• Add self-calibration for unknown camera extrinsics.
• Test epipolar-only matching on airborne targets.
• Evaluate with RGB-only, overlapping drone cameras.","• Robust online multi-camera people tracking under occlusions.
• Improve cross-view association with geometric constraints.
• Reduce ID switches during and after occlusions.
• Combine spatial geometry and adaptive Re-ID features.
• Gap: online methods struggle with dense occlusions.","• Strong multi-camera tracking; not for flying objects.
• Assumes known cameras; not calibration-free.
• Ground-plane homography unsuitable for aerial motion.
• No 3D occupancy or Gaussian Splatting.
• Useful geometric cues for RGB-only multi-view.
• Open: robustness to calibration drift and desync.","• Weighted affinity: 2D IOU, epipolar, homography, Re-ID.
• Hungarian matching for per-view bipartite association.
• Kalman filter predicts bounding boxes per camera.
• Cycle-consistent graph partitioning for initialization.
• Cosine similarity Re-ID with feature banks.
• Coverage rate to assess occlusion severity.","• AIC24 MCPT synthetic surveillance scenes.
• 90 scenes: 40 train, 20 val, 30 test.
• ~1,300 cameras, overlapping multi-view setups.
• 1080p, 30fps, cross-view tracking annotations.
• 3D annotations and camera matrices provided.
• Availability: AI City Challenge dataset.","• YOLOX CrowdHuman detector for pedestrians.
• HRNet 2D keypoints via MMPose.
• MGN-R101 Re-ID, finetuned on AIC24.
• Geometric affinities: IOU, epipolar rays, homography.
• State-aware Re-ID with feature bank.
• Triangulation updates 3D keypoints; Hungarian association.","• HOTA 67.22%, second in AIC24 Track1.
• AssA improved from 30.24% to 55.06%.
• Kalman filter boosted association by ~6 points.
• State-aware affinities yielded ~13 HOTA gain.
• Ablations confirm feature bank improves re-identification.
• Runtime and memory: N/A.",https://github.com/ZhenyuX1E/PoseTrack,,,https://ieeexplore.ieee.org/document/10677892/,https://zotero.org/alehanderoo/items/NWLZB9JD,https://doi.org/10.1109/CVPRW63382.2024.00694,09/10/2025 18:11 (GMT+2)
Yes,PointCNN,"3D features, classify","• Reuse X-Conv for 3D feature extraction.
• Apply to 4DGS splat point embeddings.
• Classify flying objects from GS reconstructions.
• Risk: needs 3D points; RGB-only requires reconstruction.
• Quick win: sample GS points, train PointCNN head.","• Convolution for unordered point clouds is ill-posed.
• Direct convolution loses shape, depends on ordering.
• Propose X-Conv to weight and permute features.
• Aim: order invariance with local shape sensitivity.
• Bridge CNNs to irregular 3D point sets.","• Not multiview; no pose, flow, tracking.
• Strong 3D features complement 4DGS pipelines.
• Lacks explainability modules like Grad-CAM.
• Good for per-object embeddings after reconstruction.
• Explore integrating X-Conv within voxel/GS representations.","• Learn K×K X-transformation from local coordinates.
• X weights features and permutes canonical ordering.
• Equivariance objective; related to Spatial Transformer Networks.
• Use MLPδ to lift coordinates into features.
• Equation 1b formalizes X-Conv weighting and permutation.","• Datasets: ModelNet40, ShapeNet Parts, S3DIS, ScanNet.
• Tasks: classification, segmentation; static scenes.
• No multi-view cameras; single point cloud input.
• Annotations: class, part, semantic labels.
• 3D motion absent; no temporal sequences.
• Public benchmarks; permissive academic licenses.","• Hierarchical X-Conv aggregating K-neighborhoods into representatives.
• Self-calibration/pose: N/A; no camera geometry.
• 2D backbones/flow: N/A; point MLPs only.
• 3D aggregation via learned X and separable convolutions.
• Heads: classification softmax; segmentation decoder with skip links.
• Training: ADAM, dropout, augmentation, dilation, farthest sampling.","• ModelNet40 OA 92.2%, competitive with SOTA.
• ShapeNet Parts pIoU 86.14%, mpIoU 84.6%.
• S3DIS mIoU 65.39%, notable improvements.
• Ablations show X-Conv crucial for accuracy.
• Inference 0.012s/batch on P100; 0.6M parameters.",https://github.com/yangyanli/PointCNN,,,http://arxiv.org/abs/1801.07791,https://zotero.org/alehanderoo/items/CN8IIRCL,https://doi.org/10.48550/arXiv.1801.07791,09/10/2025 17:37 (GMT+2)
Yes,Deep Learning-based 3D Point Cloud Classification,"2D features, 3D features, classify","• Borrow point-based transformers for 3DGS object classification.
• Test Point-BERT/Point-MAE on reconstructed dynamic objects.
• Use multi-view CNNs leveraging overlapping RGB cameras.
• Combine CLIP-based PointCLIP for label-efficient classification.
• Beware LiDAR-centric assumptions; adapt to RGB-derived geometry.
• No tracking/pose modules; integrate separate tracking components.","• Survey deep learning 3D point cloud classification.
• Address irregular, unordered, sparse point cloud challenges.
• Organize methods: multi-view, voxel, point-based, fusion.
• Compare performance across standard 3D datasets.
• Discuss challenges, trends, future research directions.","• Good for classification module selection and benchmarking.
• Mismatch: no multi-camera calibration or pose estimation.
• No occupancy reconstruction, Gaussian splatting, or tracking.
• Multi-view insights map to overlapping camera setups.
• Robustness datasets inspire evaluation under photogrammetry noise.
• Lacks explainability tools; consider Grad-CAM on 2D views.","• Taxonomy: multi-view, voxel, point-based, polymorphic fusion.
• Invariances: permutation, rotation, robustness considerations.
• Graph, attention, transformer paradigms summarized.
• Evaluation metrics: OA, MA, mIoU definitions provided.
• Feature aggregation: local and global strategies.","• Datasets: ModelNet40/10, ShapeNet, ScanNet, ScanObjectNN.
• Synthetic CAD and real-world RGB-D collections.
• Annotation: object class labels, some segmentation labels.
• #samples: ModelNet40 12,311; ScanObjectNN 2,902.
• 3D-motion present? Mostly static object instances.
• Cameras/overlap details: N/A.","• Multi-view: render views, CNN feature fusion.
• Voxel: 3D CNNs, octree, kd-tree efficiency.
• Point-based: PointNet++, DGCNN, attention modules.
• Transformers: PCT, Point-BERT, Point-MAE trends.
• Heads: classification only; no tracking modules.
• Pose strategy: N/A; optical flow: N/A.","• Recent methods reach strong ModelNet40 OA (~95%+).
• Transformers and PointMLP show competitive accuracy.
• Real-world ScanObjectNN accuracies notably lower.
• Efficiency improved with octree/kd-tree voxelization.
• Robustness addressed via ModelNet-C corruptions.
• Runtime/memory comparisons generally not reported.",,,,http://arxiv.org/abs/2311.02608,https://zotero.org/alehanderoo/items/Q6IMQ8CS,https://doi.org/10.1016/j.displa.2023.102456,09/10/2025 17:35 (GMT+2)
No,Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting,,,,,,,,,,,,http://arxiv.org/abs/2406.01042,https://zotero.org/alehanderoo/items/QM2CFQX3,https://doi.org/10.48550/arXiv.2406.01042,
No,DepthSplat: Connecting Gaussian Splatting and Depth,,,,,,,,,,,,,https://zotero.org/alehanderoo/items/JERXDLR6,,
No,DepthSplat: Connecting Gaussian Splatting and Depth,,,,,,,,,,,,,https://zotero.org/alehanderoo/items/P94RU3SA,,
No,MVSplat,,,,,,,,,,,,http://arxiv.org/abs/2403.14627,https://zotero.org/alehanderoo/items/ULQU8WJY,,
No,btsmart/splatt3r,,,,,,,,,,,,https://github.com/btsmart/splatt3r,https://zotero.org/alehanderoo/items/PNSPTL7J,,
No,DUSt3R,,,,,,,,,,,,http://arxiv.org/abs/2312.14132,https://zotero.org/alehanderoo/items/BPYAP6WW,https://doi.org/10.48550/arXiv.2312.14132,
No,MUSt3R,,,,,,,,,,,,http://arxiv.org/abs/2503.01661,https://zotero.org/alehanderoo/items/NTK88DEF,https://doi.org/10.48550/arXiv.2503.01661,
No,VGGT,,,,,,,,,,,,http://arxiv.org/abs/2503.11651,https://zotero.org/alehanderoo/items/UTVP5Y3L,https://doi.org/10.48550/arXiv.2503.11651,
No,4D Spatio-Temporal ConvNets,,,,,,,,,,,,https://ieeexplore.ieee.org/document/8953494/,https://zotero.org/alehanderoo/items/ZTS7F7EY,https://doi.org/10.1109/CVPR.2019.00319,
No,Occupancy as Set of Points,,,,,,,,,,,,https://link.springer.com/10.1007/978-3-031-73030-6_5,https://zotero.org/alehanderoo/items/GI7RHZFQ,,
No,GIFStream,,,,,,,,,,,,http://arxiv.org/abs/2505.07539,https://zotero.org/alehanderoo/items/5BU2PB5T,https://doi.org/10.48550/arXiv.2505.07539,
No,Multimodal dataset for indoor 3D drone tracking,,"• Use dataset to test multicam triangulation pipeline.
• Leverage ArUco FPV for inside-out pose validation.
• Train modern detectors; compare with provided YOLOv5.
• Generate optical flow; evaluate temporal features.
• Test calibration-free pose using dynamic drone cues.
• Beware limited camera count, 25 fps constraints.","• Provide multimodal indoor 3D drone tracking dataset.
• Address lack of multi-camera, multi-drone indoor data.
• Enable outside-in and inside-out 3D localization.
• Demonstrate triangulation and PSO tracking baselines.
• Automate labels using mocap for detector training.","• Strong fit: flying, multicam, indoor 3D motion.
• No occupancy/4DGS; add our reconstruction modules.
• Good for association via epipolar baselines.
• Sim-real pairs enable domain gap studies.
• Open: overlap geometry details, occlusion statistics.
• Verify synchronization drift and timestamp accuracy.","• Pinhole camera projection, K[R|t] formulation.
• Overdetermined linear triangulation via pseudoinverse.
• Epipolar geometry for cross-view association.
• PSO optimization with silhouette overlap fitness.
• P3P for FPV pose from ArUco.","• Indoor lab hall, controlled drone flights.
• #sequences: 31 (18 real, 13 sim).
• #cameras: 4 real corners; 8 sim viewpoints.
• Overlapping central volume: yes, synchronized views.
• 3D motion present: flying drones, varied patterns.
• Annotations: Vicon, YOLO, masks; CC BY, Zenodo.","• Pipeline: detect drones, associate, triangulate 3D.
• Pose strategy: calibrated extrinsics via OpenCV; not self-calibrated.
• 2D backbone: YOLOv5; background subtraction optional.
• 3D fusion: linear triangulation; PSO multi-drone optimization.
• Heads: tracking association via epipolar constraints.
• Training: auto 2D bboxes from Vicon projections.","• Triangulation mean errors: 0.075–0.325 m typical.
• PSO best error: 0.068 m (4 drones).
• PSO degrades with 10 drones: 0.193–0.510 m.
• FPV ArUco trajectory mean error: 0.042–0.054 m.
• Swaps cause anomalies; medians remain low.
• Runtime/memory: N/A.",,,,https://www.nature.com/articles/s41597-025-04521-y,https://zotero.org/alehanderoo/items/QMUFILWB,https://doi.org/10.1038/s41597-025-04521-y,