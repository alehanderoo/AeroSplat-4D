
@book{hartley2004,
  title     = {Multiple View Geometry in Computer Vision},
  author    = {Hartley, Richard and Zisserman, Andrew},
  edition   = {2},
  year      = {2004},
  publisher = {Cambridge University Press},
  address   = {Cambridge, UK},
  isbn      = {0521540518},
  doi       = {10.1017/CBO9780511811685}
}

@book{faugeras1993,
  title     = {Three-Dimensional Computer Vision: A Geometric Viewpoint},
  author    = {Faugeras, Olivier},
  year      = {1993},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {0262061589}
}

@article{zhang2000,
  title     = {A flexible new technique for camera calibration},
  author    = {Zhang, Zhengyou},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {22},
  number    = {11},
  pages     = {1330--1334},
  year      = {2000},
  publisher = {IEEE},
  doi       = {10.1109/34.888718}
}

@article{tsai1987,
  title     = {A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses},
  author    = {Tsai, Roger Y.},
  journal   = {IEEE Journal of Robotics and Automation},
  volume    = {3},
  number    = {4},
  pages     = {323--344},
  year      = {1987},
  publisher = {IEEE},
  doi       = {10.1109/JRA.1987.1087109}
}

@article{longuethiggins1981,
  title     = {A computer algorithm for reconstructing a scene from two projections},
  author    = {Longuet-Higgins, H. Christopher},
  journal   = {Nature},
  volume    = {293},
  number    = {5828},
  pages     = {133--135},
  year      = {1981},
  publisher = {Nature Publishing Group},
  doi       = {10.1038/293133a0},
  abstract  = {A simple algorithm for computing the three-dimensional structure of a scene from a correlated pair of perspective projections is described here, when the spatial relationship between the two projections is unknown. This problem is relevant not only to photographic surveying but also to binocular vision, where the non-visual information available to the observer about the orientation and focal length of each eye is much less accurate than the optical information supplied by the retinal images themselves.}
}

@article{laurentini1994,
  title     = {The visual hull concept for silhouette-based image understanding},
  author    = {Laurentini, Aldo},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {16},
  number    = {2},
  pages     = {150--162},
  year      = {1994},
  publisher = {IEEE},
  doi       = {10.1109/34.273735},
  abstract  = {This paper addresses the problem of finding which parts of a nonconvex object are relevant for silhouette-based image understanding. For this purpose, the geometric concept of visual hull of a 3-D object is introduced. This is the closest approximation of object S that can be obtained with the volume intersection approach; it is the maximal object silhouette-equivalent to S.}
}

@article{kutulakos2000,
  title     = {A theory of shape by space carving},
  author    = {Kutulakos, Kiriakos N. and Seitz, Steven M.},
  journal   = {International Journal of Computer Vision},
  volume    = {38},
  number    = {3},
  pages     = {199--218},
  year      = {2000},
  publisher = {Springer},
  doi       = {10.1023/A:1008191222954},
  abstract  = {This paper introduces a novel shape reconstruction framework called Space Carving. Unlike traditional approaches that incrementally add to a reconstruction, Space Carving starts with a volume that contains the true scene and progressively carves away portions of the volume that are inconsistent with the input photographs, until it converges to a maximal photo-consistent shape called the photo hull.}
}

@inproceedings{kerbl20233dgs,
  title     = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
  author    = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  booktitle = {ACM Transactions on Graphics},
  volume    = {42},
  number    = {4},
  year      = {2023},
  publisher = {ACM},
  doi       = {10.1145/3592433},
  url       = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/},
  abstract  = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution.}
}

@inproceedings{mildenhall2020nerf,
  title     = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  author    = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  booktitle = {European Conference on Computer Vision (ECCV)},
  pages     = {405--421},
  year      = {2020},
  publisher = {Springer},
  doi       = {10.1007/978-3-030-58452-8_24},
  abstract  = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (θ,φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location.}
}


@inproceedings{barron_mip-nerf_2021,
  address    = {Montreal, QC, Canada},
  title      = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-6654-2812-5},
  shorttitle = {Mip-{NeRF}},
  url        = {https://ieeexplore.ieee.org/document/9710056/},
  doi        = {10.1109/ICCV48922.2021.00580},
  abstract   = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call mip-NeRF (`a la mipmap), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRFs ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22Ã— faster.},
  language   = {en},
  urldate    = {2025-12-03},
  booktitle  = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  publisher  = {IEEE},
  author     = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  month      = oct,
  year       = {2021},
  pages      = {5835--5844},
  file       = {PDF:/home/sandro/Zotero/storage/YM9L5LA3/Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Aliasing Neural Radiance Fields.pdf:application/pdf}
}



@inproceedings{zwicker2002ewa,
  title     = {EWA Splatting},
  author    = {Zwicker, Matthias and Pfister, Hanspeter and Van Baar, Jeroen and Gross, Markus},
  booktitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume    = {8},
  number    = {3},
  pages     = {223--238},
  year      = {2002},
  publisher = {IEEE},
  doi       = {10.1109/TVCG.2002.1021576},
  abstract  = {We present a framework for high-quality splatting based on elliptical Gaussian kernels. To avoid aliasing artifacts, we introduce the concept of a resampling filter, combining a reconstruction kernel with a low-pass prefilter. We propose a novel EWA (elliptical weighted average) filter obtained by integrating the ideal resampling filter over the footprint of the reconstruction kernel.}
}

@article{wang2004ssim,
  title     = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  author    = {Wang, Zhou and Bovik, Alan C. and Sheikh, Hamid R. and Simoncelli, Eero P.},
  journal   = {IEEE Transactions on Image Processing},
  volume    = {13},
  number    = {4},
  pages     = {600--612},
  year      = {2004},
  publisher = {IEEE},
  doi       = {10.1109/TIP.2003.819861},
  abstract  = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. We propose a new philosophy in quality assessment based on the hypothesis that the human visual system is highly adapted for extracting structural information from a scene.}
}


@article{luiten_hota_2021,
  title      = {{HOTA}: {A} {Higher} {Order} {Metric} for {Evaluating} {Multi}-{Object} {Tracking}},
  volume     = {129},
  issn       = {0920-5691, 1573-1405},
  shorttitle = {{HOTA}},
  url        = {http://arxiv.org/abs/2009.07736},
  doi        = {10.1007/s11263-020-01375-2},
  abstract   = {Multi-Object Tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, HOTA (Higher Order Tracking Accuracy), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.},
  number     = {2},
  urldate    = {2025-09-10},
  journal    = {International Journal of Computer Vision},
  author     = {Luiten, Jonathon and Osep, Aljosa and Dendorfer, Patrick and Torr, Philip and Geiger, Andreas and Leal-Taixe, Laura and Leibe, Bastian},
  month      = feb,
  year       = {2021},
  note       = {arXiv:2009.07736 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, notion},
  pages      = {548--578},
  annote     = {Comment: Pre-print. Accepted for Publication in the International Journal of Computer Vision, 19 August 2020. Code is available at https://github.com/JonathonLuiten/HOTA-metrics},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/88UGE2BL/Luiten et al. - 2021 - HOTA A Higher Order Metric for Evaluating Multi-Object Tracking.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/B2UT86AZ/2009.html:text/html}
}

@inproceedings{vaswani_attention_2017,
  title     = {Attention is {All} you {Need}},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  abstract  = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  urldate   = {2025-12-02},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Å ukasz and Polosukhin, Illia},
  year      = {2017},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/QXZ6DELA/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey},
  journal = {arXiv preprint arXiv:2010.11929},
  year    = {2020}
}
@misc{thomas_tensor_2018,
  title      = {Tensor field networks: {Rotation}- and translation-equivariant neural networks for {3D} point clouds},
  shorttitle = {Tensor field networks},
  url        = {http://arxiv.org/abs/1802.08219},
  doi        = {10.48550/arXiv.1802.08219},
  abstract   = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
  urldate    = {2025-12-02},
  publisher  = {arXiv},
  author     = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  month      = may,
  year       = {2018},
  note       = {arXiv:1802.08219 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  annote     = {Comment: changes for NIPS submission},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/6FU8BVV7/Thomas et al. - 2018 - Tensor field networks Rotation- and translation-equivariant neural networks for 3D point clouds.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/58YVRGIP/1802.html:text/html}
}



@inproceedings{zaheer_deep_2017,
  title     = {Deep {Sets}},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  abstract  = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  urldate   = {2025-12-02},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  year      = {2017},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/CF5I6AYY/Zaheer et al. - 2017 - Deep Sets.pdf:application/pdf}
}

@inproceedings{lee_set_2019,
  title      = {Set {Transformer}: {A} {Framework} for {Attention}-based {Permutation}-{Invariant} {Neural} {Networks}},
  shorttitle = {Set {Transformer}},
  url        = {https://proceedings.mlr.press/v97/lee19d.html},
  abstract   = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  language   = {en},
  urldate    = {2025-12-02},
  booktitle  = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
  publisher  = {PMLR},
  author     = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  month      = may,
  year       = {2019},
  note       = {ISSN: 2640-3498},
  pages      = {3744--3753},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/8ZFFP3H8/Lee et al. - 2019 - Set Transformer A Framework for Attention-based Permutation-Invariant Neural Networks.pdf:application/pdf}
}

@inproceedings{jaegle_perceiver_2021,
  title      = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
  shorttitle = {Perceiver},
  url        = {https://proceedings.mlr.press/v139/jaegle21a.html},
  abstract   = {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver \{“\} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  language   = {en},
  urldate    = {2025-12-02},
  booktitle  = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
  publisher  = {PMLR},
  author     = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  month      = jul,
  year       = {2021},
  note       = {ISSN: 2640-3498},
  pages      = {4651--4664},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/R45IYIMC/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Attention.pdf:application/pdf}
}

@article{gu_linear-time_2024,
  title    = {Linear-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã— higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  language = {en},
  author   = {Gu, Albert and Dao, Tri},
  year     = {2024},
  file     = {PDF:/home/sandro/Zotero/storage/4SWV8DTN/Gu and Dao - 2024 - Linear-Time Sequence Modeling with Selective State Spaces.pdf:application/pdf}
}

@inproceedings{liang_pointmamba_2024,
  title     = {{PointMamba}: {A} {Simple} {State} {Space} {Model} for {Point} {Cloud} {Analysis}},
  volume    = {37},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/395371f778ebd4854b88521100af30ad-Paper-Conference.pdf},
  doi       = {10.52202/079017-1026},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Liang, Dingkang and Zhou, Xin and Xu, Wei and Zhu, Xingkui and Zou, Zhikang and Ye, Xiaoqing and Tan, Xiao and Bai, Xiang},
  editor    = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year      = {2024},
  pages     = {32653--32677}
}


@inproceedings{schonberger_structure--motion_2016,
  address   = {Las Vegas, NV, USA},
  title     = {Structure-from-{Motion} {Revisited}},
  isbn      = {978-1-4673-8851-1},
  url       = {http://ieeexplore.ieee.org/document/7780814/},
  doi       = {10.1109/CVPR.2016.445},
  abstract  = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
  language  = {en},
  urldate   = {2025-12-03},
  booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author    = {Schonberger, Johannes L. and Frahm, Jan-Michael},
  month     = jun,
  year      = {2016},
  pages     = {4104--4113},
  file      = {PDF:/home/sandro/Zotero/storage/6JZU23IS/Schonberger and Frahm - 2016 - Structure-from-Motion Revisited.pdf:application/pdf}
}

@inproceedings{schonberger_pixelwise_2016,
  address   = {Cham},
  title     = {Pixelwise {View} {Selection} for {Unstructured} {Multi}-{View} {Stereo}},
  isbn      = {978-3-319-46487-9},
  doi       = {10.1007/978-3-319-46487-9_31},
  abstract  = {This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and efficiency.},
  language  = {en},
  booktitle = {Computer {Vision} “ {ECCV} 2016},
  publisher = {Springer International Publishing},
  author    = {SchÃ¶nberger, Johannes L. and Zheng, Enliang and Frahm, Jan-Michael and Pollefeys, Marc},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year      = {2016},
  keywords  = {Reprojection Error, Scene Representation, Source Image, Source Patch, View Selection},
  pages     = {501--518},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/CFFY2VPM/SchÃ¶nberger et al. - 2016 - Pixelwise View Selection for Unstructured Multi-View Stereo.pdf:application/pdf}
}

@misc{IEC62676,
  title    = {{IEC} 62676 – {Video} {Surveillance}},
  url      = {https://www.imatest.com/imaging/iec-62676/},
  language = {en-US},
  urldate  = {2025-12-03},
  file     = {Snapshot:/home/sandro/Zotero/storage/XJ8H7ZR5/iec-62676.html:text/html}
}
@article{ShannonVision,
  title     = {Communication in the presence of noise},
  author    = {Shannon, Claude E.},
  journal   = {Proceedings of the IRE},
  volume    = {37},
  number    = {1},
  pages     = {10--21},
  year      = {1949},
  publisher = {IEEE}
}
@article{ma_remote_2023,
  title     = {Remote {Sensing} {Low} {Signal}-to-{Noise}-{Ratio} {Target} {Detection} {Enhancement}},
  volume    = {23},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {1424-8220},
  url       = {https://www.mdpi.com/1424-8220/23/6/3314},
  doi       = {10.3390/s23063314},
  abstract  = {In real-time remote sensing application, frames of data are continuously flowing into the processing system. The capability of detecting objects of interest and tracking them as they move is crucial to many critical surveillance and monitoring missions. Detecting small objects using remote sensors is an ongoing, challenging problem. Since object(s) are located far away from the sensor, the targets Signal-to-Noise-Ratio (SNR) is low. The Limit of Detection (LOD) for remote sensors is bounded by what is observable on each image frame. In this paper, we present a new method, a Multi-frame Moving Object Detection System (MMODS), to detect small, low SNR objects that are beyond what a human can observe in a single video frame. This is demonstrated by using simulated data where our technology-detected objects are as small as one pixel with a targeted SNR, close to 1:1. We also demonstrate a similar improvement using live data collected with a remote camera. The MMODS technology fills a major technology gap in remote sensing surveillance applications for small target detection. Our method does not require prior knowledge about the environment, pre-labeled targets, or training data to effectively detect and track slow- and fast-moving targets, regardless of the size or the distance.},
  language  = {en},
  number    = {6},
  urldate   = {2025-12-03},
  journal   = {Sensors},
  author    = {Ma, Tian J. and Anderson, Robert J.},
  month     = jan,
  year      = {2023},
  note      = {Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {low signal-to-noise ratio target detection, low signal-to-noise-ratio target tracking, low SNR target detection, low SNR target tracking, MMODS, multi-frame moving object detection system, remote sensing, remote sensing detection, target detection},
  pages     = {3314},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/P3AN882J/Ma and Anderson - 2023 - Remote Sensing Low Signal-to-Noise-Ratio Target Detection Enhancement.pdf:application/pdf}
}




@inproceedings{redmon_yolo_2016,
  address    = {Las Vegas, NV, USA},
  title      = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
  isbn       = {978-1-4673-8851-1},
  shorttitle = {You {Only} {Look} {Once}},
  url        = {http://ieeexplore.ieee.org/document/7780460/},
  doi        = {10.1109/CVPR.2016.91},
  abstract   = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiï¬ers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  language   = {en},
  urldate    = {2025-12-09},
  booktitle  = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month      = jun,
  year       = {2016},
  pages      = {779--788},
  file       = {PDF:/home/sandro/Zotero/storage/AZ6ITLBJ/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:application/pdf}
}

@inproceedings{coluccia_drone-vs-bird_2025,
  title     = {The {Drone}-vs-{Bird} {Detection} {Grand} {Challenge} at {IJCNN} 2025},
  url       = {https://ieeexplore.ieee.org/document/11228314/},
  doi       = {10.1109/IJCNN64981.2025.11228314},
  abstract  = {The widespread adoption of Unmanned Aerial Vehicles (UAVs) has raised critical security and safety concerns, particularly in sensitive areas and air traffic management. Modern counter-drone systems integrate multiple sensing modalities, but their development is hindered by the lack of comprehensive, publicly available datasets. To address this, the Drone-vs-Bird Detection Grand Challenge provides a manually annotated UAV dataset to advance research in drone detection. Since its inception in 2017, the competition has attracted global interest, fostering the development of advanced detection methods. This paper presents an overview of the 8th edition as data competition hosted at the International Joint Conference on Neural Networks (IJCNN) 2025. The data competition generated high engagement with 16 competing algorithms successfully submitted. The variability of the results underscores the complexity of the task and the need for future research. Over almost a decade, this data competition has been bridging the domains of signal processing, computer vision, and deep learning, paving the way for next-generation counter-drone solutions.},
  urldate   = {2025-12-09},
  booktitle = {2025 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
  author    = {Coluccia, Angelo and Fascista, Alessio and Dimou, Anastasios and Zarpalas, Dimitrios and Sommer, Lars and Schumann, Arne and Mele, Emanuele},
  month     = jun,
  year      = {2025},
  note      = {ISSN: 2161-4407},
  keywords  = {Autonomous aerial vehicles, deep learning, Deep learning, drone detection, Drones, image and video signal processing, Neural networks, Security, Sensors, Signal processing, Signal processing algorithms, Vegetation mapping, Video signal processing},
  pages     = {1--8},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/AV5HGA43/Coluccia et al. - 2025 - The Drone-vs-Bird Detection Grand Challenge at IJCNN 2025.pdf:application/pdf}
}

@inproceedings{magoulianitis_does_2019,
  address   = {Taipei, Taiwan},
  title     = {Does {Deep} {Super}-{Resolution} {Enhance} {UAV} {Detection}?},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn      = {978-1-7281-0990-9},
  url       = {https://ieeexplore.ieee.org/document/8909865/},
  doi       = {10.1109/AVSS.2019.8909865},
  abstract  = {The popularity of Unmanned Aerial Vehicles (UAVs) is increasing year by year and reportedly their applications hold great shares in global technology market. Yet, since UAVs can be also used for illegal actions, this raises various security issues that needs to be encountered. Towards this end, UAV detection systems have emerged to detect and further anticipate inimical drones. A very signiï¬cant factor is the maximum detection range in which the systems senses can see an upcoming UAV. For those systems that employ optical cameras for detecting UAVs, the main issue is the accurate drone detection when it fades away into sky. This work proposes the incorporation of Super-Resolution (SR) techniques in the detection pipeline, to increase its recall capabilities. A deep SR model is utilized prior to the UAV detector to enlarge the image by a factor of 2. Both models are trained in an end-to-end manner to fully exploit the joint optimization effects. Extensive experiments demonstrate the validity of the proposed method, where potential gains in the detectors recall performance can reach up to 32.4\%.},
  language  = {en},
  urldate   = {2025-08-12},
  booktitle = {2019 16th {IEEE} {International} {Conference} on {Advanced} {Video} and {Signal} {Based} {Surveillance} ({AVSS})},
  publisher = {IEEE},
  author    = {Magoulianitis, Vasileios and Ataloglou, Dimitrios and Dimou, Anastasios and Zarpalas, Dimitrios and Daras, Petros},
  month     = sep,
  year      = {2019},
  keywords  = {notion},
  pages     = {1--6},
  file      = {PDF:/home/sandro/Zotero/storage/UH7AWVTG/Magoulianitis et al. - 2019 - Does Deep Super-Resolution Enhance UAV Detection.pdf:application/pdf}
}


@article{barniv_dynamic_1985,
  title    = {Dynamic {Programming} {Solution} for {Detecting} {Dim} {Moving} {Targets}},
  volume   = {AES-21},
  issn     = {1557-9603},
  url      = {https://ieeexplore.ieee.org/document/4104027/},
  doi      = {10.1109/TAES.1985.310548},
  abstract = {An on-board scanning or mosaic sensor is staring down from a satellite to a fixed ground point while producing a set of frames that contain target and background signals. Detecting dim moving targets should ideally be done by exhaustively searching over all in he maery(mached fltering), as ppoed possible trajectories in the imagery (matched filtering) as opposed to assembling trajectories from thresholded frames. The dynamic programming approach equivalently substitutes the above prohibitive exhaustive search by a feasible algorithm.},
  number   = {1},
  urldate  = {2025-12-09},
  journal  = {IEEE Transactions on Aerospace and Electronic Systems},
  author   = {Barniv, Yair},
  month    = jan,
  year     = {1985},
  keywords = {Dynamic programming, Filters, Frequency estimation, Image processing, Image sensors, Probability, Signal processing, Signal processing algorithms, Thermal sensors, Trajectory},
  pages    = {144--156},
  file     = {Full Text PDF:/home/sandro/Zotero/storage/9THKHHZU/Barniv - 1985 - Dynamic Programming Solution for Detecting Dim Moving Targets.pdf:application/pdf}
}


@inproceedings{salmond_particle_2001,
  title     = {A particle filter for track-before-detect},
  volume    = {5},
  url       = {https://ieeexplore.ieee.org/document/946220/},
  doi       = {10.1109/ACC.2001.946220},
  abstract  = {A Bayesian track-before-detect particle filter is proposed. The filter provides a sample based approximation to the distribution of the target state directly from pixel array data. The filter also provides a measure of the probability that a target is present.},
  urldate   = {2025-12-09},
  booktitle = {Proceedings of the 2001 {American} {Control} {Conference}. ({Cat}. {No}.{01CH37148})},
  author    = {Salmond, D.J. and Birch, H.},
  month     = jun,
  year      = {2001},
  note      = {ISSN: 0743-1619},
  keywords  = {Background noise, Bayesian methods, Data mining, Maximum likelihood detection, Particle filters, Particle tracking, Position measurement, Sensor arrays, Target tracking, Time measurement},
  pages     = {3755--3760 vol.5},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/DEB384GH/Salmond and Birch - 2001 - A particle filter for track-before-detect.pdf:application/pdf}
}

@article{meyer_message_2018,
  title    = {Message {Passing} {Algorithms} for {Scalable} {Multitarget} {Tracking}},
  volume   = {106},
  issn     = {1558-2256},
  url      = {https://ieeexplore.ieee.org/document/8290605/},
  doi      = {10.1109/JPROC.2018.2789427},
  abstract = {Situation-aware technologies enabled by multitarget tracking will lead to new services and applications in fields such as autonomous driving, indoor localization, robotic networks, and crowd counting. In this tutorial paper, we advocate a recently proposed paradigm for scalable multitarget tracking that is based on message passing or, more concretely, the loopy sum-product algorithm. This approach has advantages regarding estimation accuracy, computational complexity, and implementation flexibility. Most importantly, it provides a highly effective, efficient, and scalable solution to the probabilistic data association problem, a major challenge in multitarget tracking. This fact makes it attractive for emerging applications requiring real-time operation on resource-limited devices. In addition, the message passing approach is intuitively appealing and suited to nonlinear and non-Gaussian models. We present message-passing-based multitarget tracking methods for single-sensor and multiple-sensor scenarios, and for a known and unknown number of targets. The presented methods can cope with clutter, missed detections, and an unknown association between targets and measurements. We also discuss the integration of message-passing-based probabilistic data association into existing multitarget tracking methods. The superior performance, low complexity, and attractive scaling properties of the presented methods are verified numerically. In addition to simulated data, we use measured data captured by two radar stations with overlapping fields-of-view observing a large number of targets simultaneously.},
  number   = {2},
  urldate  = {2025-12-09},
  journal  = {Proceedings of the IEEE},
  author   = {Meyer, Florian and Kropfreiter, Thomas and Williams, Jason L. and Lau, Roslyn and Hlawatsch, Franz and Braca, Paolo and Win, Moe Z.},
  month    = feb,
  year     = {2018},
  keywords = {Biomedical measurement, Data association, data fusion, factor graph, message passing, Message passing, multitarget tracking, Radar tracking, Sensors, sum“product algorithm, Target tracking, Time measurement},
  pages    = {221--259}
}

@inproceedings{rozantsev_flight_2017,
  title     = {Flight {Dynamics}-{Based} {Recovery} of a {UAV} {Trajectory} {Using} {Ground} {Cameras}},
  url       = {https://ieeexplore.ieee.org/document/8099749/},
  doi       = {10.1109/CVPR.2017.266},
  abstract  = {We propose a new method to estimate the 6-dof trajectory of a flying object such as a quadrotor UAV within a 3D airspace monitored using multiple fixed ground cameras. It is based on a new structure from motion formulation for the 3D reconstruction of a single moving point with known motion dynamics. Our main contribution is a new bundle adjustment procedure, which in addition to optimizing the camera poses, regularizes the point trajectory using a prior based on motion dynamics (or specifically flight dynamics). Furthermore, we can infer the underlying control input sent to the UAVs autopilot that determined its flight trajectory. Our method requires neither perfect single-view tracking nor appearance matching across views. For robustness, we allow the tracker to generate multiple detections per frame in each video. The true detections and the data association across videos is estimated using robust multi-view triangulation and subsequently refined in our bundle adjustment formulation. Quantitative evaluation on simulated data and experiments on real videos from indoor and outdoor scenes shows that our technique is superior to existing methods.},
  urldate   = {2025-09-11},
  booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  author    = {Rozantsev, Artem and Sinha, Sudipta N. and Dey, Debadeepta and Fua, Pascal},
  month     = jul,
  year      = {2017},
  note      = {ISSN: 1063-6919},
  keywords  = {Cameras, Target tracking, Trajectory, Drones, notion, Dynamics},
  pages     = {2482--2491},
  file      = {Submitted Version:/home/sandro/Zotero/storage/B6AT2RSF/Rozantsev et al. - 2017 - Flight Dynamics-Based Recovery of a UAV Trajectory Using Ground Cameras.pdf:application/pdf}
}

@misc{li_reconstruction_2020,
  title     = {Reconstruction of {3D} flight trajectories from ad-hoc camera networks},
  url       = {http://arxiv.org/abs/2003.04784},
  doi       = {10.48550/arXiv.2003.04784},
  abstract  = {We present a method to reconstruct the 3D trajectory of an airborne robotic system only from videos recorded with cameras that are unsynchronized, may feature rolling shutter distortion, and whose viewpoints are unknown. Our approach enables robust and accurate outside-in tracking of dynamically ï¬‚ying targets, with cheap and easy-to-deploy equipment. We show that, in spite of the weakly constrained setting, recent developments in computer vision make it possible to reconstruct trajectories in 3D from unsynchronized, uncalibrated networks of consumer cameras, and validate the proposed method in a realistic ï¬eld experiment. We make our code available along with the data, including cm-accurate groundtruth from differential GNSS navigation.},
  language  = {en},
  urldate   = {2025-09-11},
  publisher = {arXiv},
  author    = {Li, Jingtong and Murray, Jesse and Ismaili, Dorina and Schindler, Konrad and Albl, Cenek},
  month     = jul,
  year      = {2020},
  note      = {arXiv:2003.04784 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, notion},
  annote    = {Comment: IROS 2020 camera ready version},
  file      = {PDF:/home/sandro/Zotero/storage/A3TQDTAV/Li et al. - 2020 - Reconstruction of 3D flight trajectories from ad-hoc camera networks.pdf:application/pdf}
}

@article{rosner_multimodal_2025,
  title    = {Multimodal dataset for indoor {3D} drone tracking},
  volume   = {12},
  issn     = {2052-4463},
  url      = {https://www.nature.com/articles/s41597-025-04521-y},
  doi      = {10.1038/s41597-025-04521-y},
  abstract = {The subject of the paper is a multimodal dataset (DPJAIT) containing drone flights prepared in two variants “ simulation-based and with real measurements captured by the gold standard Vicon system. It contains video sequences registered by the synchronized and calibrated multicamera set as well as reference 3D drone positions in successive time instants obtained from simulation procedure or using the motion capture technique. Moreover, there are scenarios with ArUco markers in the scene with known 3D positions and RGB cameras mounted on drones for which internal parameters are given. Three applications of 3D tracking are demonstrated. They are based on the overdetermined set of linear equations describing camera projection, particle swarm optimization, and the determination of the extrinsic matrix of the camera attached to the drone utilizing recognized ArUco markers.},
  language = {en},
  number   = {1},
  urldate  = {2025-09-11},
  journal  = {Scientific Data},
  author   = {Rosner, Jakub and Krzeszowski, Tomasz and Åšwitoski, Adam and Josiski, Henryk and Lindenheim-Locher, Wojciech and Zieliski, MichaÅ‚ and Paleta, Grzegorz and Paszkuta, Marcin and Wojciechowski, Konrad},
  month    = feb,
  year     = {2025},
  keywords = {notion},
  pages    = {257},
  file     = {PDF:/home/sandro/Zotero/storage/N59X3N95/Rosner et al. - 2025 - Multimodal dataset for indoor 3D drone tracking.pdf:application/pdf}
}

@article{lindenheim-locher_yolov5_2023,
  title     = {{YOLOv5} {Drone} {Detection} {Using} {Multimodal} {Data} {Registered} by the {Vicon} {System}},
  volume    = {23},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn      = {1424-8220},
  url       = {https://www.mdpi.com/1424-8220/23/14/6396},
  doi       = {10.3390/s23146396},
  abstract  = {This work is focused on the preliminary stage of the 3D drone tracking challenge, namely the precise detection of drones on images obtained from a synchronized multi-camera system. The YOLOv5 deep network with different input resolutions is trained and tested on the basis of real, multimodal data containing synchronized video sequences and precise motion capture data as a ground truth reference. The bounding boxes are determined based on the 3D position and orientation of an asymmetric cross attached to the top of the tracked object with known translation to the objects center. The arms of the cross are identiï¬ed by the markers registered by motion capture acquisition. Besides the classical mean average precision (mAP), a measure more adequate in the evaluation of detection performance in 3D tracking is proposed, namely the average distance between the centroids of matched references and detected drones, including false positive and false negative ratios. Moreover, the videos generated in the AirSim simulation platform were taken into account in both the training and testing stages.},
  language  = {en},
  number    = {14},
  urldate   = {2025-08-12},
  journal   = {Sensors},
  author    = {Lindenheim-Locher, Wojciech and Åšwitoski, Adam and Krzeszowski, Tomasz and Paleta, Grzegorz and Hasiec, Piotr and Josiski, Henryk and Paszkuta, Marcin and Wojciechowski, Konrad and Rosner, Jakub},
  month     = jul,
  year      = {2023},
  keywords  = {notion},
  pages     = {6396},
  file      = {PDF:/home/sandro/Zotero/storage/9NVQHVB9/Lindenheim-Locher et al. - 2023 - YOLOv5 Drone Detection Using Multimodal Data Registered by the Vicon System.pdf:application/pdf}
}
@inproceedings{yuan_mmaud_2024,
  title      = {{MMAUD}: {A} {Comprehensive} {Multi}-{Modal} {Anti}-{UAV} {Dataset} for {Modern} {Miniature} {Drone} {Threats}},
  shorttitle = {{MMAUD}},
  url        = {https://ieeexplore.ieee.org/document/10610957/},
  doi        = {10.1109/ICRA57147.2024.10610957},
  abstract   = {In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the datasets applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://ntu-aris.github.io/MMAUD.},
  urldate    = {2025-12-09},
  booktitle  = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  author     = {Yuan, Shenghai and Yang, Yizhuo and Nguyen, Thien Hoang and Nguyen, Thien-Minh and Yang, Jianfei and Liu, Fen and Li, Jianping and Wang, Han and Xie, Lihua},
  month      = may,
  year       = {2024},
  keywords   = {Accuracy, Audio, Autonomous aerial vehicles, Classification, Detection, Estimation, Laser radar, LIDAR, Robot sensing systems, Threat assessment, Trajectory, Trajectory Estimation, UAV, video fusion},
  pages      = {2745--2751},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/Y4SNJGVR/Yuan et al. - 2024 - MMAUD A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats.pdf:application/pdf}
}
@misc{deng_multi-modal_2024,
  title     = {Multi-{Modal} {UAV} {Detection}, {Classification} and {Tracking} {Algorithm} -- {Technical} {Report} for {CVPR} 2024 {UG2} {Challenge}},
  url       = {http://arxiv.org/abs/2405.16464},
  doi       = {10.48550/arXiv.2405.16464},
  abstract  = {This technical report presents the 1st winning model for UG2+, a task in CVPR 2024 UAV Tracking and PoseEstimation Challenge. This challenge faces difficulties in drone detection, UAV-type classification, and 2D/3D trajectory estimation in extreme weather conditions with multi-modal sensor information, including stereo vision, various Lidars, Radars, and audio arrays. Leveraging this information, we propose a multi-modal UAV detection, classification, and 3D tracking method for accurate UAV classification and tracking. A novel classification pipeline which incorporates sequence fusion, region of interest (ROI) cropping, and keyframe selection is proposed. Our system integrates cutting-edge classification techniques and sophisticated post-processing steps to boost accuracy and robustness. The designed pose estimation pipeline incorporates three modules: dynamic points analysis, a multi-object tracker, and trajectory completion techniques. Extensive experiments have validated the effectiveness and precision of our approach. In addition, we also propose a novel dataset pre-processing method and conduct a comprehensive ablation study for our design. We finally achieved the best performance in the MMUAD dataset in classification and tracking. The code and configuration of our method are available at https://github.com/dtc111111/Multi-Modal-UAV .},
  language  = {en},
  urldate   = {2025-08-12},
  publisher = {arXiv},
  author    = {Deng, Tianchen and Zhou, Yi and Wu, Wenhua and Li, Mingrui and Huang, Jingwei and Liu, Shuhong and Song, Yanzeng and Zuo, Hao and Wang, Yanbo and Yue, Yutao and Wang, Hesheng and Chen, Weidong},
  month     = may,
  year      = {2024},
  note      = {arXiv:2405.16464 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, notion},
  annote    = {Comment: Accepted by CVPR 2024 workshop. The 1st winning model in CVPR 2024 UG2+ challenge. The code and configuration of our method are available at https://github.com/dtc111111/Multi-Modal-UAV},
  file      = {PDF:/home/sandro/Zotero/storage/32SREMYI/Deng et al. - 2024 - Multi-Modal UAV Detection, Classification and Tracking Algorithm -- Technical Report for CVPR 2024 U.pdf:application/pdf}
}



@inproceedings{zivkovic_improved_2004,
  title     = {Improved adaptive {Gaussian} mixture model for background subtraction},
  volume    = {2},
  url       = {https://ieeexplore.ieee.org/document/1333992/},
  doi       = {10.1109/ICPR.2004.1333992},
  abstract  = {Background subtraction is a common computer vision task. We analyze the usual pixel-level approach. We develop an efficient adaptive algorithm using Gaussian mixture probability density. Recursive equations are used to constantly update the parameters and but also to simultaneously select the appropriate number of components for each pixel.},
  urldate   = {2025-12-08},
  booktitle = {Proceedings of the 17th {International} {Conference} on {Pattern} {Recognition}, 2004. {ICPR} 2004.},
  author    = {Zivkovic, Z.},
  month     = aug,
  year      = {2004},
  note      = {ISSN: 1051-4651},
  keywords  = {Adaptive algorithm, Cameras, Computer vision, Density functional theory, Equations, Intelligent systems, Layout, Object detection, Pixel, Surveillance},
  pages     = {28--31 Vol.2},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/RF8VSIMY/Zivkovic - 2004 - Improved adaptive Gaussian mixture model for background subtraction.pdf:application/pdf}
}

@inproceedings{stauffer_adaptive_1999,
  title     = {Adaptive background mixture models for real-time tracking},
  volume    = {2},
  url       = {https://ieeexplore.ieee.org/document/784637/},
  doi       = {10.1109/CVPR.1999.784637},
  abstract  = {A common method for real-time segmentation of moving regions in image sequences involves "background subtraction", or thresholding the error between an estimate of the image without moving objects and the current image. The numerous approaches to this problem differ in the type of background model used and the procedure used to update the model. This paper discusses modeling each pixel as a mixture of Gaussians and using an on-line approximation to update the model. The Gaussian, distributions of the adaptive mixture model are then evaluated to determine which are most likely to result from a background process. Each pixel is classified based on whether the Gaussian distribution which represents it most effectively is considered part of the background model. This results in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. This system has been run almost continuously for 16 months, 24 hours a day, through rain and snow.},
  urldate   = {2025-12-08},
  booktitle = {Proceedings. 1999 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({Cat}. {No} {PR00149})},
  author    = {Stauffer, C. and Grimson, W.E.L.},
  month     = jun,
  year      = {1999},
  note      = {ISSN: 1063-6919},
  keywords  = {Adaptive systems, Artificial intelligence, Gaussian distribution, Image segmentation, Image sequences, Laboratories, Layout, Robustness, Tracking, Vehicle detection},
  pages     = {246--252 Vol. 2},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/JFKB2CCP/Stauffer and Grimson - 1999 - Adaptive background mixture models for real-time tracking.pdf:application/pdf}
}


@incollection{bouwmans_background_2014,
  edition   = {0},
  title     = {Background {Subtraction} for {Moving} {Cameras}},
  isbn      = {978-0-429-17111-6},
  url       = {https://www.taylorfrancis.com/books/9781482205381/chapters/10.1201/b17223-10},
  language  = {en},
  urldate   = {2025-12-08},
  booktitle = {Background {Modeling} and {Foreground} {Detection} for {Video} {Surveillance}},
  publisher = {Chapman and Hall/CRC},
  editor    = {Bouwmans, Thierry and Porikli, Fatih and HÃ¶ferlin, Benjamin and Vacavant, Antoine},
  month     = jul,
  year      = {2014},
  doi       = {10.1201/b17223-10},
  pages     = {133--150},
  file      = {PDF:/home/sandro/Zotero/storage/2QUMLIYW/Bouwmans et al. - 2014 - Background Subtraction for Moving Cameras.pdf:application/pdf}
}

@inproceedings{lucas1981iterative,
  title       = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
  author      = {Lucas, Bruce D and Kanade, Takeo},
  url         = {https://hal.science/hal-03697340},
  booktitle   = {{IJCAI'81: 7th international joint conference on Artificial intelligence}},
  address     = {Vancouver, Canada},
  volume      = {2},
  pages       = {674-679},
  year        = {1981},
  month       = Aug,
  pdf         = {https://hal.science/hal-03697340v1/file/Lucas1981.pdf},
  hal_id      = {hal-03697340},
  hal_version = {v1}
}
@inproceedings{farneback2003two,
  address   = {Berlin, Heidelberg},
  title     = {Two-{Frame} {Motion} {Estimation} {Based} on {Polynomial} {Expansion}},
  isbn      = {978-3-540-45103-7},
  doi       = {10.1007/3-540-45103-X_50},
  abstract  = {This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.},
  language  = {en},
  booktitle = {Image {Analysis}},
  publisher = {Springer},
  author    = {Farnebck, Gunnar},
  editor    = {Bigun, Josef and Gustavsson, Tomas},
  year      = {2003},
  keywords  = {Computer Vision, Motion Model, Orientation Tensor, Polynomial Expansion, Quadratic Polynomial},
  pages     = {363--370},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/Q23G26L9/Farnebck - 2003 - Two-Frame Motion Estimation Based on Polynomial Expansion.pdf:application/pdf}
}

@inproceedings{teed_raft_2020,
  address    = {Cham},
  title      = {{RAFT}: {Recurrent} {All}-{Pairs} {Field} {Transforms} for {Optical} {Flow}},
  isbn       = {978-3-030-58536-5},
  shorttitle = {{RAFT}},
  doi        = {10.1007/978-3-030-58536-5_24},
  abstract   = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.},
  language   = {en},
  booktitle  = {Computer {Vision} “ {ECCV} 2020},
  publisher  = {Springer International Publishing},
  author     = {Teed, Zachary and Deng, Jia},
  editor     = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year       = {2020},
  pages      = {402--419},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/PFMURXLG/Teed and Deng - 2020 - RAFT Recurrent All-Pairs Field Transforms for Optical Flow.pdf:application/pdf}
}

@inproceedings{wang_sea-raft_2025,
  address    = {Cham},
  title      = {{SEA}-{RAFT}: {Simple}, {Efficient}, {Accurate} {RAFT} for {Optical} {Flow}},
  isbn       = {978-3-031-72667-5},
  shorttitle = {{SEA}-{RAFT}},
  doi        = {10.1007/978-3-031-72667-5_3},
  abstract   = {We introduce SEA-RAFT, a more simple, efficient, and accurate RAFT for optical flow. Compared with RAFT, SEA-RAFT is trained with a new loss (mixture of Laplace). It directly regresses an initial flow for faster convergence in iterative refinements and introduces rigid-motion pre-training to improve generalization. SEA-RAFT achieves state-of-the-art accuracy on the Spring benchmark with a 3.69 endpoint-error (EPE) and a 0.36 1-pixel outlier rate (1px), representing 22.9\% and 17.8\% error reduction from best published results. In addition, SEA-RAFT obtains the best cross-dataset generalization on KITTI and Spring. With its high efficiency, SEA-RAFT operates at least 2.3\$\${\textbackslash}times \$\$Ã—faster than existing methods while maintaining competitive performance. The code is publicly available at https://github.com/princeton-vl/SEA-RAFT.},
  language   = {en},
  booktitle  = {Computer {Vision} “ {ECCV} 2024},
  publisher  = {Springer Nature Switzerland},
  author     = {Wang, Yihan and Lipson, Lahav and Deng, Jia},
  editor     = {Leonardis, AleÅ¡ and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, GÃ¼l},
  year       = {2025},
  pages      = {36--54},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/A7YP6ZVG/Wang et al. - 2025 - SEA-RAFT Simple, Efficient, Accurate RAFT for Optical Flow.pdf:application/pdf}
}

@inproceedings{kirillov_segment_2023,
  address   = {Paris, France},
  title     = {Segment {Anything}},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn      = {979-8-3503-0718-4},
  url       = {https://ieeexplore.ieee.org/document/10378323/},
  doi       = {10.1109/ICCV51070.2023.00371},
  abstract  = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efï¬cient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and ï¬nd that its zero-shot performance is impressive “ often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.},
  language  = {en},
  urldate   = {2025-12-08},
  booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  publisher = {IEEE},
  author    = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and DollÃ¡r, Piotr and Girshick, Ross},
  month     = oct,
  year      = {2023},
  pages     = {3992--4003},
  file      = {PDF:/home/sandro/Zotero/storage/IQSKRMUC/Kirillov et al. - 2023 - Segment Anything.pdf:application/pdf}
}

@misc{ravi_sam_2024,
  title      = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
  shorttitle = {{SAM} 2},
  url        = {http://arxiv.org/abs/2408.00714},
  doi        = {10.48550/arXiv.2408.00714},
  abstract   = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rdle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and DollÃ¡r, Piotr and Feichtenhofer, Christoph},
  month      = oct,
  year       = {2024},
  note       = {arXiv:2408.00714 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  annote     = {Comment: Website: https://ai.meta.com/sam2},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/7HU3QHAE/Ravi et al. - 2024 - SAM 2 Segment Anything in Images and Videos.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/BGSIYCAL/2408.html:text/html}
}

@incollection{vedaldi_lift_2020,
  address    = {Cham},
  title      = {Lift, {Splat}, {Shoot}: {Encoding} {Images} from {Arbitrary} {Camera} {Rigs} by {Implicitly} {Unprojecting} to {3D}},
  volume     = {12359},
  isbn       = {978-3-030-58567-9 978-3-030-58568-6},
  shorttitle = {Lift, {Splat}, {Shoot}},
  url        = {https://link.springer.com/10.1007/978-3-030-58568-6_12},
  abstract   = {The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single birds-eye-view coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a birds-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to lift each image individually into a frustum of features for each camera, then splat all frustums into a rasterized birds-eyeview grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard birdseye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by shooting template trajectories into a birds-eyeview cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {Computer {Vision} “ {ECCV} 2020},
  publisher  = {Springer International Publishing},
  author     = {Philion, Jonah and Fidler, Sanja},
  editor     = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year       = {2020},
  doi        = {10.1007/978-3-030-58568-6_12},
  note       = {Series Title: Lecture Notes in Computer Science},
  pages      = {194--210},
  file       = {PDF:/home/sandro/Zotero/storage/T2IMSM4B/Philion and Fidler - 2020 - Lift, Splat, Shoot Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D.pdf:application/pdf}
}

@article{li_bevformer_2025,
  title      = {{BEVFormer}: {Learning} {Bird}s-{Eye}-{View} {Representation} {From} {LiDAR}-{Camera} via {Spatiotemporal} {Transformers}},
  volume     = {47},
  issn       = {1939-3539},
  shorttitle = {{BEVFormer}},
  url        = {https://ieeexplore.ieee.org/document/10791908/},
  doi        = {10.1109/TPAMI.2024.3515454},
  abstract   = {Multi-modality fusion strategy is currently the de-facto most competitive solution for 3D perception tasks. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations from multi-modality data with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from both point cloud and camera input, thus completing multi-modality information fusion under BEV space. For temporal information, we propose temporal self-attention to fuse the history BEV information recurrently. By comparing with other fusion paradigms, we demonstrate that the fusion method proposed in this work is both succinct and effective. Our approach achieves the new state-of-the-art 74.1\% in terms of NDS metric on the nuScenes test set. In addition, we extend BEVFormer to encompass a wide range of autonomous driving tasks, including object tracking, vectorized mapping, occupancy prediction, and end-to-end autonomous driving, achieving outstanding results across these tasks.},
  number     = {3},
  urldate    = {2025-12-08},
  journal    = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author     = {Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Yu, Qiao and Dai, Jifeng},
  month      = mar,
  year       = {2025},
  keywords   = {3D object detection, Accuracy, Aggregates, Autonomous driving, Autonomous vehicles, birds-eye-view, Cameras, Feature extraction, Laser radar, LiDAR-camera, map segmentation, Point cloud compression, Semantics, Three-dimensional displays, transformer, Transformers},
  pages      = {2020--2036}
}

@misc{liu_bevfusion_2024,
  title      = {{BEVFusion}: {Multi}-{Task} {Multi}-{Sensor} {Fusion} with {Unified} {Bird}'s-{Eye} {View} {Representation}},
  shorttitle = {{BEVFusion}},
  url        = {http://arxiv.org/abs/2205.13542},
  doi        = {10.48550/arXiv.2205.13542},
  abstract   = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3\% higher mAP and NDS on 3D object detection and 13.6\% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela and Han, Song},
  month      = sep,
  year       = {2024},
  note       = {arXiv:2205.13542 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: ICRA 2023. The first two authors contributed equally to this work. Project page: https://bevfusion.mit.edu},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/G2VUDJ2E/Liu et al. - 2024 - BEVFusion Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/9U75WCQN/2205.html:text/html}
}

@inproceedings{lin_rcbevdet_2024,
  address    = {Seattle, WA, USA},
  title      = {{RCBEVDet}: {Radar}-{Camera} {Fusion} in {Bird}'s {Eye} {View} for {3D} {Object} {Detection}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-5300-6},
  shorttitle = {{RCBEVDet}},
  url        = {https://ieeexplore.ieee.org/document/10656863/},
  doi        = {10.1109/CVPR52733.2024.01414},
  abstract   = {Three-dimensional object detection is one of the key tasks in autonomous driving. To reduce costs in practice, low-cost multi-view cameras for 3D object detection are proposed to replace the expansive LiDAR sensors. However, relying solely on cameras is difficult to achieve highly accurate and robust 3D object detection. An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a radar-camera fusion 3D object detection method in the birds eye view (BEV). Specifically, we first design RadarBEVNet for radar BEV feature extraction. RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder. In the dual-stream radar backbone, a pointbased encoder and a transformer-based encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to scattering the point feature in BEV. Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the multi-modal BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers. Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D object detectors with a faster inference speed at 21-28 FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Lin, Zhiwei and Liu, Zhe and Xia, Zhongyu and Wang, Xinhao and Wang, Yongtao and Qi, Shengxiang and Dong, Yang and Dong, Nan and Zhang, Le and Zhu, Ce},
  month      = jun,
  year       = {2024},
  pages      = {14928--14937},
  file       = {PDF:/home/sandro/Zotero/storage/CPYQEZQU/Lin et al. - 2024 - RCBEVDet Radar-Camera Fusion in Bird's Eye View for 3D Object Detection.pdf:application/pdf}
}

@article{kutulakos_theory_2000,
  title    = {A {Theory} of {Shape} by {Space} {Carving}},
  volume   = {38},
  issn     = {1573-1405},
  url      = {https://doi.org/10.1023/A:1008191222954},
  doi      = {10.1023/A:1008191222954},
  abstract = {In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) capture photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their view-dependent effects on scene-appearance.},
  language = {en},
  number   = {3},
  urldate  = {2025-12-08},
  journal  = {International Journal of Computer Vision},
  author   = {Kutulakos, Kiriakos N. and Seitz, Steven M.},
  month    = jul,
  year     = {2000},
  keywords = {3D photography, metameric shapes, multi-view stereo, photorealistic reconstruction, scene modeling, shape-from-silhouettes, space carving, visual hull, volumetric shape representations, voxel coloring},
  pages    = {199--218},
  file     = {Full Text PDF:/home/sandro/Zotero/storage/57IYJHFZ/Kutulakos and Seitz - 2000 - A Theory of Shape by Space Carving.pdf:application/pdf}
}

@inproceedings{bewley_simple_2016,
  title     = {Simple online and realtime tracking},
  url       = {https://ieeexplore.ieee.org/document/7533003/},
  doi       = {10.1109/ICIP.2016.7533003},
  abstract  = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  urldate   = {2025-12-08},
  booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
  author    = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  month     = sep,
  year      = {2016},
  note      = {ISSN: 2381-8549},
  keywords  = {Benchmark testing, Complexity theory, Computer Vision, Data Association, Detection, Detectors, Kalman filters, Multiple Object Tracking, Target tracking, Visualization},
  pages     = {3464--3468},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/SPPU4AYT/Bewley et al. - 2016 - Simple online and realtime tracking.pdf:application/pdf}
}

@inproceedings{wojke_simple_2017,
  title     = {Simple online and realtime tracking with a deep association metric},
  url       = {https://ieeexplore.ieee.org/document/8296962/},
  doi       = {10.1109/ICIP.2017.8296962},
  abstract  = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a largescale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.},
  urldate   = {2025-12-08},
  booktitle = {2017 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
  author    = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  month     = sep,
  year      = {2017},
  note      = {ISSN: 2381-8549},
  keywords  = {Cameras, Computer Vision, Data Association, Extraterrestrial measurements, Kalman filters, Multiple Object Tracking, Standards, Tracking, Uncertainty},
  pages     = {3645--3649}
}

@inproceedings{zhang_bytetrack_2022,
  address    = {Cham},
  title      = {{ByteTrack}: {Multi}-object {Tracking} by\textasciitilde\ {Associating} {Every} {Detection} {Box}},
  isbn       = {978-3-031-20047-2},
  shorttitle = {{ByteTrack}},
  doi        = {10.1007/978-3-031-20047-2_1},
  abstract   = {Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.},
  language   = {en},
  booktitle  = {Computer {Vision} “ {ECCV} 2022},
  publisher  = {Springer Nature Switzerland},
  author     = {Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
  editor     = {Avidan, Shai and Brostow, Gabriel and Ciss, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year       = {2022},
  keywords   = {Data association, Detection boxes, Multi-object tracking},
  pages      = {1--21}
}

@inproceedings{cao_ocsort_2023,
  address    = {Vancouver, BC, Canada},
  title      = {Observation-{Centric} {SORT}: {Rethinking} {SORT} for {Robust} {Multi}-{Object} {Tracking}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-0129-8},
  shorttitle = {Observation-{Centric} {SORT}},
  url        = {https://ieeexplore.ieee.org/document/10204818/},
  doi        = {10.1109/CVPR52729.2023.00934},
  abstract   = {Kalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at https://github.com/noahcao/OC\_SORT.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Cao, Jinkun and Pang, Jiangmiao and Weng, Xinshuo and Khirodkar, Rawal and Kitani, Kris},
  month      = jun,
  year       = {2023},
  pages      = {9686--9696},
  file       = {PDF:/home/sandro/Zotero/storage/IKBPBJ44/Cao et al. - 2023 - Observation-Centric SORT Rethinking SORT for Robust Multi-Object Tracking.pdf:application/pdf}
}

@misc{yamane_mvtrajecter_2025,
  title      = {{MVTrajecter}: {Multi}-{View} {Pedestrian} {Tracking} with {Trajectory} {Motion} {Cost} and {Trajectory} {Appearance} {Cost}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {{MVTrajecter}},
  url        = {https://arxiv.org/abs/2509.01157},
  doi        = {10.48550/ARXIV.2509.01157},
  abstract   = {Multi-View Pedestrian Tracking (MVPT) aims to track pedestrians in the form of a birds eye view occupancy map from multi-view videos. End-to-end methods that detect and associate pedestrians within one model have shown great progress in MVPT. The motion and appearance information of pedestrians is important for the association, but previous end-to-end MVPT methods rely only on the current and its single adjacent past timestamp, discarding the past trajectories before that. This paper proposes a novel endto-end MVPT method called Multi-View Trajectory Tracker (MVTrajecter) that utilizes information from multiple timestamps in past trajectories for robust association. MVTrajecter introduces trajectory motion cost and trajectory appearance cost to effectively incorporate motion and appearance information, respectively. These costs calculate which pedestrians at the current and each past timestamp are likely identical based on the information between those timestamps. Even if a current pedestrian could be associated with a false pedestrian at some past timestamp, these costs enable the model to associate that current pedestrian with the correct past trajectory based on other past timestamps. In addition, MVTrajecter effectively captures the relationships between multiple timestamps leveraging the attention mechanism. Extensive experiments demonstrate the effectiveness of each component in MVTrajecter and show that it outperforms the previous state-of-the-art methods.},
  language   = {en},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Yamane, Taiga and Masumura, Ryo and Suzuki, Satoshi and Orihashi, Shota},
  year       = {2025},
  note       = {Version Number: 1},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  annote     = {Other
                Accepted by ICCV 2025},
  file       = {PDF:/home/sandro/Zotero/storage/DEVTD5VJ/Yamane et al. - 2025 - MVTrajecter Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Co.pdf:application/pdf}
}

@inproceedings{chavdarova_wildtrack_2018,
  address    = {Salt Lake City, UT},
  title      = {{WILDTRACK}: {A} {Multi}-camera {HD} {Dataset} for {Dense} {Unscripted} {Pedestrian} {Detection}},
  isbn       = {978-1-5386-6420-9},
  shorttitle = {{WILDTRACK}},
  url        = {https://ieeexplore.ieee.org/document/8578626/},
  doi        = {10.1109/CVPR.2018.00528},
  abstract   = {People detection methods are highly sensitive to occlusions between pedestrians, which are extremely frequent in many situations where cameras have to be mounted at a limited height. The reduction of camera prices allows for the generalization of static multi-camera set-ups. Using joint visual information from multiple synchronized cameras gives the opportunity to improve detection performance.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  publisher  = {IEEE},
  author     = {Chavdarova, Tatjana and Baque, Pierre and Bouquet, Stephane and Maksai, Andrii and Jose, Cijo and Bagautdinov, Timur and Lettry, Louis and Fua, Pascal and Van Gool, Luc and Fleuret, Francois},
  month      = jun,
  year       = {2018},
  pages      = {5030--5039},
  file       = {PDF:/home/sandro/Zotero/storage/B37NS8QU/Chavdarova et al. - 2018 - WILDTRACK A Multi-camera HD Dataset for Dense Unscripted Pedestrian Detection.pdf:application/pdf}
}

@inproceedings{meinhardt_trackformer_2022,
  address    = {New Orleans, LA, USA},
  title      = {{TrackFormer}: {Multi}-{Object} {Tracking} with {Transformers}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-6654-6946-3},
  shorttitle = {{TrackFormer}},
  url        = {https://ieeexplore.ieee.org/document/9879668/},
  doi        = {10.1109/CVPR52688.2022.00864},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Meinhardt, Tim and Kirillov, Alexander and Leal-Taixe, Laura and Feichtenhofer, Christoph},
  month      = jun,
  year       = {2022},
  pages      = {8834--8844},
  file       = {PDF:/home/sandro/Zotero/storage/AZQXWGWK/Meinhardt et al. - 2022 - TrackFormer Multi-Object Tracking with Transformers.pdf:application/pdf}
}

@inproceedings{carion_end--end_2020,
  address   = {Cham},
  title     = {End-to-{End} {Object} {Detection} with {Transformers}},
  isbn      = {978-3-030-58452-8},
  abstract  = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  booktitle = {Computer {Vision} “ {ECCV} 2020},
  publisher = {Springer International Publishing},
  author    = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year      = {2020},
  pages     = {213--229}
}

@inproceedings{zhang_motrv2_2023,
  address    = {Vancouver, BC, Canada},
  title      = {{MOTRv2}: {Bootstrapping} {End}-to-{End} {Multi}-{Object} {Tracking} by {Pretrained} {Object} {Detectors}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-0129-8},
  shorttitle = {{MOTRv2}},
  url        = {https://ieeexplore.ieee.org/document/10204828/},
  doi        = {10.1109/CVPR52729.2023.02112},
  abstract   = {In this paper, we propose MOTRv2, a simple yet effective pipeline to bootstrap end-to-end multi-object tracking with a pretrained object detector. Existing end-to-end methods, e.g. MOTR [43] and TrackFormer [20] are inferior to their tracking-by-detection counterparts mainly due to their poor detection performance. We aim to improve MOTR by elegantly incorporating an extra object detector. We first adopt the anchor formulation of queries and then use an extra object detector to generate proposals as anchors, providing detection prior to MOTR. The simple modification greatly eases the conflict between joint learning detection and association tasks in MOTR. MOTRv2 keeps the query propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the 1st place (73.4\% HOTA on DanceTrack) in the 1st Multiple People Tracking in Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on the BDD100K dataset. We hope this simple and effective pipeline can provide some new insights to the endto-end MOT community. Code is available at https: //github.com/megvii-research/MOTRv2.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Zhang, Yuang and Wang, Tiancai and Zhang, Xiangyu},
  month      = jun,
  year       = {2023},
  pages      = {22056--22065},
  file       = {PDF:/home/sandro/Zotero/storage/66QCSP2C/Zhang et al. - 2023 - MOTRv2 Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors.pdf:application/pdf}
}

@article{Weng2020_AB3DMOT_eccvw,
  author  = {Weng, Xinshuo and Wang, Jianren and Held, David and Kitani, Kris},
  journal = {ECCVW},
  title   = {{AB3DMOT: A Baseline for 3D Multi-Object Tracking and New Evaluation Metrics}},
  year    = {2020}
}
@inproceedings{yin_center-based_2021,
  address   = {Nashville, TN, USA},
  title     = {Center-based {3D} {Object} {Detection} and {Tracking}},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn      = {978-1-6654-4509-2},
  url       = {https://ieeexplore.ieee.org/document/9578166/},
  doi       = {10.1109/CVPR46437.2021.01161},
  abstract  = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difï¬culties enumerating all orientations or ï¬tting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, ï¬rst detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it reï¬nes these estimates using additional point features on the object. In CenterPoint, 3D object tracking simpliï¬es to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efï¬cient, and effective. CenterPoint achieved state-of-theart performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, CenterPoint outperforms all previous single model methods by a large margin and ranks ï¬rst among all Lidar-only submissions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.},
  language  = {en},
  urldate   = {2025-12-08},
  booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author    = {Yin, Tianwei and Zhou, Xingyi and Krahenbuhl, Philipp},
  month     = jun,
  year      = {2021},
  pages     = {11779--11788},
  file      = {PDF:/home/sandro/Zotero/storage/CILG3PZQ/Yin et al. - 2021 - Center-based 3D Object Detection and Tracking.pdf:application/pdf}
}
@inproceedings{pang2023standing,
  title     = {Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking},
  author    = {Pang, Ziqi and Li, Jie and Tokmakov, Pavel and Chen, Dian and Zagoruyko, Sergey and Wang, Yu-Xiong},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2023}
}

@misc{zhang_mutr3d_2022,
  title      = {{MUTR3D}: {A} {Multi}-camera {Tracking} {Framework} via {3D}-to-{2D} {Queries}},
  shorttitle = {{MUTR3D}},
  url        = {http://arxiv.org/abs/2205.00613},
  doi        = {10.48550/arXiv.2205.00613},
  abstract   = {Accurate and consistent 3D tracking from multiple cameras is a key component in a vision-based autonomous driving system. It involves modeling 3D dynamic objects in complex scenes across multiple cameras. This problem is inherently challenging due to depth estimation, visual occlusions, appearance ambiguity, etc. Moreover, objects are not consistently associated across time and cameras. To address that, we propose an end-to-end {\textbackslash}textbf\{MU\}lti-camera {\textbackslash}textbf\{TR\}acking framework called MUTR3D. In contrast to prior works, MUTR3D does not explicitly rely on the spatial and appearance similarity of objects. Instead, our method introduces {\textbackslash}textit\{3D track query\} to model spatial and appearance coherent track for each object that appears in multiple cameras and multiple frames. We use camera transformations to link 3D trackers with their observations in 2D images. Each tracker is further refined according to the features that are obtained from camera images. MUTR3D uses a set-to-set loss to measure the difference between the predicted tracking results and the ground truths. Therefore, it does not require any post-processing such as non-maximum suppression and/or bounding box association. MUTR3D outperforms state-of-the-art methods by 5.3 AMOTA on the nuScenes dataset. Code is available at: {\textbackslash}url\{https://github.com/a1600012888/MUTR3D\}.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Zhang, Tianyuan and Chen, Xuanyao and Wang, Yue and Wang, Yilun and Zhao, Hang},
  month      = may,
  year       = {2022},
  note       = {arXiv:2205.00613 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Appear on CVPR 2022 Workshop on Autonomous Driving},
  file       = {Snapshot:/home/sandro/Zotero/storage/WQJPUXTG/2205.html:text/html}
}

@article{kutulakos_theory_2000,
  title    = {A {Theory} of {Shape} by {Space} {Carving}},
  volume   = {38},
  issn     = {1573-1405},
  url      = {https://doi.org/10.1023/A:1008191222954},
  doi      = {10.1023/A:1008191222954},
  abstract = {In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) capture photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their view-dependent effects on scene-appearance.},
  language = {en},
  number   = {3},
  urldate  = {2025-12-08},
  journal  = {International Journal of Computer Vision},
  author   = {Kutulakos, Kiriakos N. and Seitz, Steven M.},
  month    = jul,
  year     = {2000},
  keywords = {3D photography, metameric shapes, multi-view stereo, photorealistic reconstruction, scene modeling, shape-from-silhouettes, space carving, visual hull, volumetric shape representations, voxel coloring},
  pages    = {199--218},
  file     = {Full Text PDF:/home/sandro/Zotero/storage/ZRXBL3AA/Kutulakos and Seitz - 2000 - A Theory of Shape by Space Carving.pdf:application/pdf}
}



@inproceedings{yu_pixelnerf_2021,
  title      = {{pixelNeRF}: {Neural} {Radiance} {Fields} from {One} or {Few} {Images}},
  shorttitle = {{pixelNeRF}},
  url        = {https://ieeexplore.ieee.org/document/9577688/},
  doi        = {10.1109/CVPR46437.2021.00455},
  abstract   = {We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website:https://alexyu.net/pixelnerf.},
  urldate    = {2025-12-08},
  booktitle  = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  author     = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
  month      = jun,
  year       = {2021},
  note       = {ISSN: 2575-7075},
  keywords   = {Benchmark testing, Computer architecture, Computer vision, Convolutional codes, Image resolution, Solid modeling, Three-dimensional displays},
  pages      = {4576--4585},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/MXX2R3C5/Yu et al. - 2021 - pixelNeRF Neural Radiance Fields from One or Few Images.pdf:application/pdf}
}

@inproceedings{chen_mvsnerf_2021,
  address    = {Montreal, QC, Canada},
  title      = {{MVSNeRF}: {Fast} {Generalizable} {Radiance} {Field} {Reconstruction} from {Multi}-{View} {Stereo}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-6654-2812-5},
  shorttitle = {{MVSNeRF}},
  url        = {https://ieeexplore.ieee.org/document/9711430/},
  doi        = {10.1109/ICCV48922.2021.01386},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  publisher  = {IEEE},
  author     = {Chen, Anpei and Xu, Zexiang and Zhao, Fuqiang and Zhang, Xiaoshuai and Xiang, Fanbo and Yu, Jingyi and Su, Hao},
  month      = oct,
  year       = {2021},
  pages      = {14104--14113},
  file       = {PDF:/home/sandro/Zotero/storage/3IEVEF3Y/Chen et al. - 2021 - MVSNeRF Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo.pdf:application/pdf}
}

@article{muller_instant_2022,
  title    = {Instant neural graphics primitives with a multiresolution hash encoding},
  volume   = {41},
  issn     = {0730-0301, 1557-7368},
  url      = {https://dl.acm.org/doi/10.1145/3528223.3530127},
  doi      = {10.1145/3528223.3530127},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920Ã—1080.},
  language = {en},
  number   = {4},
  urldate  = {2025-12-08},
  journal  = {ACM Transactions on Graphics},
  author   = {MÃ¼ller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  month    = jul,
  year     = {2022},
  pages    = {1--15},
  file     = {Full Text PDF:/home/sandro/Zotero/storage/PCNAMNKY/MÃ¼ller et al. - 2022 - Instant neural graphics primitives with a multiresolution hash encoding.pdf:application/pdf}
}

@inproceedings{niemeyer_regnerf_2022,
  address    = {New Orleans, LA, USA},
  title      = {{RegNeRF}: {Regularizing} {Neural} {Radiance} {Fields} for {View} {Synthesis} from {Sparse} {Inputs}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-6654-6946-3},
  shorttitle = {{RegNeRF}},
  url        = {https://ieeexplore.ieee.org/document/9879664/},
  doi        = {10.1109/CVPR52688.2022.00540},
  abstract   = {Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Niemeyer, Michael and Barron, Jonathan T. and Mildenhall, Ben and Sajjadi, Mehdi S. M. and Geiger, Andreas and Radwan, Noha},
  month      = jun,
  year       = {2022},
  pages      = {5470--5480},
  file       = {PDF:/home/sandro/Zotero/storage/8C8NQMW9/Niemeyer et al. - 2022 - RegNeRF Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs.pdf:application/pdf}
}

@inproceedings{warburg_nerfbusters_2023,
  address    = {Paris, France},
  title      = {Nerfbusters: {Removing} {Ghostly} {Artifacts} from {Casually} {Captured} {NeRFs}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-0718-4},
  shorttitle = {Nerfbusters},
  url        = {https://ieeexplore.ieee.org/document/10376739/},
  doi        = {10.1109/ICCV51070.2023.01661},
  abstract   = {Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the input camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To aid in the development and evaluation of new methods in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  publisher  = {IEEE},
  author     = {Warburg, Frederik and Weber, Ethan and Tancik, Matthew and Holynski, Aleksander and Kanazawa, Angjoo},
  month      = oct,
  year       = {2023},
  pages      = {18074--18084},
  file       = {PDF:/home/sandro/Zotero/storage/B6ITFJM5/Warburg et al. - 2023 - Nerfbusters Removing Ghostly Artifacts from Casually Captured NeRFs.pdf:application/pdf}
}

@misc{kerbl_3d_2023,
  title     = {{3D} {Gaussian} {Splatting} for {Real}-{Time} {Radiance} {Field} {Rendering}},
  url       = {http://arxiv.org/abs/2308.04079},
  doi       = {10.48550/arXiv.2308.04079},
  abstract  = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({\textgreater}= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  language  = {en},
  urldate   = {2025-09-06},
  publisher = {arXiv},
  author    = {Kerbl, Bernhard and Kopanas, Georgios and LeimkÃ¼hler, Thomas and Drettakis, George},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2308.04079 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, notion},
  annote    = {Comment: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/},
  file      = {PDF:/home/sandro/Zotero/storage/6BRC4T6W/Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf:application/pdf}
}

@inproceedings{charatan_pixelsplat_2024,
  address    = {Seattle, WA, USA},
  title      = {{PixelSplat}: {3D} {Gaussian} {Splats} from {Image} {Pairs} for {Scalable} {Generalizable} {3D} {Reconstruction}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-5300-6},
  shorttitle = {{PixelSplat}},
  url        = {https://ieeexplore.ieee.org/document/10655681/},
  doi        = {10.1109/CVPR52733.2024.01840},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Charatan, David and Li, Sizhe Lester and Tagliasacchi, Andrea and Sitzmann, Vincent},
  month      = jun,
  year       = {2024},
  pages      = {19457--19467},
  file       = {PDF:/home/sandro/Zotero/storage/8KX7335U/Charatan et al. - 2024 - PixelSplat 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction.pdf:application/pdf}
}

@inproceedings{szymanowicz_splatter_2024,
  address    = {Seattle, WA, USA},
  title      = {Splatter {Image}: {Ultra}-{Fast} {Single}-{View} {3D} {Reconstruction}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-5300-6},
  shorttitle = {Splatter {Image}},
  url        = {https://ieeexplore.ieee.org/document/10656101/},
  doi        = {10.1109/CVPR52733.2024.00972},
  abstract   = {We introduce the Splatter Image, an ultra-efï¬cient approach for monocular 3D object reconstruction. Splatter Image is based on Gaussian Splatting, which allows fast and high-quality reconstruction of 3D scenes from multiple images. We apply Gaussian Splatting to monocular reconstruction by learning a neural network that, at test time, performs reconstruction in a feed-forward manner, at 38 FPS. Our main innovation is the surprisingly straightforward design of this network, which, using 2D operators, maps the input image to one 3D Gaussian per pixel. The resulting set of Gaussians thus has the form an image, the Splatter Image. We further extend the method take several images as input via cross-view attention. Owning to the speed of the renderer (588 FPS), we use a single GPU for training while generating entire images at each iteration to optimize perceptual metrics like LPIPS. On several synthetic, real, multi-category and large-scale benchmark datasets, we achieve better results in terms of PSNR, LPIPS, and other metrics while training and evaluating much faster than prior works. Code, models and more results are available at https://szymanowiczs.github.io/ splatter-image.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Szymanowicz, Stanislaw and Rupprecht, Christian and Vedaldi, Andrea},
  month      = jun,
  year       = {2024},
  pages      = {10208--10217},
  file       = {PDF:/home/sandro/Zotero/storage/CLQ6YPQJ/Szymanowicz et al. - 2024 - Splatter Image Ultra-Fast Single-View 3D Reconstruction.pdf:application/pdf}
}

@misc{meta_sam_3d_2025,
  title      = {{SAM} {3D}: {3Dfy} {Anything} in {Images}},
  shorttitle = {{SAM} {3D}},
  url        = {http://arxiv.org/abs/2511.16624},
  doi        = {10.48550/arXiv.2511.16624},
  abstract   = {We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Team, SAM 3D and Chen, Xingyu and Chu, Fu-Jen and Gleize, Pierre and Liang, Kevin J. and Sax, Alexander and Tang, Hao and Wang, Weiyao and Guo, Michelle and Hardin, Thibaut and Li, Xiang and Lin, Aohan and Liu, Jiawei and Ma, Ziqi and Sagar, Anushka and Song, Bowen and Wang, Xiaodong and Yang, Jianing and Zhang, Bowen and DollÃ¡r, Piotr and Gkioxari, Georgia and Feiszli, Matt and Malik, Jitendra},
  month      = nov,
  year       = {2025},
  note       = {arXiv:2511.16624 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Website: https://ai.meta.com/sam3d/},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/FN625AQD/Team et al. - 2025 - SAM 3D 3Dfy Anything in Images.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/WU9XI6HI/2511.html:text/html}
}

@inproceedings{chen_mvsplat_2025,
  address    = {Cham},
  title      = {{MVSplat}: {Efficient} {3D} {Gaussian} {Splatting} from {Sparse} {Multi}-view {Images}},
  isbn       = {978-3-031-72664-4},
  shorttitle = {{MVSplat}},
  doi        = {10.1007/978-3-031-72664-4_21},
  abstract   = {We introduce MVSplat,an efficient model that, given sparse multi-view images as input, predicts clean feed-forward 3D Gaussians. To accurately localize the Gaussian centers, we build a cost volume representation via plane sweeping, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We also learn other Gaussian primitives parameters jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussians via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplat achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). More impressively, compared to the latest state-of-the-art method pixelSplat, MVSplat uses \$\$10{\textbackslash}times \$\$10Ã—fewer parameters and infers more than \$\$2{\textbackslash}times \$\$2Ã—faster while providing higher appearance and geometry quality as well as better cross-dataset generalization.},
  language   = {en},
  booktitle  = {Computer {Vision} “ {ECCV} 2024},
  publisher  = {Springer Nature Switzerland},
  author     = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
  editor     = {Leonardis, AleÅ¡ and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, GÃ¼l},
  year       = {2025},
  keywords   = {Cost Volume, Feature Matching, Gaussian Splatting},
  pages      = {370--386},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/QRZG8CBL/Chen et al. - 2025 - MVSplat Efficient 3D Gaussian Splatting from Sparse Multi-view Images.pdf:application/pdf}
}

@inproceedings{zheng_gps-gaussian_2024,
  address    = {Seattle, WA, USA},
  title      = {{GPS}-{Gaussian}: {Generalizable} {Pixel}-{Wise} {3D} {Gaussian} {Splatting} for {Real}-{Time} {Human} {Novel} {View} {Synthesis}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-5300-6},
  shorttitle = {{GPS}-{Gaussian}},
  url        = {https://ieeexplore.ieee.org/document/10657862/},
  doi        = {10.1109/CVPR52733.2024.01861},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Zheng, Shunyuan and Zhou, Boyao and Shao, Ruizhi and Liu, Boning and Zhang, Shengping and Nie, Liqiang and Liu, Yebin},
  month      = jun,
  year       = {2024},
  pages      = {19680--19690},
  file       = {GPS-Gaussian\: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis:/home/sandro/Zotero/storage/GI3KT6SC/Zheng_GPS-Gaussian_Generalizable_Pixel-wise_3D_Gaussian_Splatting_for_Real-time_Human_Novel_CVPR.pdf:application/pdf}
}

@inproceedings{Xu_2025_depthsplat,
  author    = {Xu, Haofei and Peng, Songyou and Wang, Fangjinhua and Blum, Hermann and Barath, Daniel and Geiger, Andreas and Pollefeys, Marc},
  abstract  = {Gaussian splatting and single-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pretrained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale multi-view posed datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. In addition, DepthSplat enables feed-forward reconstruction from 12 input views (512 Ã— 960 resolutions) in 0.6 seconds.},
  title     = {DepthSplat: Connecting Gaussian Splatting and Depth},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2025},
  pages     = {16453-16463},
  file      = {PDF:/home/sandro/Zotero/storage/3E7I58CB/Xu et al. - DepthSplat Connecting Gaussian Splatting and Depth.pdf:application/pdf}
}
@misc{zhang_gs-lrm_2024,
  title      = {{GS}-{LRM}: {Large} {Reconstruction} {Model} for {3D} {Gaussian} {Splatting}},
  shorttitle = {{GS}-{LRM}},
  url        = {http://arxiv.org/abs/2404.19702},
  doi        = {10.48550/arXiv.2404.19702},
  abstract   = {We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ .},
  urldate    = {2025-11-05},
  publisher  = {arXiv},
  author     = {Zhang, Kai and Bi, Sai and Tan, Hao and Xiangli, Yuanbo and Zhao, Nanxuan and Sunkavalli, Kalyan and Xu, Zexiang},
  month      = apr,
  year       = {2024},
  note       = {arXiv:2404.19702 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, notion},
  annote     = {Comment: Project webpage: https://sai-bi.github.io/project/gs-lrm/},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/LCVAALF8/Zhang et al. - 2024 - GS-LRM Large Reconstruction Model for 3D Gaussian Splatting.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/FQCJR8ET/2404.html:text/html}
}

@misc{tang_lgm_2024,
  title = {{LGM}: {Large} {Multi}-{View} {Gaussian} {Model} for {High}-{Resolution} {3D} {Content} {Creation}},
  shorttitle = {{LGM}},
  url = {http://arxiv.org/abs/2402.05054},
  doi = {10.48550/arXiv.2402.05054},
  abstract = {3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.},
  urldate = {2025-11-06},
  publisher = {arXiv},
  author = {Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},
  month = feb,
  year = {2024},
  note = {arXiv:2402.05054 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
  annote = {Comment: Project page: https://me.kiui.moe/lgm/},
  file = {Full Text PDF:/home/sandro/Zotero/storage/99JEA97E/Tang et al. - 2024 - LGM Large Multi-View Gaussian Model for High-Resolution 3D Content Creation.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/7VSV226K/2402.html:text/html},
}

@incollection{leonardis_lgm_2025,
  address = {Cham},
  title = {{LGM}: {Large} {Multi}-view {Gaussian} {Model} for {High}-{Resolution} {3D} {Content} {Creation}},
  volume = {15062},
  isbn = {978-3-031-73234-8 978-3-031-73235-5},
  shorttitle = {{LGM}},
  url = {https://link.springer.com/10.1007/978-3-031-73235-5_1},
  language = {en},
  urldate = {2025-12-23},
  booktitle = {Computer {Vision} - {ECCV} 2024},
  publisher = {Springer Nature Switzerland},
  author = {Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  year = {2025},
  doi = {10.1007/978-3-031-73235-5_1},
  note = {Series Title: Lecture Notes in Computer Science},
  pages = {1--18},
  file = {PDF:/home/sandro/Zotero/storage/9ZQM6Z6U/Tang et al. - 2025 - LGM Large Multi-view Gaussian Model for High-Resolution 3D Content Creation.pdf:application/pdf},
}


@misc{zou_triplane_2023,
  title = {Triplane {Meets} {Gaussian} {Splatting}: {Fast} and {Generalizable} {Single}-{View} {3D} {Reconstruction} with {Transformers}},
  shorttitle = {Triplane {Meets} {Gaussian} {Splatting}},
  url = {http://arxiv.org/abs/2312.09147},
  doi = {10.48550/arXiv.2312.09147},
  abstract = {Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at https://zouzx.github.io/TriplaneGaussian/.},
  urldate = {2025-11-24},
  publisher = {arXiv},
  author = {Zou, Zi-Xin and Yu, Zhipeng and Guo, Yuan-Chen and Li, Yangguang and Liang, Ding and Cao, Yan-Pei and Zhang, Song-Hai},
  month = dec,
  year = {2023},
  note = {arXiv:2312.09147 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
  annote = {Comment: Project Page: https://zouzx.github.io/TriplaneGaussian/},
  file = {Full Text PDF:/home/sandro/Zotero/storage/9LXIF832/Zou et al. - 2023 - Triplane Meets Gaussian Splatting Fast and Generalizable Single-View 3D Reconstruction with Transfo.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/9QSJKXRF/2312.html:text/html},
}


@misc{xu_freesplatter_2025,
  title      = {{FreeSplatter}: {Pose}-free {Gaussian} {Splatting} for {Sparse}-view {3D} {Reconstruction}},
  shorttitle = {{FreeSplatter}},
  url        = {http://arxiv.org/abs/2412.09573},
  doi        = {10.48550/arXiv.2412.09573},
  abstract   = {Sparse-view reconstruction models typically require precise camera poses, yet obtaining these parameters from sparse-view images remains challenging. We introduce FreeSplatter, a scalable feed-forward framework that generates high-quality 3D Gaussians from uncalibrated sparse-view images while estimating camera parameters within seconds. Our approach employs a streamlined transformer architecture where self-attention blocks facilitate information exchange among multi-view image tokens, decoding them into pixel-aligned 3D Gaussian primitives within a unified reference frame. This representation enables both high-fidelity 3D modeling and efficient camera parameter estimation using off-the-shelf solvers. We develop two specialized variants--for object-centric and scene-level reconstruction--trained on comprehensive datasets. Remarkably, FreeSplatter outperforms several pose-dependent Large Reconstruction Models (LRMs) by a notable margin while achieving comparable or even better pose estimation accuracy compared to state-of-the-art pose-free reconstruction approach MASt3R in challenging benchmarks. Beyond technical benchmarks, FreeSplatter streamlines text/image-to-3D content creation pipelines, eliminating the complexity of camera pose management while delivering exceptional visual fidelity.},
  urldate    = {2025-11-04},
  publisher  = {arXiv},
  author     = {Xu, Jiale and Gao, Shenghua and Shan, Ying},
  month      = sep,
  year       = {2025},
  note       = {arXiv:2412.09573 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, notion},
  annote     = {Comment: Project page: https://bluestyle97.github.io/projects/freesplatter/},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/5LT2CDLP/Xu et al. - 2025 - FreeSplatter Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/NL224P4C/2412.html:text/html}
}


@misc{fujimura_ufv-splatter_2025,
  title      = {{UFV}-{Splatter}: {Pose}-{Free} {Feed}-{Forward} {3D} {Gaussian} {Splatting} {Adapted} to {Unfavorable} {Views}},
  shorttitle = {{UFV}-{Splatter}},
  url        = {http://arxiv.org/abs/2507.22342},
  doi        = {10.48550/arXiv.2507.22342},
  abstract   = {This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework designed to handle unfavorable input views. A common rendering setup for training feed-forward approaches places a 3D object at the world origin and renders it from cameras pointed toward the origin -- i.e., from favorable views, limiting the applicability of these models to real-world scenarios involving varying and unknown camera poses. To overcome this limitation, we introduce a novel adaptation framework that enables pretrained pose-free feed-forward 3DGS models to handle unfavorable views. We leverage priors learned from favorable images by feeding recentered images into a pretrained model augmented with low-rank adaptation (LoRA) layers. We further propose a Gaussian adapter module to enhance the geometric consistency of the Gaussians derived from the recentered inputs, along with a Gaussian alignment method to render accurate target views for training. Additionally, we introduce a new training strategy that utilizes an off-the-shelf dataset composed solely of favorable images. Experimental results on both synthetic images from the Google Scanned Objects dataset and real images from the OmniObject3D dataset validate the effectiveness of our method in handling unfavorable input views.},
  urldate = {2025-11-04},
  publisher = {arXiv},
  author = {Fujimura, Yuki and Kushida, Takahiro and Kitano, Kazuya and Funatomi, Takuya and Mukaigawa, Yasuhiro},
  month = aug,
  year = {2025},
  note = {arXiv:2507.22342 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
  annote = {Comment: Project page: https://yfujimura.github.io/UFV-Splatter\_page/},
  file = {Full Text PDF:/home/sandro/Zotero/storage/ECDNH4SJ/Fujimura et al. - 2025 - UFV-Splatter Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/A6H7H9ND/2507.html:text/html},
}

@misc{xu_resplat_2025,
  title = {{ReSplat}: {Learning} {Recurrent} {Gaussian} {Splats}},
  shorttitle = {{ReSplat}},
  url = {http://arxiv.org/abs/2510.08575},
  doi = {10.48550/arXiv.2510.08575},
  abstract = {While feed-forward Gaussian splatting models offer computational efficiency and can generalize to sparse input settings, their performance is fundamentally constrained by relying on a single forward pass for inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization across datasets, view counts and image resolutions. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a \$16 {\textbackslash}times\$ subsampled space, producing \$16 {\textbackslash}times\$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16, 32), resolutions (\$256 {\textbackslash}times 256\$ to \$540 {\textbackslash}times 960\$), and datasets (DL3DV, RealEstate10K and ACID) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.},
  urldate = {2025-12-09},
  publisher = {arXiv},
  author = {Xu, Haofei and Barath, Daniel and Geiger, Andreas and Pollefeys, Marc},
  month = dec,
  year = {2025},
  note = {arXiv:2510.08575 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote = {Comment: Project page: https://haofeixu.github.io/resplat/},
  file = {Full Text PDF:/home/sandro/Zotero/storage/2WC2BZ2G/Xu et al. - 2025 - ReSplat Learning Recurrent Gaussian Splats.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/GBVUC77H/2510.html:text/html},
}


@misc{smart_splatt3r_2024,
  title      = {{Splatt3R}: {Zero}-shot {Gaussian} {Splatting} from {Uncalibrated} {Image} {Pairs}},
  shorttitle = {{Splatt3R}},
  url        = {http://arxiv.org/abs/2408.13912},
  doi        = {10.48550/arXiv.2408.13912},
  abstract   = {In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Smart, Brandon and Zheng, Chuanxia and Laina, Iro and Prisacariu, Victor Adrian},
  month      = aug,
  year       = {2024},
  note       = {arXiv:2408.13912 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  annote     = {Comment: Our project page can be found at: https://splatt3r.active.vision/},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/ZEX9EM3N/Smart et al. - 2024 - Splatt3R Zero-shot Gaussian Splatting from Uncalibrated Image Pairs.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/H4X6446D/2408.html:text/html}
}

@misc{cheng_gaussianpro_2024,
  title      = {{GaussianPro}: {3D} {Gaussian} {Splatting} with {Progressive} {Propagation}},
  shorttitle = {{GaussianPro}},
  url        = {http://arxiv.org/abs/2402.14650},
  doi        = {10.48550/arXiv.2402.14650},
  abstract   = {The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Cheng, Kai and Long, Xiaoxiao and Yang, Kaizhi and Yao, Yao and Yin, Wei and Ma, Yuexin and Wang, Wenping and Chen, Xuejin},
  month      = feb,
  year       = {2024},
  note       = {arXiv:2402.14650 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: See the project page for code, data: https://kcheng1021.github.io/gaussianpro.github.io},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/GB5Y3GXN/Cheng et al. - 2024 - GaussianPro 3D Gaussian Splatting with Progressive Propagation.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/BCZ8IQJZ/2402.html:text/html}
}

@misc{lu_prosplat_2025,
  title      = {{ProSplat}: {Improved} {Feed}-{Forward} {3D} {Gaussian} {Splatting} for {Wide}-{Baseline} {Sparse} {Views}},
  shorttitle = {{ProSplat}},
  url        = {http://arxiv.org/abs/2506.07670},
  doi        = {10.48550/arXiv.2506.07670},
  abstract   = {Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising results for novel view synthesis (NVS) from sparse input views, particularly under narrow-baseline conditions. However, its performance significantly degrades in wide-baseline scenarios due to limited texture details and geometric inconsistencies across views. To address these challenges, in this paper, we propose ProSplat, a two-stage feed-forward framework designed for high-fidelity rendering under wide-baseline conditions. The first stage involves generating 3D Gaussian primitives via a 3DGS generator. In the second stage, rendered views from these primitives are enhanced through an improvement model. Specifically, this improvement model is based on a one-step diffusion model, further optimized by our proposed Maximum Overlap Reference view Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI supplements missing texture and color by strategically selecting a reference view with maximum viewpoint overlap, while DWEA enforces geometric consistency using epipolar constraints. Additionally, we introduce a divide-and-conquer training strategy that aligns data distributions between the two stages through joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K datasets under wide-baseline settings. Experimental results demonstrate that ProSplat achieves an average improvement of 1 dB in PSNR compared to recent SOTA methods.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Lu, Xiaohan and Fu, Jiaye and Zhang, Jiaqi and Song, Zetian and Jia, Chuanmin and Ma, Siwei},
  month      = jun,
  year       = {2025},
  note       = {arXiv:2506.07670 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/VCJVXG8V/Lu et al. - 2025 - ProSplat Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/4BWVXEEI/2506.html:text/html}
}

@misc{wang_volsplat_2025,
  title      = {{VolSplat}: {Rethinking} {Feed}-{Forward} {3D} {Gaussian} {Splatting} with {Voxel}-{Aligned} {Prediction}},
  shorttitle = {{VolSplat}},
  url        = {http://arxiv.org/abs/2509.19297},
  doi        = {10.48550/arXiv.2509.19297},
  abstract   = {Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Wang, Weijie and Chen, Yeqing and Zhang, Zeyu and Liu, Hengyu and Wang, Haoxiao and Feng, Zhiyuan and Qin, Wenkang and Zhu, Zheng and Chen, Donny Y. and Zhuang, Bohan},
  month      = sep,
  year       = {2025},
  note       = {arXiv:2509.19297 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Project Page: https://lhmd.top/volsplat, Code: https://github.com/ziplab/VolSplat},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/4ITV6IQK/Wang et al. - 2025 - VolSplat Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/H6S9QQ5C/2509.html:text/html}
}

@misc{lin_depth_2025,
  title      = {Depth {Anything} 3: {Recovering} the {Visual} {Space} from {Any} {Views}},
  shorttitle = {Depth {Anything} 3},
  url        = {http://arxiv.org/abs/2511.10647},
  doi        = {10.48550/arXiv.2511.10647},
  abstract   = {We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3\% in camera pose accuracy and 25.1\% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Lin, Haotong and Chen, Sili and Liew, Junhao and Chen, Donny Y. and Li, Zhenyu and Shi, Guang and Feng, Jiashi and Kang, Bingyi},
  month      = nov,
  year       = {2025},
  note       = {arXiv:2511.10647 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: https://depth-anything-3.github.io/},
  file       = {Snapshot:/home/sandro/Zotero/storage/W63B7UUM/2511.html:text/html}
}

@misc{wang_vggt_2025,
  title      = {{VGGT}: {Visual} {Geometry} {Grounded} {Transformer}},
  shorttitle = {{VGGT}},
  url        = {http://arxiv.org/abs/2503.11651},
  doi        = {10.48550/arXiv.2503.11651},
  abstract   = {We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},
  month      = mar,
  year       = {2025},
  note       = {arXiv:2503.11651 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: CVPR 2025, Project Page: https://vgg-t.github.io/},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/YP7XV6HG/Wang et al. - 2025 - VGGT Visual Geometry Grounded Transformer.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/VPDBA7II/2503.html:text/html}
}

@misc{leroy_grounding_2024,
  title     = {Grounding {Image} {Matching} in {3D} with {MASt3R}},
  url       = {http://arxiv.org/abs/2406.09756},
  doi       = {10.48550/arXiv.2406.09756},
  abstract  = {Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision. Yet despite matching being fundamentally a 3D problem, intrinsically linked to camera pose and scene geometry, it is typically treated as a 2D problem. This makes sense as the goal of matching is to establish correspondences between 2D pixel fields, but also seems like a potentially hazardous choice. In this work, we take a different stance and propose to cast matching as a 3D task with DUSt3R, a recent and powerful 3D reconstruction framework based on Transformers. Based on pointmaps regression, this method displayed impressive robustness in matching views with extreme viewpoint changes, yet with limited accuracy. We aim here to improve the matching capabilities of such an approach while preserving its robustness. We thus propose to augment the DUSt3R network with a new head that outputs dense local features, trained with an additional matching loss. We further address the issue of quadratic complexity of dense matching, which becomes prohibitively slow for downstream applications if not carefully treated. We introduce a fast reciprocal matching scheme that not only accelerates matching by orders of magnitude, but also comes with theoretical guarantees and, lastly, yields improved results. Extensive experiments show that our approach, coined MASt3R, significantly outperforms the state of the art on multiple matching tasks. In particular, it beats the best published methods by 30\% (absolute improvement) in VCRE AUC on the extremely challenging Map-free localization dataset.},
  urldate   = {2025-12-08},
  publisher = {arXiv},
  author    = {Leroy, Vincent and Cabon, Yohann and Revaud, JrÃ´me},
  month     = jun,
  year      = {2024},
  note      = {arXiv:2406.09756 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  file      = {Snapshot:/home/sandro/Zotero/storage/U36BB4BY/2406.html:text/html}
}

@misc{wu_4d_2024,
  title     = {{4D} {Gaussian} {Splatting} for {Real}-{Time} {Dynamic} {Scene} {Rendering}},
  url       = {http://arxiv.org/abs/2310.08528},
  doi       = {10.48550/arXiv.2310.08528},
  abstract  = {Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800\${\textbackslash}times\$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.},
  urldate   = {2025-12-08},
  publisher = {arXiv},
  author    = {Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei, Wei and Liu, Wenyu and Tian, Qi and Wang, Xinggang},
  month     = jul,
  year      = {2024},
  note      = {arXiv:2310.08528 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  annote    = {Comment: CVPR 2024. Project page: https://guanjunwu.github.io/4dgs/},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/DLFC4N3T/Wu et al. - 2024 - 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/PFZMS4IC/2310.html:text/html}
}

@misc{duan_4d-rotor_2024,
  title      = {{4D}-{Rotor} {Gaussian} {Splatting}: {Towards} {Efficient} {Novel} {View} {Synthesis} for {Dynamic} {Scenes}},
  shorttitle = {{4D}-{Rotor} {Gaussian} {Splatting}},
  url        = {http://arxiv.org/abs/2402.03307},
  doi        = {10.48550/arXiv.2402.03307},
  abstract   = {We consider the problem of novel-view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or generating high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities for modeling complicated dynamics and fine details--especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DRotorGS, which consistently outperforms existing methods both quantitatively and qualitatively.},
  language   = {en},
  urldate    = {2025-08-22},
  author     = {Duan, Yuanxing and Wei, Fangyin and Dai, Qiyu and He, Yuhang and Chen, Wenzheng and Chen, Baoquan},
  month      = jul,
  year       = {2024},
  note       = {arXiv:2402.03307 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, notion},
  file       = {PDF:/home/sandro/Zotero/storage/3ZE3GPQ5/Duan et al. - 2024 - 4D-Rotor Gaussian Splatting Towards Efficient Novel View Synthesis for Dynamic Scenes.pdf:application/pdf}
}

@misc{oh_hybrid_2025,
  title     = {Hybrid {3D}-{4D} {Gaussian} {Splatting} for {Fast} {Dynamic} {Scene} {Representation}},
  url       = {http://arxiv.org/abs/2505.13215},
  doi       = {10.48550/arXiv.2505.13215},
  abstract  = {Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.},
  urldate   = {2025-09-10},
  publisher = {arXiv},
  author    = {Oh, Seungjun and Lee, Younggeun and Jeon, Hyejin and Park, Eunbyung},
  month     = may,
  year      = {2025},
  note      = {arXiv:2505.13215 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, notion},
  annote    = {Comment: https://ohsngjun.github.io/3D-4DGS/},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/CDMTHIK6/Oh et al. - 2025 - Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/F9IA6QYI/2505.html:text/html}
}

@misc{yuan_1000_2025,
  title     = {1000+ {FPS} {4D} {Gaussian} {Splatting} for {Dynamic} {Scene} {Rendering}},
  url       = {http://arxiv.org/abs/2503.16422},
  doi       = {10.48550/arXiv.2503.16422},
  abstract  = {4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) {\textbackslash}textbf\{Short-Lifespan Gaussians\}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) {\textbackslash}textbf\{Inactive Gaussians\}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present {\textbackslash}textbf\{4DGS-1K\}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a \$41{\textbackslash}times\$ reduction in storage and \$9{\textbackslash}times\$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.},
  urldate   = {2025-09-12},
  publisher = {arXiv},
  author    = {Yuan, Yuheng and Shen, Qiuhong and Yang, Xingyi and Wang, Xinchao},
  month     = mar,
  year      = {2025},
  note      = {arXiv:2503.16422 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, notion},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/JWTG7YFR/Yuan et al. - 2025 - 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/KDKGPUBL/2503.html:text/html}
}

@misc{hu_4dgc_2025,
  title      = {{4DGC}: {Rate}-{Aware} {4D} {Gaussian} {Compression} for {Efficient} {Streamable} {Free}-{Viewpoint} {Video}},
  shorttitle = {{4DGC}},
  url        = {http://arxiv.org/abs/2503.18421},
  doi        = {10.48550/arXiv.2503.18421},
  abstract   = {3D Gaussian Splatting (3DGS) has substantial potential for enabling photorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number of Gaussians and their associated attributes poses significant challenges for storage and transmission. Existing methods typically handle dynamic 3DGS representation and compression separately, neglecting motion information and the rate-distortion (RD) trade-off during training, leading to performance degradation and increased model redundancy. To address this gap, we propose 4DGC, a novel rate-aware 4D Gaussian compression framework that significantly reduces storage size while maintaining superior RD performance for FVV. Specifically, 4DGC introduces a motion-aware dynamic Gaussian representation that utilizes a compact motion grid combined with sparse compensated Gaussians to exploit inter-frame similarities. This representation effectively handles large motions, preserving quality and reducing temporal redundancy. Furthermore, we present an end-to-end compression scheme that employs differentiable quantization and a tiny implicit entropy model to compress the motion grid and compensated Gaussians efficiently. The entire framework is jointly optimized using a rate-distortion trade-off. Extensive experiments demonstrate that 4DGC supports variable bitrates and consistently outperforms existing methods in RD performance across multiple datasets.},
  urldate    = {2025-09-12},
  publisher  = {arXiv},
  author     = {Hu, Qiang and Zheng, Zihan and Zhong, Houqiang and Fu, Sihua and Song, Li and XiaoyunZhang and Zhai, Guangtao and Wang, Yanfeng},
  month      = mar,
  year       = {2025},
  note       = {arXiv:2503.18421 [cs]
                version: 1},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, notion, Electrical Engineering and Systems Science - Image and Video Processing},
  annote     = {Comment: CVPR2025},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/U6UXGLVL/Hu et al. - 2025 - 4DGC Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/7IUG9IF5/2503.html:text/html}
}







@inproceedings{charles_pointnet_2017,
  address    = {Honolulu, HI},
  title      = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
  isbn       = {978-1-5386-0457-1},
  shorttitle = {{PointNet}},
  url        = {http://ieeexplore.ieee.org/document/8099499/},
  doi        = {10.1109/CVPR.2017.16},
  abstract   = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniï¬ed architecture for applications ranging from object classiï¬cation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efï¬cient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  month      = jul,
  year       = {2017},
  pages      = {77--85},
  file       = {PDF:/home/sandro/Zotero/storage/4GSHFUYH/Charles et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf:application/pdf}
}
@inproceedings{qi_pointnet_2017,
  title      = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
  volume     = {30},
  shorttitle = {{PointNet}++},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html},
  abstract   = {Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  urldate    = {2025-12-08},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates, Inc.},
  author     = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  year       = {2017},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/CE4G5PR3/Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on Point Sets in a Metric Space.pdf:application/pdf}
}
@misc{zhao_point_2021,
  title     = {Point {Transformer}},
  url       = {http://arxiv.org/abs/2012.09164},
  doi       = {10.48550/arXiv.2012.09164},
  abstract  = {Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4\% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70\% mIoU threshold for the first time.},
  urldate   = {2025-12-08},
  publisher = {arXiv},
  author    = {Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip and Koltun, Vladlen},
  month     = sep,
  year      = {2021},
  note      = {arXiv:2012.09164 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/8B9WLJJF/Zhao et al. - 2021 - Point Transformer.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/ZC7NYHVM/2012.html:text/html}
}
@misc{wu_point_2024,
  title      = {Point {Transformer} {V3}: {Simpler}, {Faster}, {Stronger}},
  shorttitle = {Point {Transformer} {V3}},
  url        = {http://arxiv.org/abs/2312.10035},
  doi        = {10.48550/arXiv.2312.10035},
  abstract   = {This paper is not motivated to seek innovation within the attention mechanism. Instead, it focuses on overcoming the existing trade-offs between accuracy and efficiency within the context of point cloud processing, leveraging the power of scale. Drawing inspiration from recent advances in 3D large-scale representation learning, we recognize that model performance is more influenced by scale than by intricate design. Therefore, we present Point Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the accuracy of certain mechanisms that are minor to the overall performance after scaling, such as replacing the precise neighbor search by KNN with an efficient serialized neighbor mapping of point clouds organized with specific patterns. This principle enables significant scaling, expanding the receptive field from 16 to 1024 points while remaining efficient (a 3x increase in processing speed and a 10x improvement in memory efficiency compared with its predecessor, PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that span both indoor and outdoor scenarios. Further enhanced with multi-dataset joint training, PTv3 pushes these results to a higher level.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Wu, Xiaoyang and Jiang, Li and Wang, Peng-Shuai and Liu, Zhijian and Liu, Xihui and Qiao, Yu and Ouyang, Wanli and He, Tong and Zhao, Hengshuang},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2312.10035 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: CVPR 2024, code available at Pointcept (https://github.com/Pointcept/PointTransformerV3)},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/AHQH6BVP/Wu et al. - 2024 - Point Transformer V3 Simpler, Faster, Stronger.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/R5TYS2L2/2312.html:text/html}
}

@misc{ma_shapesplat_2025,
  title      = {{ShapeSplat}: {A} {Large}-scale {Dataset} of {Gaussian} {Splats} and {Their} {Self}-{Supervised} {Pretraining}},
  shorttitle = {{ShapeSplat}},
  url        = {http://arxiv.org/abs/2408.10906},
  doi        = {10.48550/arXiv.2408.10906},
  abstract   = {3D Gaussian Splatting (3DGS) has become the de facto method of 3D representation in many vision tasks. This calls for the 3D understanding directly in this representation space. To facilitate the research in this direction, we first build ShapeSplat, a large-scale dataset of 3DGS using the commonly used ShapeNet, ModelNet and Objaverse datasets. Our dataset ShapeSplat consists of 206K objects spanning over 87 unique categories, whose labels are in accordance with the respective datasets. The creation of this dataset utilized the compute equivalent of 3.8 GPU years on a TITAN XP GPU. We utilize our dataset for unsupervised pretraining and supervised finetuning for classification and segmentation tasks. To this end, we introduce Gaussian-MAE, which highlights the unique benefits of representation learning from Gaussian parameters. Through exhaustive experiments, we provide several valuable insights. In particular, we show that (1) the distribution of the optimized GS centroids significantly differs from the uniformly sampled point cloud (used for initialization) counterpart; (2) this change in distribution results in degradation in classification but improvement in segmentation tasks when using only the centroids; (3) to leverage additional Gaussian parameters, we propose Gaussian feature grouping in a normalized feature space, along with splats pooling layer, offering a tailored solution to effectively group and embed similar Gaussians, which leads to notable improvement in finetuning tasks.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Ma, Qi and Li, Yue and Ren, Bin and Sebe, Nicu and Konukoglu, Ender and Gevers, Theo and Gool, Luc Van and Paudel, Danda Pani},
  month      = sep,
  year       = {2025},
  note       = {arXiv:2408.10906 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Accepted as 3DV'25 Oral, project page: https://unique1i.github.io/ShapeSplat\_webpage/},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/C5L23JWR/Ma et al. - 2025 - ShapeSplat A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/GBPMUNSL/2408.html:text/html}
}

@inproceedings{zaheer_deep_2017,
  title     = {Deep {Sets}},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  abstract  = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  urldate   = {2025-12-08},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  year      = {2017},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/TVBEEH6B/Zaheer et al. - 2017 - Deep Sets.pdf:application/pdf}
}

@inproceedings{lee_set_2019,
  title     = {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author    = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {3744--3753},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf},
  url       = {https://proceedings.mlr.press/v97/lee19d.html},
  abstract  = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  urldate   = {2025-12-08},
  booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/CKYU2XDL/Lee et al. - 2019 - Set Transformer A Framework for Attention-based Permutation-Invariant Neural Networks.pdf:application/pdf}
}

@inproceedings{jaegle_perceiver_2021,
  title     = {Perceiver: General Perception with Iterative Attention},
  author    = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {4651--4664},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {18--24 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url       = {https://proceedings.mlr.press/v139/jaegle21a.html},
  abstract  = {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/BWVRQ8UA/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Attention.pdf:application/pdf}
}

@misc{bertasius_is_2021,
  title     = {Is {Space}-{Time} {Attention} {All} {You} {Need} for {Video} {Understanding}?},
  url       = {http://arxiv.org/abs/2102.05095},
  doi       = {10.48550/arXiv.2102.05095},
  abstract  = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
  urldate   = {2025-12-08},
  publisher = {arXiv},
  author    = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  month     = jun,
  year      = {2021},
  note      = {arXiv:2102.05095 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  annote    = {Comment: Accepted to ICML 2021},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/6EDYCQ5Q/Bertasius et al. - 2021 - Is Space-Time Attention All You Need for Video Understanding.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/H6NICY3V/2102.html:text/html}
}

@inproceedings{han_span_2024,
  address    = {Melbourne VIC Australia},
  title      = {{\textless}span style="font-variant:small-caps;"{\textgreater}{Mamba3D}:{\textless}/span{\textgreater} {Enhancing} {Local} {Features} for {3D} {Point} {Cloud} {Analysis} via {State} {Space} {Model}},
  isbn       = {979-8-4007-0686-8},
  shorttitle = {{\textless}span style="font-variant},
  url        = {https://dl.acm.org/doi/10.1145/3664647.3681173},
  doi        = {10.1145/3664647.3681173},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
  publisher  = {ACM},
  author     = {Han, Xu and Tang, Yuan and Wang, Zhaoxuan and Li, Xianzhi},
  month      = oct,
  year       = {2024},
  pages      = {4995--5004},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/HLZL76PS/Han et al. - 2024 - Mamba3D Enhancing Local Features for 3D Point Cloud A.pdf:application/pdf}
}


@inproceedings{Liu_mamba4D_2025,
  author    = {Liu, Jiuming and Han, Jinru and Liu, Lihao and Aviles-Rivero, Angelica I. and Jiang, Chaokang and Liu, Zhe and Wang, Hesheng},
  title     = {Mamba4D: Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2025},
  pages     = {17626-17636},
  file      = {PDF:/home/sandro/Zotero/storage/ICBCN43R/Liu et al. - Mamba4D Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space.pdf:application/pdf}
}

@inproceedings{fan_point_2021,
  address = {Nashville, TN, USA},
  title = {Point {4D} {Transformer} {Networks} for {Spatio}-{Temporal} {Modeling} in {Point} {Cloud} {Videos}},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-4509-2},
  url = {https://ieeexplore.ieee.org/document/9578674/},
  doi = {10.1109/CVPR46437.2021.01398},
  abstract = {Point cloud videos exhibit irregularities and lack of order along the spatial dimension where points emerge inconsistently across different frames. To capture the dynamics in point cloud videos, point tracking is usually employed. However, as points may ï¬‚ow in and out across frames, computing accurate point trajectories is extremely difï¬cult. Moreover, tracking usually relies on point colors and thus may fail to handle colorless point clouds. In this paper, to avoid point tracking, we propose a novel Point 4D Transformer (P4Transformer) network to model raw point cloud videos. Speciï¬cally, P4Transformer consists of (i) a point 4D convolution to embed the spatio-temporal local structures presented in a point cloud video and (ii) a transformer to capture the appearance and motion information across the entire video by performing self-attention on the embedded local features. In this fashion, related or similar local areas are merged with attention weight rather than by explicit tracking. Extensive experiments, including 3D action recognition and 4D semantic segmentation, on four benchmarks demonstrate the effectiveness of our P4Transformer for point cloud video modeling.},
  language = {en},
  urldate = {2025-10-15},
  booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Fan, Hehe and Yang, Yi and Kankanhalli, Mohan},
  month = jun,
  year = {2021},
  keywords = {notion},
  pages = {14199--14208},
  file = {PDF:/home/sandro/Zotero/storage/2R8LCUYC/Fan et al. - 2021 - Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos.pdf:application/pdf},
}


@misc{thomas_tensor_2018,
  title      = {Tensor field networks: {Rotation}- and translation-equivariant neural networks for {3D} point clouds},
  shorttitle = {Tensor field networks},
  url        = {http://arxiv.org/abs/1802.08219},
  doi        = {10.48550/arXiv.1802.08219},
  abstract   = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  month      = may,
  year       = {2018},
  note       = {arXiv:1802.08219 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  annote     = {Comment: changes for NIPS submission},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/GYFWRZSK/Thomas et al. - 2018 - Tensor field networks Rotation- and translation-equivariant neural networks for 3D point clouds.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/S2ECUGU3/1802.html:text/html}
}

@inproceedings{fuchs_se3-transformers_2020,
  title      = {{SE}(3)-{Transformers}: {3D} {Roto}-{Translation} {Equivariant} {Attention} {Networks}},
  volume     = {33},
  shorttitle = {{SE}(3)-{Transformers}},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html},
  abstract   = {We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point-clouds, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.},
  urldate    = {2025-12-08},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates, Inc.},
  author     = {Fuchs, Fabian and Worrall, Daniel and Fischer, Volker and Welling, Max},
  year       = {2020},
  pages      = {1970--1981},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/FLUY4BHN/Fuchs et al. - 2020 - SE(3)-Transformers 3D Roto-Translation Equivariant Attention Networks.pdf:application/pdf}
}

@misc{liao_equiformer_2023,
  title      = {Equiformer: {Equivariant} {Graph} {Attention} {Transformer} for {3D} {Atomistic} {Graphs}},
  shorttitle = {Equiformer},
  url        = {http://arxiv.org/abs/2206.11990},
  doi        = {10.48550/arXiv.2206.11990},
  abstract   = {Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.},
  urldate    = {2025-12-08},
  publisher  = {arXiv},
  author     = {Liao, Yi-Lun and Smidt, Tess},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2206.11990 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Computational Physics},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/LQRGZ4ZE/Liao and Smidt - 2023 - Equiformer Equivariant Graph Attention Transformer for 3D Atomistic Graphs.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/6DPXMNSH/2206.html:text/html}
}

@inproceedings{deng_vector_2021,
  address    = {Montreal, QC, Canada},
  title      = {Vector {Neurons}: {A} {General} {Framework} for {SO}(3)-{Equivariant} {Networks}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-6654-2812-5},
  shorttitle = {Vector {Neurons}},
  url        = {https://ieeexplore.ieee.org/document/9711441/},
  doi        = {10.1109/ICCV48922.2021.01198},
  abstract   = {Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations “ including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network. Source code is available at https://github.com/FlyingGiraffe/vnn.},
  language   = {en},
  urldate    = {2025-12-08},
  booktitle  = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  publisher  = {IEEE},
  author     = {Deng, Congyue and Litany, Or and Duan, Yueqi and Poulenard, Adrien and Tagliasacchi, Andrea and Guibas, Leonidas},
  month      = oct,
  year       = {2021},
  pages      = {12180--12189},
  file       = {PDF:/home/sandro/Zotero/storage/7PGABMQX/Deng et al. - 2021 - Vector Neurons A General Framework for SO(3)-Equivariant Networks.pdf:application/pdf}
}

@misc{assaad_vn-transformer_2023,
  title = {{VN}-{Transformer}: {Rotation}-{Equivariant} {Attention} for {Vector} {Neurons}},
  shorttitle = {{VN}-{Transformer}},
  url = {http://arxiv.org/abs/2206.04176},
  doi = {10.48550/arXiv.2206.04176},
  abstract = {Rotation equivariance is a desirable property in many practical applications such as motion forecasting and 3D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations. Vector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional "vector neurons." We introduce a novel "VN-Transformer" architecture to address several shortcomings of the current VN models. Our contributions are: \$(i)\$ we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; \$(ii)\$ we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; \$(iii)\$ we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; \$(iv)\$ we show that small tradeoffs in equivariance (\$Îµ\$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models. Finally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results.},
  urldate = {2025-12-15},
  publisher = {arXiv},
  author = {Assaad, Serge and Downey, Carlton and Al-Rfou, Rami and Nayakanti, Nigamaa and Sapp, Ben},
  month = jan,
  year = {2023},
  note = {arXiv:2206.04176 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
  annote = {Comment: Published in Transactions on Machine Learning Research (TMLR), 2023; Previous version appeared in Workshop on Machine Learning for Autonomous Driving, Conference on Neural Information Processing Systems (NeurIPS), 2022},
  file = {Full Text PDF:/home/sandro/Zotero/storage/XFKV4X9D/Assaad et al. - 2023 - VN-Transformer Rotation-Equivariant Attention for Vector Neurons.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/PTSW3QRR/2206.html:text/html},
}




% @inproceedings{pmlr-v139-satorras21a,
%   title     = {E(n) Equivariant Graph Neural Networks},
%   author    = {Satorras, V\'{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
%   booktitle = {Proceedings of the 38th International Conference on Machine Learning},
%   pages     = {9323--9332},
%   year      = {2021},
%   editor    = {Meila, Marina and Zhang, Tong},
%   volume    = {139},
%   series    = {Proceedings of Machine Learning Research},
%   month     = {18--24 Jul},
%   publisher = {PMLR},
%   pdf       = {http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf},
%   url       = {https://proceedings.mlr.press/v139/satorras21a.html},
%   abstract  = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.}
% }

@article{rahman_radar_2018,
  title     = {Radar micro-{Doppler} signatures of drones and birds at {K}-band and {W}-band},
  volume    = {8},
  copyright = {2018 The Author(s)},
  issn      = {2045-2322},
  url       = {https://www.nature.com/articles/s41598-018-35880-9},
  doi       = {10.1038/s41598-018-35880-9},
  abstract  = {Due to the substantial increase in the number of affordable drones in the consumer market and their regrettable misuse, there is a need for efficient technology to detect drones in airspace. This paper presents the characteristic radar micro-Doppler properties of drones and birds. Drones and birds both induce micro-Doppler signatures due to their propeller blade rotation and wingbeats, respectively. These distinctive signatures can then be used to differentiate a drone from a bird, along with studying them separately. Here, experimental measurements of micro-Doppler signatures of different types of drones and birds are presented and discussed. The data have been collected using two radars operating at different frequencies; K-band (24‰GHz) and W-band (94‰GHz). Three different models of drones and four species of birds of varying sizes have been used for data collection. The results clearly demonstrate that a phase coherent radar system can retrieve highly reliable and distinctive micro-Doppler signatures of these flying targets, both at K-band and W-band. Comparison of the signatures obtained at the two frequencies indicates that the micro-Doppler return from the W-band radar has higher SNR. However, micro-Doppler features in the K-band radar returns also reveal the micro-motion characteristics of drones and birds very effectively.},
  language  = {en},
  number    = {1},
  urldate   = {2025-12-08},
  journal   = {Scientific Reports},
  author    = {Rahman, Samiur and Robertson, Duncan A.},
  month     = nov,
  year      = {2018},
  note      = {Publisher: Nature Publishing Group},
  keywords  = {Electrical and electronic engineering, Scientific data},
  pages     = {17396},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/VBG8TKS7/Rahman and Robertson - 2018 - Radar micro-Doppler signatures of drones and birds at K-band and W-band.pdf:application/pdf}
}

@article{molchanov_classification_2014,
  title    = {Classification of small {UAVs} and birds by micro-{Doppler} signatures},
  volume   = {6},
  issn     = {1759-0787, 1759-0795},
  url      = {https://www.cambridge.org/core/journals/international-journal-of-microwave-and-wireless-technologies/article/classification-of-small-uavs-and-birds-by-microdoppler-signatures/C5A57FC02BBC4261CE563738ED9D6D76},
  doi      = {10.1017/S1759078714000282},
  abstract = {The popularity of small unmanned aerial vehicles (UAVs) is increasing. Therefore, the importance of security systems able to detect and classify them is increasing as well. In this paper, we propose a new approach for UAVs classification using continuous wave radar or high pulse repetition frequency (PRF) pulse radars. We consider all steps of processing required to make a decision out of the raw radar data. Before the classification, the micro-Doppler signature is filtered and aligned to compensate the Doppler shift caused by the target's body motion. Then, classification features are extracted from the micro-Doppler signature in order to represent information about class at a lower dimension space. Eigenpairs extracted from the correlation matrix of the signature are used as informative features for classification. The proposed approach is verified on real radar measurements collected with X-band radar. Planes, quadrocopter, helicopters, and stationary rotors as well as birds are considered for classification. Moreover, a possibility of distinguishing different number of rotors is considered. The obtained results show the effectiveness of the proposed approach. It provides the capability of correct classification with a probability of around 92\%.},
  language = {en},
  number   = {3-4},
  urldate  = {2025-12-08},
  journal  = {International Journal of Microwave and Wireless Technologies},
  author   = {Molchanov, Pavlo and Harmanny, Ronny I. A. and Wit, Jaco J. M. de and Egiazarian, Karen and Astola, Jaakko},
  month    = jun,
  year     = {2014},
  keywords = {Radar applications, Radar signal processing and system modeling},
  pages    = {435--444},
  file     = {Full Text PDF:/home/sandro/Zotero/storage/8MRQN24A/Molchanov et al. - 2014 - Classification of small UAVs and birds by micro-Doppler signatures.pdf:application/pdf}
}

@article{rahman_classification_2020,
  title     = {Classification of drones and birds using convolutional neural networks applied to radar micro-{Doppler} spectrogram images},
  volume    = {14},
  copyright = {© 2020 The Institution of Engineering and Technology},
  issn      = {1751-8792},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-rsn.2019.0493},
  doi       = {10.1049/iet-rsn.2019.0493},
  abstract  = {This study presents a convolutional neural network-based drone classification method. The primary criterion for a high-fidelity neural network-based classification is a real dataset of large size and diversity for training. The first goal of the study was to create a large database of micro-Doppler spectrogram images of in-flight drones and birds. Two separate datasets with the same images have been created, one with RGB images and others with greyscale images. The RGB dataset was used for GoogLeNet architecture-based training. The greyscale dataset was used for training with a series of architecture developed during this study. Each dataset was further divided into two categories, one with four classes (drone, bird, clutter and noise) and the other with two classes (drone and non-drone). During training, 20\% of the dataset has been used as a validation set. After the completion of training, the models were tested with previously unseen and unlabelled sets of data. The validation and testing accuracy for the developed series network have been found to be 99.6 and 94.4\%, respectively, for four classes and 99.3 and 98.3\%, respectively, for two classes. The GoogLenet based model showed both validation and testing accuracies to be around 99\% for all the cases.},
  language  = {en},
  number    = {5},
  urldate   = {2025-12-08},
  journal   = {IET Radar, Sonar \& Navigation},
  author    = {Rahman, Samiur and Robertson, Duncan A.},
  year      = {2020},
  note      = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-rsn.2019.0493},
  keywords  = {clutter, convolutional neural nets, convolutional neural network-based drone classification method, convolutional neural networks, Doppler radar, feature extraction, GoogLeNet architecture-based training, GoogLenet based model, greyscale dataset, greyscale images, high-fidelity neural network-based classification, image classification, image colour analysis, in-flight drones, learning (artificial intelligence), object detection, radar microDoppler spectrogram images, radio networks, RGB dataset, RGB images},
  pages     = {653--661},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/6UMLBYNX/Rahman and Robertson - 2020 - Classification of drones and birds using convolutional neural networks applied to radar micro-Dopple.pdf:application/pdf}
}

@misc{akyon_sequence_2022,
  title     = {Sequence {Models} for {Drone} vs {Bird} {Classification}},
  url       = {http://arxiv.org/abs/2207.10409},
  doi       = {10.48550/arXiv.2207.10409},
  abstract  = {Drone detection has become an essential task in object detection as drone costs have decreased and drone technology has improved. It is, however, difficult to detect distant drones when there is weak contrast, long range, and low visibility. In this work, we propose several sequence classification architectures to reduce the detected false-positive ratio of drone tracks. Moreover, we propose a new drone vs. bird sequence classification dataset to train and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer based sequence classification architectures have been trained on the proposed dataset to show the effectiveness of the proposed idea. As experiments show, using sequence information, bird classification and overall F1 scores can be increased by up to 73\% and 35\%, respectively. Among all sequence classification models, R(2+1)D-based fully convolutional model yields the best transfer learning and fine-tuning results.},
  urldate   = {2025-12-08},
  publisher = {arXiv},
  author    = {Akyon, Fatih Cagatay and Akagunduz, Erdem and Altinuc, Sinan Onur and Temizel, Alptekin},
  month     = dec,
  year      = {2022},
  note      = {arXiv:2207.10409 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  file      = {Full Text PDF:/home/sandro/Zotero/storage/RGWA9BIM/Akyon et al. - 2022 - Sequence Models for Drone vs Bird Classification.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/UE9JD9B9/2207.html:text/html}
}


@article{sun_enhancing_2023,
  title     = {Enhancing {UAV} {Detection} in {Surveillance} {Camera} {Videos} through {Spatiotemporal} {Information} and {Optical} {Flow}},
  volume    = {23},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn      = {1424-8220},
  url       = {https://www.mdpi.com/1424-8220/23/13/6037},
  doi       = {10.3390/s23136037},
  abstract  = {The growing intelligence and prevalence of drones have led to an increase in their disorderly and illicit usage, posing substantial risks to aviation and public safety. This paper focuses on addressing the issue of drone detection through surveillance cameras. Drone targets in images possess distinctive characteristics, including small size, weak energy, low contrast, and limited and varying features, rendering precise detection a challenging task. To overcome these challenges, we propose a novel detection method that extends the input of YOLOv5s to a continuous sequence of images and inter-frame optical ï¬‚ow, emulating the visual mechanisms employed by humans. By incorporating the image sequence as input, our model can leverage both temporal and spatial information, extracting more features of small and weak targets through the integration of spatiotemporal data. This integration augments the accuracy and robustness of drone detection. Furthermore, the inclusion of optical ï¬‚ow enables the model to directly perceive the motion information of drone targets across consecutive frames, enhancing its ability to extract and utilize features from dynamic objects. Comparative experiments demonstrate that our proposed method of extended input significantly enhances the networks capability to detect small moving targets, showcasing competitive performance in terms of accuracy and speed. Speciï¬cally, our method achieves a ï¬nal average precision of 86.87\%, representing a noteworthy 11.49\% improvement over the baseline, and the speed remains above 30 frames per second. Additionally, our approach is adaptable to other detection models with different backbones, providing valuable insights for domains such as Urban Air Mobility and autonomous driving.},
  language  = {en},
  number    = {13},
  urldate   = {2025-08-12},
  journal   = {Sensors},
  author    = {Sun, Yu and Zhi, Xiyang and Han, Haowen and Jiang, Shikai and Shi, Tianjun and Gong, Jinnan and Zhang, Wei},
  month     = jun,
  year      = {2023},
  keywords  = {notion},
  pages     = {6037},
  file      = {PDF:/home/sandro/Zotero/storage/68PQUQE5/Sun et al. - 2023 - Enhancing UAV Detection in Surveillance Camera Videos through Spatiotemporal Information and Optical.pdf:application/pdf}
}


@misc{xu_depthsplat_2025,
  title      = {{DepthSplat}: {Connecting} {Gaussian} {Splatting} and {Depth}},
  shorttitle = {{DepthSplat}},
  url        = {http://arxiv.org/abs/2410.13862},
  doi        = {10.48550/arXiv.2410.13862},
  abstract   = {Gaussian splatting and single-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale multi-view posed datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. In addition, DepthSplat enables feed-forward reconstruction from 12 input views (512x960 resolutions) in 0.6 seconds.},
  urldate    = {2025-12-09},
  publisher  = {arXiv},
  author     = {Xu, Haofei and Peng, Songyou and Wang, Fangjinhua and Blum, Hermann and Barath, Daniel and Geiger, Andreas and Pollefeys, Marc},
  month      = mar,
  year       = {2025},
  note       = {arXiv:2410.13862 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: CVPR 2025, Project page: https://haofeixu.github.io/depthsplat/, Code: https://github.com/cvg/depthsplat},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/VK5JZEMT/Xu et al. - 2025 - DepthSplat Connecting Gaussian Splatting and Depth.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/E3MP9BEM/2410.html:text/html}
}

@misc{lin_objaverse_2025,
  title      = {Objaverse++: {Curated} {3D} {Object} {Dataset} with {Quality} {Annotations}},
  shorttitle = {Objaverse++},
  url        = {http://arxiv.org/abs/2504.07334},
  doi        = {10.48550/arXiv.2504.07334},
  abstract   = {This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset.},
  urldate    = {2025-12-09},
  publisher  = {arXiv},
  author     = {Lin, Chendi and Liu, Heshan and Lin, Qunshu and Bright, Zachary and Tang, Shitao and He, Yihui and Liu, Minghao and Zhu, Ling and Le, Cindy},
  month      = apr,
  year       = {2025},
  note       = {arXiv:2504.07334 [cs]
                version: 1},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  annote     = {Comment: 8 pages, 8 figures. Accepted to CVPR 2025 Workshop on Efficient Large Vision Models (April 2025)},
  file       = {Full Text PDF:/home/sandro/Zotero/storage/99MSH7YK/Lin et al. - 2025 - Objaverse++ Curated 3D Object Dataset with Quality Annotations.pdf:application/pdf;Snapshot:/home/sandro/Zotero/storage/N4UBKER9/2504.html:text/html}
}

@inproceedings{yang_depth_2024,
  title = {Depth {Anything} {V2}},
  volume = {37},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/26cfdcd8fe6fd75cc53e92963a656c58-Paper-Conference.pdf},
  doi = {10.52202/079017-0688},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  pages = {21875--21911},
  file = {26cfdcd8fe6fd75cc53e92963a656c58-Paper-Conference.pdf:/home/sandro/Zotero/storage/893E4NME/26cfdcd8fe6fd75cc53e92963a656c58-Paper-Conference.pdf:application/pdf},
}

@inproceedings{ren_faster_2015,
  title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
  volume = {28},
  shorttitle = {Faster {R}-{CNN}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
  urldate = {2025-12-15},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  file = {Full Text PDF:/home/sandro/Zotero/storage/SDBN8PPX/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:application/pdf},
}




@misc{nvidia_isaac_2025,
  title = {Isaac {Sim}},
  url = {https://github.com/isaac-sim/IsaacSim},
  abstract = {NVIDIA Isaac Sim„¢ is an open-source application on NVIDIA Omniverse for developing, simulating, and testing AI-driven robots in realistic virtual environments.},
  urldate = {2025-12-15},
  author = {{NVIDIA}},
  month = dec,
  year = {2025},
  note = {original-date: 2025-05-28T18:38:18Z},
}

@article{kazhdan_rotation_nodate,
  title = {Rotation {Invariant} {Spherical} {Harmonic} {Representation} of {3D} {Shape} {Descriptors}},
  abstract = {One of the challenges in 3D shape matching arises from the fact that in many applications, models should be considered to be the same if they differ by a rotation. Consequently, when comparing two models, a similarity metric implicitly provides the measure of similarity at the optimal alignment. Explicitly solving for the optimal alignment is usually impractical. So, two general methods have been proposed for addressing this issue: (1) Every model is represented using rotation invariant descriptors. (2) Every model is described by a rotation dependent descriptor that is aligned into a canonical coordinate system deï¬ned by the model. In this paper, we describe the limitations of canonical alignment and discuss an alternate method, based on spherical harmonics, for obtaining rotation invariant representations. We describe the properties of this tool and show how it can be applied to a number of existing, orientation dependent descriptors to improve their matching performance. The advantages of this tool are two-fold: First, it improves the matching performance of many descriptors. Second, it reduces the dimensionality of the descriptor, providing a more compact representation, which in turn makes comparing two models more efï¬cient.},
  language = {en},
  author = {Kazhdan, Michael and Funkhouser, Thomas and Rusinkiewicz, Szymon},
  file = {PDF:/home/sandro/Zotero/storage/UCW349CC/Kazhdan et al. - Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors.pdf:application/pdf},
}
