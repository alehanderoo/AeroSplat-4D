# @package _global_
# Objaverse object-centric 3D Gaussian Splatting experiment - MASK INTEGRATION
#
# This experiment enables ALL mask integration strategies from research literature:
# - Soft cost volume masking (preserves gradients at boundaries)
# - Gradient matching loss (enforces sharp depth edges)
# - Mask-guided DPT fusion (attention gates in skip connections)
# - Mask boundary-guided upsampling (edge features for sharp boundaries)
#
# Base config from objaverse_white_small_gauss with:
# - Reduced gaussian_scale_max: 0.01 → 0.005 (smaller gaussians)
# - Reduced gaussian_scale_min: 0.0005 → 0.0003 (finer details)
# - Increased mask weight: 0.1 → 0.2 (crisper silhouettes)
# - Increased LPIPS weight: 0.05 → 0.08 (preserve perceptual sharpness)

defaults:
  - override /dataset: objaverse # Uses Objaverse-specific dataset loader
  - override /model/encoder: depthsplat # Uses standard DepthSplat encoder
  - override /loss: [mse, lpips, depth, mask, gradient] # Full-image RGB + direct alpha supervision + gradient matching

wandb:
  name: objaverse_white_small_gauss_maskIntegration # Display name in Weights & Biases
  tags: [objaverse, object_centric, white_background, vits, small_gaussians, mask_integration] # Filtering tags for W&B

# Data loading
data_loader:
  train:
    batch_size: 1 # Images per batch during training
    num_workers: 8 # CPU threads for data loading
  test:
    batch_size: 1 # Process one scene at a time
  val:
    batch_size: 1 # Process one scene at a time

# Training
trainer:
  max_steps: 100000 # Total training iterations
  num_nodes: 1 # Number of GPU nodes

# Optimizer - higher LR for training from scratch
optimizer:
  lr: 1.5e-4 # Global learning rate
  lr_monodepth: 1.e-5  # Lower LR for pretrained Depth Anything V2 to preserve weights
  warm_up_steps: 2000 # Linear warmup steps for stability
  weight_decay: 0.01 # L2 regularization factor

# Model configuration
model:
  encoder:
    # Depth estimation
    num_depth_candidates: 128 # Number of depth planes for initial cost volume

    # Cost volume
    costvolume_unet_feat_dim: 32 # Feature channels in cost volume UNet
    costvolume_unet_channel_mult: [1, 1, 1] # Channel multipliers per level
    costvolume_unet_attn_res: [4] # Resolution to apply attention

    # Gaussians
    gaussians_per_pixel: 1 # Number of Gaussian primitives per pixel

    # Depth head
    depth_unet_feat_dim: 64 # Feature channels for depth head
    depth_unet_attn_res: [16] # Attention resolution for depth head
    depth_unet_channel_mult: [1, 1, 1, 1, 1] # Channel multipliers

    # Gaussian adapter - SMALLER scales for sharper details
    gaussian_adapter:
      gaussian_scale_min: 0.0003 # Reduced from 0.0005 for finer details
      gaussian_scale_max: 0.005 # Reduced from 0.01 to prevent blobby renders
      sh_degree: 2 # Spherical harmonics degree for color

    # Use ViT-Small for Depth Anything V2
    monodepth_vit_type: vits # Backbone type for Depth Anything V2

    # Multi-view matching
    local_mv_match: 5 # Number of adjacent views for feature matching

    # ============================================================================
    # MASK INTEGRATION OPTIONS (Research-based improvements)
    # ENABLED for training run with all mask integration strategies
    # ============================================================================

    # Soft Cost Volume Masking
    # Replaces binary cost volume zeroing with soft confidence weighting
    # Preserves gradients at mask boundaries for smoother depth transitions
    soft_mask_enabled: true
    soft_mask_temperature: 0.5  # Lower = sharper (0 approaches binary)
    soft_mask_edge_weight: 1.0  # Boost factor at mask edges

    # Mask-Guided DPT Fusion
    # Adds mask-guided attention gates in DPT feature fusion blocks
    # Emphasizes foreground boundaries during depth refinement
    mask_guided_dpt_enabled: true

    # Mask Boundary-Guided Upsampling
    # Concatenates mask edge features to final depth upsampling
    # Preserves sharp object boundaries during 8x upsampling
    mask_guided_upsample_enabled: true


# Loss weights - TUNED FOR SHARPER RENDERS
loss:
  mse:
    weight: 1.0 # Pixel Reconstruction Error
    use_foreground_mask: false # Full image loss for white backgrounds
  lpips:
    apply_after_step: 0 # Start applying perceptual loss immediately
    weight: 0.08 # Increased from 0.05 for better perceptual sharpness
    use_foreground_mask: false # Full image loss for white backgrounds
  depth:
    weight: 0.5 # Weight for depth supervision using GT Blender depth
    min_depth: 0.55 # Minimum valid depth (matches dataset.near)
    max_depth: 2.54 # Maximum valid depth (matches dataset.far)
    background_weight: 0.1 # Push background depth to far plane
  mask:
    weight: 0.2 # Increased from 0.1 for crisper silhouettes
    use_bce: false # MSE is standard approach
  # Gradient matching loss for depth edge sharpness
  gradient:
    weight: 0.1  # Gradient matching loss weight
    boundary_weight: 2.0  # Extra weight at mask boundaries
    boundary_dilation: 3  # Dilation for boundary detection
    multi_scale: true  # Use multi-scale gradient matching
    min_depth: 0.55  # Minimum valid depth
    max_depth: 2.54  # Maximum valid depth

# Dataset configuration
dataset:
  image_shape: [256, 256] # Training image resolution
  roots: [/mnt/raid0/objaverse] # Path to WHITE background dataset
  background_color: [1.0, 1.0, 1.0] # WHITE background

  # Object-centric depth bounds (data-driven, from scripts/compute_depth_bounds.py)
  near: 0.55 # Near plane
  far: 2.54 # Far plane

  # Enable masks and depth for supervision
  use_masks: true # Load ground truth masks
  use_depth: true # Load ground truth depth

  # View sampling
  view_sampler:
    name: object_centric # Sampling strategy
    num_context_views: 5 # Number of input views
    num_target_views: 2 # Number of novel views to predict (loss calculation)
    sampling_strategy: farthest_point # Sample views to maximize coverage

  # Training
  augment: true # Enable random horizontal flips (images & poses)
  train_times_per_scene: 1 # Epochs per scene per load
  cameras_are_circular: true # Normalize assuming circular camera path

# Checkpointing - only load Depth Anything V2, train rest from scratch
checkpointing:
  every_n_train_steps: 5000 # Save checkpoint frequency
  save_top_k: 3 # Keep top 3 best checkpoints
  pretrained_monodepth: pretrained/depth_anything_v2_vits.pth # Pretrained backbone path
  pretrained_model: null # Train scratch
  pretrained_mvdepth: null # Train scratch
  pretrained_depth: null # Train scratch
  no_strict_load: false # Enforce strict state dict matching
  resume: false # Do not resume from previous run

# Training settings
train:
  l1_loss: false # Use L2 (MSE) instead of L1
  intermediate_loss_weight: 0.9 # Weight for intermediate supervision
  print_log_every_n_steps: 100 # Logging frequency
  eval_model_every_n_val: 2 # Validation frequency
  train_ignore_large_loss: 0.0 # Threshold to ignore gradients (0 = off)
  forward_depth_only: false # Full forward pass
  no_viz_video: true # Disable video rendering (wandb compatibility issue)

# Testing
test:
  eval_time_skip_steps: 5 # Skip steps during timing eval
  compute_scores: true # Calculate PSNR/SSIM/LPIPS
  save_image: true # Save output images
  save_depth: false # Don't save depth maps
  save_gaussian: false # Don't save point clouds

# Output
output_dir: outputs/objaverse_white_small_gauss_maskIntegration # Directory for logs and checkpoints
