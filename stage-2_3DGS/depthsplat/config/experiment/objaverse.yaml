# @package _global_
# Objaverse object-centric 3D Gaussian Splatting experiment
#
# Training from scratch with only Depth Anything V2 (ViT-S) pretrained weights.
# The rest of DepthSplat (cost volume, Gaussian heads, etc.) is trained from scratch.
#
# Key characteristics:
# - Object-centric: cameras arranged around central object
# - Tighter depth bounds suitable for objects
# - Black background (no background Gaussians)
# - GT masks used at inference time (no silhouette prediction needed)

defaults:
  - override /dataset: objaverse # Uses Objaverse-specific dataset loader
  - override /model/encoder: depthsplat # Uses standard DepthSplat encoder
  - override /loss: [mse, lpips, depth, silhouette] # Active losses: MSE (foreground-masked), LPIPS, Depth supervision, Silhouette (penalizes floaters)

wandb:
  name: objaverse_from_scratch # Display name in Weights & Biases
  tags: [objaverse, object_centric, from_scratch, vits] # Filtering tags for W&B

# Data loading
data_loader:
  train:
    batch_size: 1 # Images per batch during training
    num_workers: 8 # CPU threads for data loading
  test:
    batch_size: 1 # Process one scene at a time
  val:
    batch_size: 1 # Process one scene at a time

# Training
trainer:
  max_steps: 100000 # Total training iterations
  num_nodes: 1 # Number of GPU nodes

# Optimizer - higher LR for training from scratch
optimizer:
  lr: 1.5e-4 # Global learning rate
  lr_monodepth: 1.e-5  # Lower LR for pretrained Depth Anything V2 to preserve weights
  warm_up_steps: 2000 # Linear warmup steps for stability
  weight_decay: 0.01 # L2 regularization factor

# Model configuration
model:
  encoder:
    # Depth estimation
    num_depth_candidates: 64 # Number of depth planes for initial cost volume

    # Cost volume
    costvolume_unet_feat_dim: 64 # Feature channels in cost volume UNet
    costvolume_unet_channel_mult: [1, 1, 1] # Channel multipliers per level
    costvolume_unet_attn_res: [4] # Resolution to apply attention

    # Gaussians
    gaussians_per_pixel: 1 # Number of Gaussian primitives per pixel

    # Depth head
    depth_unet_feat_dim: 32 # Feature channels for depth head
    depth_unet_attn_res: [16] # Attention resolution for depth head
    depth_unet_channel_mult: [1, 1, 1, 1, 1] # Channel multipliers

    # Gaussian adapter - smaller scales for objects
    gaussian_adapter:
      gaussian_scale_min: 1.e-6 # Minimum scale for Gaussians
      gaussian_scale_max: 0.1 # Maximum scale (suited for object level)
      sh_degree: 2 # Spherical harmonics degree for color

    # Use ViT-Small for Depth Anything V2
    monodepth_vit_type: vits # Backbone type for Depth Anything V2

    # Multi-view matching
    local_mv_match: 5 # Number of adjacent views for feature matching (increased for better coverage)

# Loss weights (MSE and LPIPS are now computed only on foreground pixels)
loss:
  mse:
    weight: 1.0 # Weight for Pixel Reconstruction Error (foreground-masked)
  lpips:
    apply_after_step: 0 # Start applying perceptual loss immediately
    weight: 0.05 # Weight for LPIPS perceptual loss
  depth:
    weight: 0.5 # Weight for depth supervision using GT Blender depth
    min_depth: 0.52 # Minimum valid depth (matches dataset.near)
    max_depth: 2.54 # Maximum valid depth (matches dataset.far)
    background_weight: 0.1 # Push background depth to far plane (prevents grid artifacts)
  silhouette:
    weight: 0.2 # Penalizes Gaussians in background regions (prevents floaters)
    method: "alpha_from_color" # Estimate opacity from color difference to background
    background_color: [0.0, 0.0, 0.0] # Black background

# Dataset configuration
dataset:
  image_shape: [256, 256] # Training image resolution
  roots: [datasets/objaverse] # Path to dataset
  background_color: [0.0, 0.0, 0.0] # Force black background

  # Object-centric depth bounds (data-driven, from scripts/compute_depth_bounds.py)
  near: 0.52 # Near plane (5th percentile of dataset min depths)
  far: 2.54 # Far plane (95th percentile of dataset max depths)

  # Enable masks and depth for supervision
  use_masks: true # Load ground truth masks
  use_depth: true # Load ground truth depth

  # View sampling
  view_sampler:
    name: object_centric # Sampling strategy
    num_context_views: 5 # Number of input views
    num_target_views: 2 # Number of novel views to predict (loss calculation)
    sampling_strategy: farthest_point # or "random", "uniform" Sample views to maximize coverage

  # Training
  augment: true # Enable random horizontal flips (images & poses)
  train_times_per_scene: 1 # Epochs per scene per load
  cameras_are_circular: true # Normalize assuming circular camera path

# Checkpointing - only load Depth Anything V2, train rest from scratch
checkpointing:
  every_n_train_steps: 2000 # Save checkpoint frequency
  save_top_k: 3 # Keep top 3 best checkpoints
  pretrained_monodepth: pretrained/depth_anything_v2_vits.pth # Pretrained backbone path
  pretrained_model: null # Train scratch
  pretrained_mvdepth: null # Train scratch
  pretrained_depth: null # Train scratch
  no_strict_load: false # Enforce strict state dict matching
  resume: false # Do not resume from previous run

# Training settings
train:
  l1_loss: false # Use L2 (MSE) instead of L1
  intermediate_loss_weight: 0.9 # Weight for intermediate supervision
  print_log_every_n_steps: 100 # Logging frequency
  eval_model_every_n_val: 2 # Validation frequency
  train_ignore_large_loss: 0.0 # Threshold to ignore gradients (0 = off)
  forward_depth_only: false # Full forward pass
  no_viz_video: true # Disable video rendering (wandb compatibility issue)

# Testing
test:
  eval_time_skip_steps: 5 # Skip steps during timing eval
  compute_scores: true # Calculate PSNR/SSIM/LPIPS
  save_image: true # Don't save output images
  save_depth: false # Don't save depth maps
  save_gaussian: false # Don't save point clouds

# Output
output_dir: outputs/objaverse # Directory for logs and checkpoints
