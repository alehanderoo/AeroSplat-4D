# Training Process Documentation

This document describes in full detail how the **DepthSplat** model is trained to generate 3D Gaussian Splatting models from input pictures (context views) and render them from novel viewpoints (target views).

## High-Level Overview

The training pipeline is designed to learn a generalizable model that can take a sparse set of images (context views) of an object and predict a 3D representation consisting of 3D Gaussians. This representation allows for rendering the object from any novel viewpoint.

The process follows an **Encoder-Decoder** architecture:
1.  **Encoder**: Takes context images and camera poses, estimates depth and features, and "lifts" pixels into 3D Gaussians.
2.  **Decoder**: Takes the generated 3D Gaussians and differentiable renders them from target viewpoints.
3.  **Training**: The model is trained end-to-end by minimizing the difference between the rendered images and the ground truth target images.

## Entry Point

The training process is initiated via `src/main.py`, which uses **Hydra** for configuration management and **PyTorch Lightning** for the training loop.

-   **Wrapper**: `src.model.model_wrapper.ModelWrapper` orchestrates the training step, calling the encoder, decoder, and loss functions.
-   **Data**: `src.dataset.data_module.DataModule` provides batches containing `context` views (input) and `target` views (ground truth for supervision).

## Model Architecture

### 1. Encoder (`src.model.encoder.encoder_depthsplat.EncoderDepthSplat`)

The encoder is responsible for converting 2D input images into a cloud of 3D Gaussians.

*   **Multi-View Stereo (MVS)**: It uses `MultiViewUniMatch` (a cost-volume-based depth estimator) to predict depth maps for the context views. It leverages cross-view attention and epipolar geometry to estimate accurate geometry.
*   **Feature Extraction**: It extracts deep features from the input images, fusing information from the MVS backbone (`DPTHead`).
*   **Depth Prediction**: Predicts a depth map for each context view. This depth is effectively used to unproject pixels into 3D space.
*   **Gaussian Parameter Regressor**:
    *   A CNN (`gaussian_regressor` and `gaussian_head`) takes the concatenated image, features, and depth embeddings.
    *   It predicts the attributes for each pixel-aligned Gaussian:
        *   **Opacity**: Softmax/Sigmoid activation.
        *   **Scale**: 3D scale factors.
        *   **Rotation**: Quaternions.
        *   **Harmonics**: Spherical Harmonic coefficients (for view-dependent color).
        *   **Offset**: Small positional adjustments to the unprojected 3D point.
*   **Lifting to 3D**: Using the predicted depth and camera intrinsics/extrinsics, each pixel in the context/input views is unprojected into a 3D point, which becomes the center of a Gaussian.

### 2. Decoder (`src.model.decoder.decoder_splatting_cuda.DecoderSplattingCUDA`)

The decoder is a differentiable Gaussian Rasterizer.

*   **Input**: The cloud of 3D Gaussians generated by the Encoder.
*   **Rendering**: It projects these Gaussians onto the 2D image plane of the *target* camera poses.
*   **Output**:
    *   **Color**: The rendered RGB image.
    *   **Depth**: The rendered depth map (optional, used for visualization or auxiliary supervision).

## Training Loop Details

The `training_step` in `ModelWrapper` proceeds as follows for each batch:

1.  **Input Data**: A batch contains $N$ scenes. For each scene, we have:
    *   `context`: $K$ input images and their poses.
    *   `target`: $M$ novel view images and their poses (Ground Truth).
    *   `mask`: Foreground masks (to ignore background pixels).

2.  **Forward Pass (Encoder)**:
    *   The `encoder` processes the `context` views.
    *   It estimates depth for each context view.
    *   It generates a set of 3D Gaussians (means, covariances, colors, opacities) defined in the world coordinate system.

3.  **Forward Pass (Decoder)**:
    *   The `decoder` takes the generated Gaussians and renders them from the `target` viewpoints.
    *   This produces `output.color` (predicted RGB) and optional depth.

4.  **Loss Calculation**:
    The Loss is computed between the `output.color` and `target.image`. To ensure the model focuses on the object, a **valid depth mask** (derived from the ground truth foreground mask) is used to exclude background pixels from the loss.

    *   **Reconstruction Loss**:
        *   **MSE / L1**: Computes pixel-wise difference between predicted and ground truth color.
        *   **LPIPS**: Perceptual loss to ensure high-frequency details and structural fidelity.
        *   **SSIM**: Structural Similarity Index.
    *   **Depth Supervision** (Optional but common):
        *   **Depth Loss**: The depth maps predicted by the `encoder` (for context views) are supervised directly if ground truth depth is available.
        *   **Intermediate Supervision**: If the encoder predicts depths at multiple scales/stages, losses are applied to all valid intermediate predictions.
    *   **Regularization**:
        *   **Edge-aware Spatial Variation (Smoothness) loss**: often used on depth maps.

5.  **Optimization**:
    *   Gradients are backpropagated through the Decoder (differentiable rasterization) and Encoder.
    *   The weights of the Encoder (MVS network, feature extractors, Gaussian heads) are updated.

## Summary

The model learns to "look" at a few images of an object, understand its 3D geometry (via depth estimation), and represent it as a collection of 3D ellipsoids (Gaussians). This representation is explicitly trained to be renderable from any angle, matching the ground truth images of the object.
